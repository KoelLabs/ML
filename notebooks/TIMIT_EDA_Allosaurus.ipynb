{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from scripts.audio import audio_array_to_wav_file\n",
    "from scripts.ipa import timit2ipa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Loading TIMIT \n",
    "Download the TIMIT.zip from: DARPA TIMIT on Kaggle, save to /.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit = zipfile.ZipFile('../.data/TIMIT.zip', 'r')\n",
    "timit_files = timit.namelist()\n",
    "print(timit_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print README.DOC\n",
    "timit_readme = timit.open('README.DOC')\n",
    "for line in timit_readme:\n",
    "    print(line.decode('utf-8').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PHN', 'TXT', 'WAV', 'WRD'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the file extensions for each data point\n",
    "set(map(lambda x: x.split('.')[1], filter(lambda x: x.startswith('data/TRAIN'), timit_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = list(set(map(lambda x: x.split('.')[0], filter(lambda x: x.startswith('data/TRAIN'), timit_files))))\n",
    "print(training_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timit_file_to_dict(filename):\n",
    "    with timit.open(filename + '.PHN') as phn_file:\n",
    "        timestamped_phonemes = []\n",
    "        for line in phn_file.read().decode('utf-8').split('\\n'):\n",
    "            if line == '':\n",
    "                continue\n",
    "            start, end, phoneme = line.split()\n",
    "            timestamped_phonemes.append((timit2ipa(phoneme), int(start) / 16_000, int(end) / 16_000))\n",
    "    with timit.open(filename + '.TXT') as txt_file:\n",
    "        transcription = txt_file.read().decode('utf-8').strip()\n",
    "        # first two numbers are the start and end times\n",
    "        transcription_start, transcription_end, *words = transcription.split()\n",
    "        transcription = ' '.join(words)\n",
    "    with timit.open(filename + '.WRD') as wrd_file:\n",
    "        timestamped_words = []\n",
    "        for line in wrd_file.read().decode('utf-8').split('\\n'):\n",
    "            if line == '':\n",
    "                continue\n",
    "            start, end, word = line.split()\n",
    "            timestamped_words.append((word, int(start) / 16_000, int(end) / 16_000))\n",
    "    path_segments = pathlib.Path(filename).parts\n",
    "    speaker_initials = path_segments[-2][1:]\n",
    "    return {'speaker_initials': speaker_initials, 'timestamped_phonemes': timestamped_phonemes, 'transcription': transcription, 'transcription_start': int(transcription_start) / 16_000, 'transcription_end': int(transcription_end) / 16_000, 'timestamped_words': timestamped_words, 'wav_filename': filename + '.WAV'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'speaker_initials': 'GRP0', 'timestamped_phonemes': [('', 0.0, 0.1325), ('', 0.1325, 0.1625), ('oʊ', 0.1625, 0.27), ('ɾ̃', 0.27, 0.29), ('æ', 0.29, 0.4525), ('s', 0.4525, 0.5351875), ('k', 0.5351875, 0.5975), ('m', 0.5975, 0.6325), ('i', 0.6325, 0.7525), ('t', 0.7525, 0.80675), ('', 0.80675, 0.8425), ('ɨ', 0.8425, 0.8975), ('k', 0.8975, 0.9275), ('', 0.9275, 0.9925), ('ɪ', 0.9925, 1.086125), ('ɹ', 1.086125, 1.180125), ('i', 1.180125, 1.2871875), ('æ', 1.2871875, 1.37725), ('ɾ̃', 1.37725, 1.416125), ('ɔ', 1.416125, 1.624), ('l', 1.624, 1.671), ('ɨ', 1.671, 1.7310625), ('ɹ', 1.7310625, 1.8485625), ('æ', 1.8485625, 2.0425), ('g', 2.0425, 2.0825), ('', 2.0825, 2.103875), ('l', 2.103875, 2.15525), ('aɪ', 2.15525, 2.2575), ('k', 2.2575, 2.368125), ('ð', 2.368125, 2.3925), ('æ', 2.3925, 2.6086875), ('t', 2.6086875, 2.7075625), ('', 2.7075625, 2.7551875), ('', 2.7551875, 3.39)], 'transcription': \"Don't ask me to carry an oily rag like that.\", 'transcription_start': 0.0, 'transcription_end': 3.392, 'timestamped_words': [(\"don't\", 0.1325, 0.29), ('ask', 0.29, 0.5975), ('me', 0.5975, 0.7525), ('to', 0.7525, 0.8975), ('carry', 0.8975, 1.2871875), ('an', 1.2871875, 1.416125), ('oily', 1.416125, 1.7310625), ('rag', 1.7310625, 2.103875), ('like', 2.103875, 2.368125), ('that', 2.368125, 2.7551875)], 'wav_filename': 'data/TRAIN/DR4/MGRP0/SA2.WAV'}\n",
      "{'speaker_initials': 'MBS0', 'timestamped_phonemes': [('', 0.0, 0.1525), ('', 0.1525, 0.1825), ('ɔ', 0.1825, 0.245375), ('ɹ', 0.245375, 0.2925), ('k', 0.2925, 0.3475), ('', 0.3475, 0.3825), ('j', 0.3825, 0.404125), ('ʉ', 0.404125, 0.4475), ('p', 0.4475, 0.5225), ('', 0.5225, 0.550875), ('aɪ', 0.550875, 0.7425), ('n', 0.7425, 0.7875), ('z', 0.7875, 0.874625), ('ɹ', 0.874625, 0.9164375), ('i', 0.9164375, 0.9925), ('z', 0.9925, 1.0875), ('ɛ', 1.0875, 1.1475), ('m', 1.1475, 1.2075), ('b', 1.2075, 1.2621875), ('', 1.2621875, 1.2710625), ('l̩ ', 1.2710625, 1.3825), ('s', 1.3825, 1.5675), ('i', 1.5675, 1.757125), ('ʔ', 1.757125, 1.832875), ('ɝ', 1.832875, 1.9373125), ('t', 1.9373125, 1.99475), ('tʃ', 1.99475, 2.0756875), ('ɨ', 2.0756875, 2.1226875), ('n', 2.1226875, 2.2208125), ('', 2.2208125, 2.3733125), ('z', 2.3733125, 2.4699375), ('', 2.4699375, 2.98)], 'transcription': 'Porcupines resemble sea urchins.', 'transcription_start': 0.0, 'transcription_end': 2.9825, 'timestamped_words': [('porcupines', 0.1525, 0.874625), ('resemble', 0.874625, 1.3825), ('sea', 1.3825, 1.757125), ('urchins', 1.832875, 2.4699375)], 'wav_filename': 'data/TRAIN/DR4/MMBS0/SX71.WAV'}\n"
     ]
    }
   ],
   "source": [
    "parsed_0 = timit_file_to_dict(training_files[0])\n",
    "print(parsed_0)\n",
    "parsed_1 = timit_file_to_dict(training_files[1])\n",
    "print(parsed_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_HEADER_SIZE = 44\n",
    "def zipped_wav_to_array(filename):\n",
    "    with timit.open(filename) as wav_file:\n",
    "        return np.frombuffer(wav_file.read(), dtype=np.int16)[WAV_HEADER_SIZE//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_to_df(files):\n",
    "    records = []\n",
    "    for filename in files:\n",
    "        parsed = timit_file_to_dict(filename)\n",
    "        parsed['audio'] = zipped_wav_to_array(parsed['wav_filename'])\n",
    "        del parsed['wav_filename']\n",
    "        parsed['ipa'] = \"\".join(phoneme for phoneme, _, _ in parsed['timestamped_phonemes'])\n",
    "        parsed['phoneme_starts'] = [start for _, start, _ in parsed['timestamped_phonemes']]\n",
    "        parsed['phoneme_ends'] = [end for _, _, end in parsed['timestamped_phonemes']]\n",
    "        del parsed['timestamped_phonemes']\n",
    "        records.append(parsed)\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker_initials</th>\n",
       "      <th>transcription</th>\n",
       "      <th>transcription_start</th>\n",
       "      <th>transcription_end</th>\n",
       "      <th>timestamped_words</th>\n",
       "      <th>audio</th>\n",
       "      <th>ipa</th>\n",
       "      <th>phoneme_starts</th>\n",
       "      <th>phoneme_ends</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GRP0</td>\n",
       "      <td>Don't ask me to carry an oily rag like that.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.392000</td>\n",
       "      <td>[(don't, 0.1325, 0.29), (ask, 0.29, 0.5975), (...</td>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>oʊɾ̃æskmitɨkɪɹiæɾ̃ɔlɨɹæglaɪkðæt</td>\n",
       "      <td>[0.0, 0.1325, 0.1625, 0.27, 0.29, 0.4525, 0.53...</td>\n",
       "      <td>[0.1325, 0.1625, 0.27, 0.29, 0.4525, 0.5351875...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MBS0</td>\n",
       "      <td>Porcupines resemble sea urchins.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.982500</td>\n",
       "      <td>[(porcupines, 0.1525, 0.874625), (resemble, 0....</td>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>ɔɹkjʉpaɪnzɹizɛmbl̩ siʔɝttʃɨnz</td>\n",
       "      <td>[0.0, 0.1525, 0.1825, 0.245375, 0.2925, 0.3475...</td>\n",
       "      <td>[0.1525, 0.1825, 0.245375, 0.2925, 0.3475, 0.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SMS0</td>\n",
       "      <td>Insulate, weatherstrip, double-glaze to the ma...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.404812</td>\n",
       "      <td>[(insulate, 0.1425, 0.7775), (weatherstrip, 0....</td>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>ʔɪnsəleɪtwɛðɚstɹɪpʌbl̩ gleɪztɨðɨmæksəməm</td>\n",
       "      <td>[0.0, 0.1425, 0.188375, 0.2440625, 0.280625, 0...</td>\n",
       "      <td>[0.1425, 0.188375, 0.2440625, 0.280625, 0.3875...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STF0</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.456000</td>\n",
       "      <td>[(she, 0.136875, 0.3025), (had, 0.3025, 0.5725...</td>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>siɦæddʒɚdɑɹksʉtʔɨŋgɹisiwɑʃwɔɾɚʔɔljɪɝ</td>\n",
       "      <td>[0.0, 0.136875, 0.241875, 0.3025, 0.3425, 0.49...</td>\n",
       "      <td>[0.136875, 0.241875, 0.3025, 0.3425, 0.4975, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SMR0</td>\n",
       "      <td>The full moon shone brightly that night.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.316813</td>\n",
       "      <td>[(the, 0.1474375, 0.194375), (full, 0.194375, ...</td>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>ðɨfʊlmunʃoʊnbɹaɪʔliðætnaɪt</td>\n",
       "      <td>[0.0, 0.1474375, 0.1719375, 0.194375, 0.3275, ...</td>\n",
       "      <td>[0.1474375, 0.1719375, 0.194375, 0.3275, 0.383...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  speaker_initials                                      transcription  \\\n",
       "0             GRP0       Don't ask me to carry an oily rag like that.   \n",
       "1             MBS0                   Porcupines resemble sea urchins.   \n",
       "2             SMS0  Insulate, weatherstrip, double-glaze to the ma...   \n",
       "3             STF0  She had your dark suit in greasy wash water al...   \n",
       "4             SMR0           The full moon shone brightly that night.   \n",
       "\n",
       "   transcription_start  transcription_end  \\\n",
       "0                  0.0           3.392000   \n",
       "1                  0.0           2.982500   \n",
       "2                  0.0           3.404812   \n",
       "3                  0.0           3.456000   \n",
       "4                  0.0           2.316813   \n",
       "\n",
       "                                   timestamped_words  \\\n",
       "0  [(don't, 0.1325, 0.29), (ask, 0.29, 0.5975), (...   \n",
       "1  [(porcupines, 0.1525, 0.874625), (resemble, 0....   \n",
       "2  [(insulate, 0.1425, 0.7775), (weatherstrip, 0....   \n",
       "3  [(she, 0.136875, 0.3025), (had, 0.3025, 0.5725...   \n",
       "4  [(the, 0.1474375, 0.194375), (full, 0.194375, ...   \n",
       "\n",
       "                                               audio  \\\n",
       "0  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "1  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "2  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "3  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "4  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "\n",
       "                                        ipa  \\\n",
       "0           oʊɾ̃æskmitɨkɪɹiæɾ̃ɔlɨɹæglaɪkðæt   \n",
       "1             ɔɹkjʉpaɪnzɹizɛmbl̩ siʔɝttʃɨnz   \n",
       "2  ʔɪnsəleɪtwɛðɚstɹɪpʌbl̩ gleɪztɨðɨmæksəməm   \n",
       "3      siɦæddʒɚdɑɹksʉtʔɨŋgɹisiwɑʃwɔɾɚʔɔljɪɝ   \n",
       "4                ðɨfʊlmunʃoʊnbɹaɪʔliðætnaɪt   \n",
       "\n",
       "                                      phoneme_starts  \\\n",
       "0  [0.0, 0.1325, 0.1625, 0.27, 0.29, 0.4525, 0.53...   \n",
       "1  [0.0, 0.1525, 0.1825, 0.245375, 0.2925, 0.3475...   \n",
       "2  [0.0, 0.1425, 0.188375, 0.2440625, 0.280625, 0...   \n",
       "3  [0.0, 0.136875, 0.241875, 0.3025, 0.3425, 0.49...   \n",
       "4  [0.0, 0.1474375, 0.1719375, 0.194375, 0.3275, ...   \n",
       "\n",
       "                                        phoneme_ends  \n",
       "0  [0.1325, 0.1625, 0.27, 0.29, 0.4525, 0.5351875...  \n",
       "1  [0.1525, 0.1825, 0.245375, 0.2925, 0.3475, 0.3...  \n",
       "2  [0.1425, 0.188375, 0.2440625, 0.280625, 0.3875...  \n",
       "3  [0.136875, 0.241875, 0.3025, 0.3425, 0.4975, 0...  \n",
       "4  [0.1474375, 0.1719375, 0.194375, 0.3275, 0.383...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = files_to_df(training_files)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting vocabulary of TIMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_phoneme_vocabulary(train_df, phoneme_column):\n",
    "    \"\"\"\n",
    "    Extracts a unique vocabulary of phonemes from concatenated phoneme strings in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        train_df (pd.DataFrame): DataFrame containing phoneme sequences.\n",
    "        phoneme_column (str): Name of the column containing phoneme sequences.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of unique phonemes.\n",
    "    \"\"\"\n",
    "    phoneme_vocabulary = set()\n",
    "\n",
    "    for phoneme_sequence in train_df[phoneme_column]:\n",
    "        # Split the phoneme sequence into smaller phonemes\n",
    "        if isinstance(phoneme_sequence, str):\n",
    "            # Assuming phonemes are space-separated; otherwise adjust this\n",
    "            phonemes = list(phoneme_sequence.strip())  # Splits by each character\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected format in phoneme sequence: {phoneme_sequence}\")\n",
    "        \n",
    "        # Add each phoneme to the vocabulary\n",
    "        phoneme_vocabulary.update(phonemes)\n",
    "\n",
    "    return phoneme_vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoneme Vocabulary: {'b', ' ', 'ə', 'l̩', 'ŋ̍', 'ʔ', 'd', '̍', 'ʊ', 'θ', '̩', 'p', 'ð', '̃', 'a', 'g', 'e', 'ɨ', 'n', 'w', 'ʒ', 't', 'f', 'ɾ', 'ʉ', 'u', 'ɝ', 'ŋ', 'h', 'ɾ̃', 'm', 's', 'v', 'm̩', 'dʒ', 'l', 'ɪ', 'tʃ', 'eɪ', 'o', 'ɹ', 'ʌ', 'z', '̥', 'k', 'aɪ', 'aʊ', 'ɔɪ', 'æ', 'ɔ', 'ɛ', 'n̩', 'ʃ', 'ɦ', 'ə̥', 'oʊ', 'ɚ', 'j', 'i', 'ɑ'}\n",
      "Length of Vocab:  60\n"
     ]
    }
   ],
   "source": [
    "timit_phoneme_vocab = extract_phoneme_vocabulary(train_df, 'ipa')\n",
    "# Update: unfortunately this function fails to parse diphones so we will manually add them to the vocab: \n",
    "phonemes_to_add = {\"aɪ\", \"aʊ\", \"dʒ\", \"eɪ\", \"l̩\", \"m̩\", \"n̩\", \"oʊ\", \"tʃ\", \"ŋ̍\", \"ɔɪ\", \"ə̥\", \"ɾ̃\"}\n",
    "timit_phoneme_vocab.update(phonemes_to_add)\n",
    "\n",
    "print(\"Phoneme Vocabulary:\", timit_phoneme_vocab)\n",
    "print(\"Length of Vocab: \", len(timit_phoneme_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Creating Timit Files for Allosaurus\n",
    "File 1: Wave file (titled WAVE no file extension) with \\<utt_id\\> \\<full path to .wav\\>\n",
    "\n",
    "File 2: Phoneme file (titled TEXT no file extension) with \\<utt_id> \\<phoneme sequence\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def create_wav_and_phoneme_files(wav_files, wav_output, phoneme_output):\n",
    "    \"\"\"\n",
    "    Creates two files: one for WAV paths and another for phoneme transcriptions.\n",
    "\n",
    "    Args:\n",
    "        wav_files (list): List of paths to WAV files.\n",
    "        wav_output (str): Path to save the WAV file list.\n",
    "        phoneme_output (str): Path to save the phoneme transcription file.\n",
    "    \"\"\"\n",
    "    wav_records = []\n",
    "    phoneme_records = []\n",
    "\n",
    "    for wav_path in wav_files:\n",
    "        parsed = timit_file_to_dict(wav_path)  \n",
    "        # Extract unique utt_id from the filename\n",
    "        utt_id = os.path.splitext(os.path.basename(wav_path))[0]\n",
    "        phoneme_sequence = [phoneme for phoneme, _, _ in parsed['timestamped_phonemes']]\n",
    "\n",
    "        # Clean up the phoneme sequence\n",
    "        phoneme_sequence_cleaned = \" \".join(phoneme_sequence)  # Join the phonemes with spaces\n",
    "\n",
    "        # Extract the parent directory of the script\n",
    "        parent_dir = os.path.dirname(os.path.abspath(wav_output))  # Parent directory where the output will be saved\n",
    "\n",
    "        # Convert the relative path to absolute and add .WAV.wav\n",
    "        full_wav_path = os.path.join(parent_dir, wav_path) + \".WAV.wav\"\n",
    "\n",
    "        # Add entry for the WAV file list\n",
    "        wav_records.append((utt_id, full_wav_path))\n",
    "        \n",
    "        # Add entry for the phoneme file list\n",
    "        phoneme_records.append((utt_id, phoneme_sequence_cleaned))\n",
    "\n",
    "    # Save the WAV file list\n",
    "    wav_df = pd.DataFrame(wav_records, columns=[\"utt_id\", \"file_path\"])\n",
    "    wav_df.to_csv(wav_output, sep=\" \", index=False, header=False)\n",
    "\n",
    "    # Save the phoneme file list\n",
    "    with open(phoneme_output, \"w\") as phoneme_file:\n",
    "        for utt_id, phoneme_sequence_cleaned in phoneme_records:\n",
    "            phoneme_file.write(f\"{utt_id} {phoneme_sequence_cleaned}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <TODO> create a allosaurus directory containing a train directory, validate directory, and your audio/wave.txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "create_wav_and_phoneme_files(training_files, \"../data/allosaurus_data/audio.txt\", \"../data/allosaurus_data/phoneme.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want test files to be created, you can use the following code: (make sure to create a test dir first)\n",
    " # create_wav_and_phoneme_files(test_files, \"../data/allosaurus_data/test/audio.txt\", \"../data/allosaurus_data/test/phoneme.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Splitting for Train/Val\n",
    "Just split those first two files into 4 files for train/val\n",
    "\n",
    "We will do a 80-20 split for the train-val files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split_txt_files(wav_file, phoneme_file, train_percentage=0.8):\n",
    "    \"\"\"\n",
    "    Splits wav and phoneme TXT files into train and validation subsets.\n",
    "\n",
    "    Args:\n",
    "        wav_file (str): Path to the wav TXT file (utt_id and file paths).\n",
    "        phoneme_file (str): Path to the phoneme TXT file (utt_id and transcriptions).\n",
    "        train_percentage (float): Percentage of data for training (default: 80%).\n",
    "\n",
    "    Outputs:\n",
    "        Four files: train_wav.txt, val_wav.txt, train_phoneme.txt, val_phoneme.txt\n",
    "    \"\"\"\n",
    "    # Read the wav file and phoneme file\n",
    "    with open(wav_file, \"r\") as f:\n",
    "        wav_lines = f.readlines()\n",
    "\n",
    "    with open(phoneme_file, \"r\") as f:\n",
    "        phoneme_lines = f.readlines()\n",
    "\n",
    "    assert len(wav_lines) == len(phoneme_lines), \"Mismatch in wav and phoneme file lengths!\"\n",
    "    # Verify that the utterance IDs match for every line\n",
    "    for wav_line, phoneme_line in zip(wav_lines, phoneme_lines):    \n",
    "        # Extract utterance ID from the WAV line\n",
    "        wav_utt_id = wav_line.split()[0]  # Assumes the utterance ID is the first token\n",
    "        # Extract utterance ID from the phoneme line\n",
    "        phoneme_utt_id = phoneme_line.split()[0]  # Assumes the utterance ID is the first token\n",
    "\n",
    "        # Assert that the IDs are equal\n",
    "        assert wav_utt_id == phoneme_utt_id, f\"Mismatch in utterance IDs: {wav_utt_id} vs {phoneme_utt_id}\"\n",
    "    \n",
    "    # Pair up lines to maintain utt_id consistency\n",
    "    combined = list(zip(wav_lines, phoneme_lines))\n",
    "    random.shuffle(combined)  # Shuffle the data for randomness\n",
    "\n",
    "    # Split into train and validation subsets\n",
    "    split_idx = int(len(combined) * train_percentage)\n",
    "    train_data = combined[:split_idx]\n",
    "    val_data = combined[split_idx:]\n",
    "\n",
    "    # Unpack the train and validation data\n",
    "    train_wav, train_phoneme = zip(*train_data)\n",
    "    val_wav, val_phoneme = zip(*val_data)\n",
    "\n",
    "    # Save the train and validation splits\n",
    "    with open(\"../data/allosaurus_data/train/wave\", \"w\") as f:\n",
    "        f.writelines(train_wav)\n",
    "\n",
    "    with open(\"../data/allosaurus_data/validate/wave\", \"w\") as f:\n",
    "        f.writelines(val_wav)\n",
    "\n",
    "    with open(\"../data/allosaurus_data/train/text\", \"w\") as f:\n",
    "        f.writelines(train_phoneme)\n",
    "\n",
    "    with open(\"../data/allosaurus_data/validate/text\", \"w\") as f:\n",
    "        f.writelines(val_phoneme)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_txt_files(\"../data/allosaurus_data/audio.txt\", \"../data/allosaurus_data/phoneme.txt\", train_percentage=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Check the difference between Allosaurus and TIMIT\n",
    "We will need to ensure that the Allosaurus vocab aligns to TIMIT's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbols in Allosaurus but not in TIMIT:\n",
      "['aː', 'd̠', 'eː', 'e̞', 'iː', 'kʰ', 'oː', 'pʰ', 'r', 'tʰ', 't̠', 'uː', 'x', 'øː', 'ɐ', 'ɐː', 'ɑː', 'ɒ', 'ɒː', 'ɔː', 'ɘ', 'əː', 'ɛː', 'ɜː', 'ɡ', 'ɪ̯', 'ɯ', 'ɵː', 'ɻ', 'ʉː', 'ʍ']\n",
      "\n",
      "Symbols in TIMIT but not in Allosaurus:\n",
      "[' ', 'aɪ', 'aʊ', 'dʒ', 'eɪ', 'g', 'l̩', 'm̩', 'n̩', 'oʊ', 'tʃ', 'ŋ̍', 'ɔɪ', 'ə̥', 'ɚ', 'ɝ', 'ɦ', 'ɨ', 'ɾ', 'ɾ̃', '̃', '̍', '̥', '̩']\n"
     ]
    }
   ],
   "source": [
    "# Allosaurus vocabulary\n",
    "allosaurus_vocab = {\n",
    "    \"a\", \"aː\", \"b\", \"d\", \"d̠\", \"e\", \"eː\", \"e̞\", \"f\", \"h\", \"i\", \"iː\", \"j\", \"k\", \"kʰ\", \"l\", \n",
    "    \"m\", \"n\", \"o\", \"oː\", \"p\", \"pʰ\", \"r\", \"s\", \"t\", \"tʰ\", \"t̠\", \"u\", \"uː\", \"v\", \"w\", \"x\", \n",
    "    \"z\", \"æ\", \"ð\", \"øː\", \"ŋ\", \"ɐ\", \"ɐː\", \"ɑ\", \"ɑː\", \"ɒ\", \"ɒː\", \"ɔ\", \"ɔː\", \"ɘ\", \"ə\", \n",
    "    \"əː\", \"ɛ\", \"ɛː\", \"ɜː\", \"ɡ\", \"ɪ\", \"ɪ̯\", \"ɯ\", \"ɵː\", \"ɹ\", \"ɻ\", \"ʃ\", \"ʉ\", \"ʉː\", \"ʊ\", \n",
    "    \"ʌ\", \"ʍ\", \"ʒ\", \"ʔ\", \"θ\"\n",
    "}\n",
    "# Note: you can see they have aspirated t and plosives that might not be needed, you can adjust there probabilities if needed\n",
    "\n",
    "# Compute differences\n",
    "allosaurus_only = allosaurus_vocab - timit_phoneme_vocab\n",
    "timit_only = timit_phoneme_vocab - allosaurus_vocab\n",
    "\n",
    "# Print results\n",
    "print(\"Symbols in Allosaurus but not in TIMIT:\")\n",
    "print(sorted(allosaurus_only))\n",
    "\n",
    "print(\"\\nSymbols in TIMIT but not in Allosaurus:\")\n",
    "print(sorted(timit_only))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the Allosaurus vocab accordingly so it has the full TIMIT vocab\n",
    "\n",
    "On each new line in the file add these phonemes: [' ', 'aɪ', 'aʊ', 'dʒ', 'eɪ', 'g', 'l̩', 'm̩', 'n̩', 'oʊ', 'tʃ', 'ŋ̍', 'ɔɪ', 'ə̥', 'ɚ', 'ɝ', 'ɦ', 'ɨ', 'ɾ', 'ɾ̃', '̃', '̍', '̥', '̩']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the vocab, run this or just make the file yourself\n",
    "!python -m allosaurus.bin.write_phone --lang eng --output ./data/allosaurus_data/allosaurus_eng_inventory.txt\n",
    "# Add the missing phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of phonemes\n",
    "phonemes = sorted(timit_only)\n",
    "\n",
    "# File path\n",
    "file_path = './data/allosaurus_data/allosaurus_eng_inventory.txt'\n",
    "\n",
    "# Read the existing file content into a set of phonemes\n",
    "with open(file_path, 'r') as file:\n",
    "    existing_phonemes = set(file.read().splitlines())\n",
    "\n",
    "# Write new phonemes to the file, only if they are not already present\n",
    "with open(file_path, 'a') as file:\n",
    "    for phoneme in phonemes:\n",
    "        if phoneme not in existing_phonemes:\n",
    "            file.write(f\"{phoneme}\\n\")\n",
    "            existing_phonemes.add(phoneme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocab \n",
    "!python -m allosaurus.bin.update_phone --lang eng --input ./data/allosaurus_data/allosaurus_eng_inventory.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a aː b d d̠ e eː e̞ f h i iː j k kʰ l m n o oː p pʰ r s t tʰ t̠ u uː v w x z æ ð øː ŋ ɐ ɐː ɑ ɑː ɒ ɒː ɔ ɔː ɘ ə əː ɛ ɛː ɜː ɡ ɪ ɪ̯ ɯ ɵː ɹ ɻ ʃ ʉ ʉː ʊ ʌ ʍ ʒ ʔ θ aɪ aʊ dʒ eɪ g l̩ m̩ n̩ oʊ tʃ ŋ̍ ɔɪ ə̥ ɚ ɝ ɦ ɨ ɾ ɾ̃ ̃ ̍ ̥ ̩\n"
     ]
    }
   ],
   "source": [
    "!python -m allosaurus.bin.list_phone --lang eng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Unzip TIMIT files to extract audio features from WAVE path\n",
    "This is not the best way to extract, but its quick. Just delete the timit files after extracting the features and tokenizing \n",
    "\n",
    "Make sure the TIMIT directory is the same as the file paths provided in the audio.txt/WAVE file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Path to your TIMIT ZIP file\n",
    "# zip_path = \"./.data/TIMIT.zip\"\n",
    "# extract_to = \"../data/allosaurus_data/\"  # Where to extract\n",
    "\n",
    "# # Ensure the ZIP file exists\n",
    "# assert Path(zip_path).exists(), f\"{zip_path} does not exist!\"\n",
    "\n",
    "# # Extract the TIMIT data\n",
    "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(extract_to)\n",
    "\n",
    "# print(f\"Extracted {zip_path} to {extract_to}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 3696/3696 [00:00<00:00, 58419.75it/s]\n",
      "100%|██████████████████████████████████████| 924/924 [00:00<00:00, 58090.07it/s]\n",
      "100%|███████████████████████████████████████| 3696/3696 [07:39<00:00,  8.04it/s]\n",
      "100%|█████████████████████████████████████████| 924/924 [01:49<00:00,  8.46it/s]\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Create audio features\n",
    "!python -m allosaurus.bin.prep_token --model=uni2005 --lang=eng --path=/home/arunasrivastava/ML/data/allosaurus_data/train\n",
    "!python -m allosaurus.bin.prep_token --model=uni2005 --lang=eng --path=/home/arunasrivastava/ML/data/allosaurus_data/validate\n",
    "# Create text features \n",
    "!python -m allosaurus.bin.prep_feat --model=uni2005 --path=/home/arunasrivastava/ML/data/allosaurus_data/train\n",
    "!python -m allosaurus.bin.prep_feat --model=uni2005 --path=/home/arunasrivastava/ML/data/allosaurus_data/validate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/allosaurus/am/utils.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(str(path), map_location=torch.device('cpu'))\n",
      "epoch[batch]: 00[0000] | train loss 0.70162 train per 0.20244\n",
      "epoch[batch]: 00[0010] | train loss 0.63866 train per 0.18673\n",
      "epoch[batch]: 00[0020] | train loss 0.61282 train per 0.17973\n",
      "epoch0 | validate per : 0.21897\n",
      "saving model\n",
      "epoch[batch]: 01[0000] | train loss 0.53440 train per 0.16050\n",
      "epoch[batch]: 01[0010] | train loss 0.61022 train per 0.18236\n",
      "epoch[batch]: 01[0020] | train loss 0.58034 train per 0.17165\n",
      "epoch1 | validate per : 0.21381\n",
      "saving model\n",
      "epoch[batch]: 02[0000] | train loss 0.63332 train per 0.19220\n",
      "epoch[batch]: 02[0010] | train loss 0.54775 train per 0.16100\n",
      "epoch[batch]: 02[0020] | train loss 0.56646 train per 0.17035\n",
      "epoch2 | validate per : 0.21035\n",
      "saving model\n",
      "epoch[batch]: 03[0000] | train loss 0.51749 train per 0.16041\n",
      "epoch[batch]: 03[0010] | train loss 0.53707 train per 0.15972\n",
      "epoch[batch]: 03[0020] | train loss 0.52731 train per 0.15880\n",
      "epoch3 | validate per : 0.20825\n",
      "saving model\n",
      "epoch[batch]: 04[0000] | train loss 0.53565 train per 0.15769\n",
      "epoch[batch]: 04[0010] | train loss 0.49754 train per 0.14742\n",
      "epoch[batch]: 04[0020] | train loss 0.50039 train per 0.15255\n",
      "epoch4 | validate per : 0.20429\n",
      "saving model\n",
      "epoch[batch]: 05[0000] | train loss 0.47032 train per 0.14704\n",
      "epoch[batch]: 05[0010] | train loss 0.50248 train per 0.15011\n",
      "epoch[batch]: 05[0020] | train loss 0.45973 train per 0.14018\n",
      "epoch5 | validate per : 0.20525\n",
      "epoch[batch]: 06[0000] | train loss 0.40506 train per 0.12408\n",
      "epoch[batch]: 06[0010] | train loss 0.45983 train per 0.13808\n",
      "epoch[batch]: 06[0020] | train loss 0.46908 train per 0.14406\n",
      "epoch6 | validate per : 0.20389\n",
      "saving model\n",
      "epoch[batch]: 07[0000] | train loss 0.50190 train per 0.15318\n",
      "epoch[batch]: 07[0010] | train loss 0.44364 train per 0.13518\n",
      "epoch[batch]: 07[0020] | train loss 0.43289 train per 0.12951\n",
      "epoch7 | validate per : 0.20168\n",
      "saving model\n",
      "epoch[batch]: 08[0000] | train loss 0.36800 train per 0.10222\n",
      "epoch[batch]: 08[0010] | train loss 0.42509 train per 0.12779\n",
      "epoch[batch]: 08[0020] | train loss 0.41498 train per 0.12564\n",
      "epoch8 | validate per : 0.20054\n",
      "saving model\n",
      "epoch[batch]: 09[0000] | train loss 0.40825 train per 0.12240\n",
      "epoch[batch]: 09[0010] | train loss 0.41080 train per 0.12531\n",
      "epoch[batch]: 09[0020] | train loss 0.39928 train per 0.12114\n",
      "epoch9 | validate per : 0.19992\n",
      "saving model\n",
      "epoch[batch]: 10[0000] | train loss 0.32217 train per 0.09223\n",
      "epoch[batch]: 10[0010] | train loss 0.38217 train per 0.11600\n",
      "epoch[batch]: 10[0020] | train loss 0.39901 train per 0.12160\n",
      "epoch10 | validate per : 0.19879\n",
      "saving model\n",
      "epoch[batch]: 11[0000] | train loss 0.39310 train per 0.11618\n",
      "epoch[batch]: 11[0010] | train loss 0.36812 train per 0.10869\n",
      "epoch[batch]: 11[0020] | train loss 0.37210 train per 0.11443\n",
      "epoch11 | validate per : 0.20015\n",
      "epoch[batch]: 12[0000] | train loss 0.38550 train per 0.11111\n",
      "epoch[batch]: 12[0010] | train loss 0.34055 train per 0.10201\n",
      "epoch[batch]: 12[0020] | train loss 0.37055 train per 0.11292\n",
      "epoch12 | validate per : 0.19930\n",
      "epoch[batch]: 13[0000] | train loss 0.38109 train per 0.11734\n",
      "epoch[batch]: 13[0010] | train loss 0.34200 train per 0.10174\n",
      "epoch[batch]: 13[0020] | train loss 0.33543 train per 0.10015\n",
      "epoch13 | validate per : 0.19839\n",
      "saving model\n",
      "epoch[batch]: 14[0000] | train loss 0.38286 train per 0.12049\n",
      "epoch[batch]: 14[0010] | train loss 0.31212 train per 0.09154\n",
      "epoch[batch]: 14[0020] | train loss 0.33687 train per 0.09834\n",
      "epoch14 | validate per : 0.19833\n",
      "saving model\n",
      "epoch[batch]: 15[0000] | train loss 0.26166 train per 0.07114\n",
      "epoch[batch]: 15[0010] | train loss 0.30683 train per 0.08902\n",
      "epoch[batch]: 15[0020] | train loss 0.31865 train per 0.09474\n",
      "epoch15 | validate per : 0.19788\n",
      "saving model\n",
      "epoch[batch]: 16[0000] | train loss 0.25337 train per 0.06813\n",
      "epoch[batch]: 16[0010] | train loss 0.30395 train per 0.08706\n",
      "epoch[batch]: 16[0020] | train loss 0.30223 train per 0.08975\n",
      "epoch16 | validate per : 0.19816\n",
      "epoch[batch]: 17[0000] | train loss 0.23355 train per 0.06198\n",
      "epoch[batch]: 17[0010] | train loss 0.29243 train per 0.08315\n",
      "epoch[batch]: 17[0020] | train loss 0.28627 train per 0.08298\n",
      "epoch17 | validate per : 0.19828\n",
      "epoch[batch]: 18[0000] | train loss 0.31438 train per 0.08522\n",
      "epoch[batch]: 18[0010] | train loss 0.25958 train per 0.07214\n",
      "epoch[batch]: 18[0020] | train loss 0.27863 train per 0.07950\n",
      "epoch18 | validate per : 0.19714\n",
      "saving model\n",
      "epoch[batch]: 19[0000] | train loss 0.29910 train per 0.08721\n",
      "epoch[batch]: 19[0010] | train loss 0.27614 train per 0.07611\n",
      "epoch[batch]: 19[0020] | train loss 0.25307 train per 0.07057\n",
      "epoch19 | validate per : 0.19760\n"
     ]
    }
   ],
   "source": [
    "# training the model: (im using english universal model)\n",
    "!python -m allosaurus.bin.adapt_model --pretrained_model uni25a --new_model uni25b --path /home/arunasrivastava/ML/data/allosaurus_data --lang eng --device_id -1 --epoch 20 --log /home/arunasrivastava/ML/data/allosaurus_data/log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/allosaurus/am/utils.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(str(path), map_location=torch.device('cpu'))\n",
      "ʔ aɪ ɹ eɪ t ʔ æ k ɚ s t ʌ m ð ə w eɪ ʔ ɨ ɾ i æ ɾ ɨ k l i\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# to test the model\n",
    "# python -m allosaurus.run [--lang <language name>] [--model <model name>] [--device_id <gpu_id>] [--output <output_file>] [--topk <int>] -i <audio file/directory>\n",
    "!python -m allosaurus.run --lang eng --model uni25a -i /home/arunasrivastava/ML/data/ExamplesWithComments/TIMIT_sample_0.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('..')\n",
    "from scripts.eval_tests.panphon_model_eval import panphon_model_eval "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Sample Evaluations\n",
    "We will use a timit sample and see how it goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ʔ aɪ ɹ eɪ t ʔ æ k ɚ s t ʌ m ð ə w eɪ ʔ ɨ ɾ i æ ɾ ɨ k l i\n",
      "Evaluation Results:\n",
      "Feature edit distance: 1.6136363636363635\n",
      "Weighted feature edit distance: 11.625\n",
      "Hamming distance: 1.7727272727272727\n",
      "CER: 0.967741935483871\n"
     ]
    }
   ],
   "source": [
    "# We can do a baby evaluation on this model by running the following command\n",
    "# Call panphon_model_eval with label and predictedipa\n",
    "pred_str = \"ʔ aɪ ɹ eɪ t ʔ æ k ɚ s t ʌ m ð ə w eɪ ʔ ɨ ɾ i æ ɾ ɨ k l i\" # <TODO> replace this with the predicted ipa string from above\n",
    "print(pred_str)\n",
    "label_str = \"ðɨaɪɹeɪtʔækɚstɑmpəweɪʔɨɾiɑɾɨkli\"\n",
    "results = panphon_model_eval(label_str, pred_str)\n",
    "\n",
    "# Output results\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"Feature edit distance: {results['feature_dist']}\")\n",
    "print(f\"Weighted feature edit distance: {results['weighted_feature_dist']}\")\n",
    "print(f\"Hamming distance: {results['hamming_feature_dist']}\")\n",
    "print(f\"CER: {results['cer_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # non required options\n",
    "    parser.add_argument('--batch_frame_size', type=int,   default=6000,  help='this indicates how many frame in each batch, if you get any memory related errors, please use a lower value for this size')\n",
    "    parser.add_argument('--criterion',        type=str,   default='ctc', choices=['ctc'], help='criterion, only ctc now')\n",
    "    parser.add_argument('--optimizer',        type=str,   default='sgd', choices=['sgd'], help='optimizer, only sgd now')\n",
    "    parser.add_argument('--lr',               type=float, default=0.01,  help='learning rate')\n",
    "    parser.add_argument('--grad_clip',        type=float, default=5.0,   help='grad clipping')\n",
    "    parser.add_argument('--epoch',            type=int,   default=10,    help='number of epoch to run')\n",
    "    parser.add_argument('--log',              type=str,   default='none',help='file to store training logs. do not save if none')\n",
    "    parser.add_argument('--verbose',          type=bool,  default=True,  help='print all training logs on stdout')\n",
    "    parser.add_argument('--report_per_batch', type=int,   default=10,    help='report training stats every N epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model on the TIMIT test\n",
    "We will use cer and a phonemic distance calculation score that is averaged across all test pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
