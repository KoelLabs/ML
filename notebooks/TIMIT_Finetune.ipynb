{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import zipfile\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "# run as much as possible accelerated by apple silicon, fall back to cpu if not possible\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from scripts.audio import audio_file_to_array\n",
    "from scripts.ipa import timit2ipa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set espeak library path for macOS\n",
    "if sys.platform == \"darwin\":\n",
    "    from phonemizer.backend.espeak.wrapper import EspeakWrapper\n",
    "\n",
    "    _ESPEAK_LIBRARY = \"/opt/homebrew/Cellar/espeak/1.48.04_1/lib/libespeak.1.1.48.dylib\"\n",
    "    EspeakWrapper.set_library(_ESPEAK_LIBRARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCTC, AutoTokenizer, AutoFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = os.path.join('..', 'models', 'timit-xlsr-finetune')\n",
    "PRE_TRAINED_ID = \"facebook/wav2vec2-lv-60-espeak-cv-ft\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit = zipfile.ZipFile('../.data/TIMIT.zip', 'r')\n",
    "timit_files = timit.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = list(set(map(lambda x: x.split('.')[0], filter(lambda x: x.startswith('data/TRAIN'), timit_files))))\n",
    "test_files = list(set(map(lambda x: x.split('.')[0], filter(lambda x: x.startswith('data/TEST'), timit_files))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timit_file_to_dict(filename):\n",
    "    with timit.open(filename + '.PHN') as phn_file:\n",
    "        phonemes = []\n",
    "        for line in phn_file.read().decode('utf-8').split('\\n'):\n",
    "            if line == '':\n",
    "                continue\n",
    "            _, _, phoneme = line.split()\n",
    "            phonemes.append(timit2ipa(phoneme))\n",
    "    return {'phonemes': phonemes, 'wav_filename': filename + '.WAV'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def timit_file_to_dict(filename):\n",
    "    with timit.open(filename + '.PHN') as phn_file:\n",
    "        timestamped_phonemes = []\n",
    "        for line in phn_file.read().decode('utf-8').split('\\n'):\n",
    "            if line == '':\n",
    "                continue\n",
    "            start, end, phoneme = line.split()\n",
    "            timestamped_phonemes.append((timit2ipa(phoneme, \"eng\"), int(start), int(end)))\n",
    "    return {'timestamped_phonemes': timestamped_phonemes, 'wav_filename': filename + '.WAV'}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_HEADER_SIZE = 44\n",
    "def zipped_wav_to_array(filename):\n",
    "    with timit.open(filename) as wav_file:\n",
    "        return np.frombuffer(wav_file.read(), dtype=np.int16)[WAV_HEADER_SIZE//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timestamped_phonemes': [('', 0, 2879), ('h', 2879, 3910), ('aʊ', 3910, 4995), ('p', 4995, 6080), ('', 6080, 6690), ('ɝ', 6690, 7960), ('m', 7960, 8772), ('ɨ', 8772, 9315), ('ɾ̃', 9315, 9608), ('ɛ', 9608, 10840), ('n', 10840, 11640), ('ɑ', 11640, 13785), ('ð', 13785, 14590), ('ɛ', 14590, 16041), ('ɹ', 16041, 17545), ('ɛ', 17545, 18972), ('k', 18972, 20260), ('', 20260, 20820), ('ɨ', 20820, 21960), ('d', 21960, 23225), ('', 23225, 23520), ('z', 23520, 25650), ('', 25650, 27680)], 'wav_filename': 'data/TRAIN/DR1/FCJF0/SX217.WAV'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parsed_0 = timit_file_to_dict(training_files[0])\n",
    "print(parsed_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_to_df(files):\n",
    "    records = []\n",
    "    for filename in files:\n",
    "        parsed = timit_file_to_dict(filename)\n",
    "        parsed['audio'] = zipped_wav_to_array(parsed['wav_filename'])\n",
    "        del parsed['wav_filename']\n",
    "        parsed['phonemes'] = [phoneme for phoneme, _, _ in parsed['timestamped_phonemes']]\n",
    "        parsed['phoneme_starts'] = [start for _, start, _ in parsed['timestamped_phonemes']]\n",
    "        parsed['phoneme_ends'] = [end for _, _, end in parsed['timestamped_phonemes']]\n",
    "        del parsed['timestamped_phonemes']\n",
    "        records.append(parsed)\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>phonemes</th>\n",
       "      <th>phoneme_starts</th>\n",
       "      <th>phoneme_ends</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>[, h, aʊ, p, , ɝ, m, ɨ, ɾ̃, ɛ, n, ɑ, ð, ɛ, ɹ, ...</td>\n",
       "      <td>[0, 2879, 3910, 4995, 6080, 6690, 7960, 8772, ...</td>\n",
       "      <td>[2879, 3910, 4995, 6080, 6690, 7960, 8772, 931...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>[, h, ɨ, s, ʌ, d, n̩, d, , ɨ, p, , ɑ, ɹ, t, tʃ...</td>\n",
       "      <td>[0, 2280, 3300, 4320, 7080, 8640, 9190, 10120,...</td>\n",
       "      <td>[2280, 3300, 4320, 7080, 8640, 9190, 10120, 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>[, j, ɛ, s, ɪ, n, d, , i, d, , ɨ, w, i, t, , ʉ...</td>\n",
       "      <td>[0, 2826, 3383, 4678, 6475, 7227, 8218, 8663, ...</td>\n",
       "      <td>[2826, 3383, 4678, 6475, 7227, 8218, 8663, 885...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>[, s, oʊ, ɨ, ŋ, b, , ɹ, ɪ, ŋ, z, n, ʌ, m, n, ɨ...</td>\n",
       "      <td>[0, 3000, 5240, 7018, 8020, 9240, 10120, 10240...</td>\n",
       "      <td>[3000, 5240, 7018, 8020, 9240, 10120, 10240, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>[, w, aɪ, ɾ, ɝ, d, , ɨ, s, k, , ɹ, ɛ, ʃ, ɨ, ɾ̃...</td>\n",
       "      <td>[0, 2240, 3053, 4680, 5080, 6600, 7289, 7494, ...</td>\n",
       "      <td>[2240, 3053, 4680, 5080, 6600, 7289, 7494, 783...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               audio  \\\n",
       "0  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "1  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "2  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "3  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "4  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "\n",
       "                                            phonemes  \\\n",
       "0  [, h, aʊ, p, , ɝ, m, ɨ, ɾ̃, ɛ, n, ɑ, ð, ɛ, ɹ, ...   \n",
       "1  [, h, ɨ, s, ʌ, d, n̩, d, , ɨ, p, , ɑ, ɹ, t, tʃ...   \n",
       "2  [, j, ɛ, s, ɪ, n, d, , i, d, , ɨ, w, i, t, , ʉ...   \n",
       "3  [, s, oʊ, ɨ, ŋ, b, , ɹ, ɪ, ŋ, z, n, ʌ, m, n, ɨ...   \n",
       "4  [, w, aɪ, ɾ, ɝ, d, , ɨ, s, k, , ɹ, ɛ, ʃ, ɨ, ɾ̃...   \n",
       "\n",
       "                                      phoneme_starts  \\\n",
       "0  [0, 2879, 3910, 4995, 6080, 6690, 7960, 8772, ...   \n",
       "1  [0, 2280, 3300, 4320, 7080, 8640, 9190, 10120,...   \n",
       "2  [0, 2826, 3383, 4678, 6475, 7227, 8218, 8663, ...   \n",
       "3  [0, 3000, 5240, 7018, 8020, 9240, 10120, 10240...   \n",
       "4  [0, 2240, 3053, 4680, 5080, 6600, 7289, 7494, ...   \n",
       "\n",
       "                                        phoneme_ends  \n",
       "0  [2879, 3910, 4995, 6080, 6690, 7960, 8772, 931...  \n",
       "1  [2280, 3300, 4320, 7080, 8640, 9190, 10120, 11...  \n",
       "2  [2826, 3383, 4678, 6475, 7227, 8218, 8663, 885...  \n",
       "3  [3000, 5240, 7018, 8020, 9240, 10120, 10240, 1...  \n",
       "4  [2240, 3053, 4680, 5080, 6600, 7289, 7494, 783...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = files_to_df(training_files)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>phonemes</th>\n",
       "      <th>phoneme_starts</th>\n",
       "      <th>phoneme_ends</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>[, w, ɨ, n, i, s, ɑ, m, i, k, , ʌ, m, ɨ, ŋ, ɦ,...</td>\n",
       "      <td>[0, 2263, 2840, 3312, 3880, 4910, 6952, 9080, ...</td>\n",
       "      <td>[2263, 2840, 3312, 3880, 4910, 6952, 9080, 101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>[, ʃ, i, h, æ, d, , j, ɚ, d, , ɑ, ɹ, k, , s, ʉ...</td>\n",
       "      <td>[0, 2206, 4296, 5640, 7445, 9138, 10060, 10520...</td>\n",
       "      <td>[2206, 4296, 5640, 7445, 9138, 10060, 10520, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>[, ʔ, aɪ, d, ɹ, aɪ, d, ð, ɨ, s, ʌ, b, w, eɪ, ,...</td>\n",
       "      <td>[0, 2840, 3358, 5080, 5848, 6600, 9393, 10022,...</td>\n",
       "      <td>[2840, 3358, 5080, 5848, 6600, 9393, 10022, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>[, j, ʉ, t, , ʉ, w, ɝ, ɨ, s, t, , ʉ, p, , ɨ, d...</td>\n",
       "      <td>[0, 3571, 4600, 5880, 6760, 8040, 8920, 10569,...</td>\n",
       "      <td>[3571, 4600, 5880, 6760, 8040, 8920, 10569, 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>[, oʊ, n, l, i, ð, oʊ, s, t, , ɔ, ɹ, i, t, , ɛ...</td>\n",
       "      <td>[0, 4120, 5849, 6600, 7240, 8520, 9400, 12520,...</td>\n",
       "      <td>[4120, 5849, 6600, 7240, 8520, 9400, 12520, 14...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               audio  \\\n",
       "0  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "1  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "2  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "3  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "4  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "\n",
       "                                            phonemes  \\\n",
       "0  [, w, ɨ, n, i, s, ɑ, m, i, k, , ʌ, m, ɨ, ŋ, ɦ,...   \n",
       "1  [, ʃ, i, h, æ, d, , j, ɚ, d, , ɑ, ɹ, k, , s, ʉ...   \n",
       "2  [, ʔ, aɪ, d, ɹ, aɪ, d, ð, ɨ, s, ʌ, b, w, eɪ, ,...   \n",
       "3  [, j, ʉ, t, , ʉ, w, ɝ, ɨ, s, t, , ʉ, p, , ɨ, d...   \n",
       "4  [, oʊ, n, l, i, ð, oʊ, s, t, , ɔ, ɹ, i, t, , ɛ...   \n",
       "\n",
       "                                      phoneme_starts  \\\n",
       "0  [0, 2263, 2840, 3312, 3880, 4910, 6952, 9080, ...   \n",
       "1  [0, 2206, 4296, 5640, 7445, 9138, 10060, 10520...   \n",
       "2  [0, 2840, 3358, 5080, 5848, 6600, 9393, 10022,...   \n",
       "3  [0, 3571, 4600, 5880, 6760, 8040, 8920, 10569,...   \n",
       "4  [0, 4120, 5849, 6600, 7240, 8520, 9400, 12520,...   \n",
       "\n",
       "                                        phoneme_ends  \n",
       "0  [2263, 2840, 3312, 3880, 4910, 6952, 9080, 101...  \n",
       "1  [2206, 4296, 5640, 7445, 9138, 10060, 10520, 1...  \n",
       "2  [2840, 3358, 5080, 5848, 6600, 9393, 10022, 10...  \n",
       "3  [3571, 4600, 5880, 6760, 8040, 8920, 10569, 12...  \n",
       "4  [4120, 5849, 6600, 7240, 8520, 9400, 12520, 14...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = files_to_df(test_files)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Up Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# graph one audio to see start signal that we will remove\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_df['audio'][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to crop audio to remove start signal at first silence end timestamp\n",
    "def crop_audio(df, sr=16000):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend Phoneme Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_vocab = set(\"\".join(train_df['ipa'])) | set(\"\".join(test_df['ipa']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_ID)\n",
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'̥', 'ɝ', '̃', '̩', 'g', 'ɦ', '̍'}\n"
     ]
    }
   ],
   "source": [
    "additional_vocab = timit_vocab.difference(set(vocab.keys()) | {' '})\n",
    "tokenizer.add_tokens(list(additional_vocab))\n",
    "print(additional_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4620/4620 [01:06<00:00, 69.88 examples/s]\n",
      "Map: 100%|██████████| 1680/1680 [00:24<00:00, 69.78 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# replace ' ' in ipa with tokenizer.pad_token\n",
    "train_ds = train_ds.map(lambda x: {'audio': x['audio'], 'ipa': x['ipa'].replace(' ', tokenizer.pad_token)})\n",
    "test_ds = test_ds.map(lambda x: {'audio': x['audio'], 'ipa': x['ipa'].replace(' ', tokenizer.pad_token)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(PRE_TRAINED_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to update the feature extractor since it has been pretrained on 16kHz audio which matches the TIMIT dataset.\n",
    "\n",
    "For datasets with different sampling rates, the feature extractor should be updated or the audio resampled (easier).\n",
    "\n",
    "This is also where code to add extra features (such as conditioning on speaker's native language etc.) would be added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Updated Tokenizer and Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(PRE_TRAINED_ID)\n",
    "processor.tokenizer = tokenizer\n",
    "processor.feature_extractor = feature_extractor\n",
    "processor.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    batch[\"input_values\"] = processor(batch[\"audio\"], sampling_rate=feature_extractor.sampling_rate).input_values\n",
    "    batch[\"labels\"] = processor(text=batch[\"ipa\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 4620/4620 [01:06<00:00, 69.69 examples/s] \n"
     ]
    }
   ],
   "source": [
    "train_ds_prepared = train_ds.map(prepare_dataset, batch_size=8, num_proc=4, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 1680/1680 [00:27<00:00, 60.24 examples/s] \n"
     ]
    }
   ],
   "source": [
    "test_ds_prepared = test_ds.map(prepare_dataset, batch_size=8, num_proc=4, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>ipa</th>\n",
       "      <th>input_values</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>oʊnʔæsmitɨkɛɹiʔɨnʔɔɪliɹæglaɪkðæt</td>\n",
       "      <td>[18.135152024062002, 21.17466781012437, 20.460...</td>\n",
       "      <td>[49, 29, 18, 5, 17, 8, 41, 4, 14, 4, 35, 8, 41...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>ɹiɾiŋʔɨnpɔɹlaɪtɪvʒʉʔaɪstɹeɪn</td>\n",
       "      <td>[18.094273809159557, 21.125626761823945, 20.41...</td>\n",
       "      <td>[6, 63, 4, 12, 41, 27, 37, 68, 15, 36, 18, 37,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>ɨmplɔɪɨʔleɪɔfskoʊɨntsaɪdɪdwɨθɨkʌmpn̩izɹɨɔɹgɨnɨ...</td>\n",
       "      <td>[16.754583715965623, 19.561161243340255, 18.90...</td>\n",
       "      <td>[37, 26, 68, 14, 13, 18, 30, 14, 8, 49, 18, 7,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>ægaɪpsɨnbɑŋgoʊzɚmjʉzɨkl̩&lt;pad&gt;ɨntstə̥mɨnts</td>\n",
       "      <td>[15.056215957312128, 17.58146924092826, 16.988...</td>\n",
       "      <td>[8, 17, 35, 20, 66, 7, 44, 30, 396, 20, 5, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>ʃiɦæddʒɚdɑɹksʉɾɨngɹisiwɔɹʃwɔɾɚɔljɪɚ</td>\n",
       "      <td>[19.301096442287164, 22.5337553816438, 21.7740...</td>\n",
       "      <td>[14, 38, 37, 397, 8, 17, 35, 20, 66, 7, 44, 30...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               audio  \\\n",
       "0  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "1  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "2  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "3  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "4  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "\n",
       "                                                 ipa  \\\n",
       "0                   oʊnʔæsmitɨkɛɹiʔɨnʔɔɪliɹæglaɪkðæt   \n",
       "1                       ɹiɾiŋʔɨnpɔɹlaɪtɪvʒʉʔaɪstɹeɪn   \n",
       "2  ɨmplɔɪɨʔleɪɔfskoʊɨntsaɪdɪdwɨθɨkʌmpn̩izɹɨɔɹgɨnɨ...   \n",
       "3          ægaɪpsɨnbɑŋgoʊzɚmjʉzɨkl̩<pad>ɨntstə̥mɨnts   \n",
       "4                ʃiɦæddʒɚdɑɹksʉɾɨngɹisiwɔɹʃwɔɾɚɔljɪɚ   \n",
       "\n",
       "                                        input_values  \\\n",
       "0  [18.135152024062002, 21.17466781012437, 20.460...   \n",
       "1  [18.094273809159557, 21.125626761823945, 20.41...   \n",
       "2  [16.754583715965623, 19.561161243340255, 18.90...   \n",
       "3  [15.056215957312128, 17.58146924092826, 16.988...   \n",
       "4  [19.301096442287164, 22.5337553816438, 21.7740...   \n",
       "\n",
       "                                              labels  \n",
       "0  [49, 29, 18, 5, 17, 8, 41, 4, 14, 4, 35, 8, 41...  \n",
       "1  [6, 63, 4, 12, 41, 27, 37, 68, 15, 36, 18, 37,...  \n",
       "2  [37, 26, 68, 14, 13, 18, 30, 14, 8, 49, 18, 7,...  \n",
       "3  [8, 17, 35, 20, 66, 7, 44, 30, 396, 20, 5, 13,...  \n",
       "4  [14, 38, 37, 397, 8, 17, 35, 20, 66, 7, 44, 30...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_prepared.to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, processor: AutoProcessor, padding=True, max_length=None, max_length_labels=None, pad_to_multiple_of=None, pad_to_multiple_of_labels=None):\n",
    "        self.processor = processor\n",
    "        self.padding = padding\n",
    "        self.max_length = max_length\n",
    "        self.max_length_labels = max_length_labels\n",
    "        self.pad_to_multiple_of = pad_to_multiple_of\n",
    "        self.pad_to_multiple_of_labels = pad_to_multiple_of_labels\n",
    "\n",
    "    def __call__(self, features: \"list[dict[str, list[int] | torch.Tensor]]\") -> \"dict[str, torch.Tensor]\":\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cer(pred, label):\n",
    "    distances = np.zeros((len(pred) + 1, len(label) + 1))\n",
    "\n",
    "    for t1 in range(len(pred) + 1):\n",
    "        distances[t1][0] = t1\n",
    "\n",
    "    for t2 in range(len(label) + 1):\n",
    "        distances[0][t2] = t2\n",
    "        \n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "    \n",
    "    for t1 in range(1, len(pred) + 1):\n",
    "        for t2 in range(1, len(label) + 1):\n",
    "            if (pred[t1-1] == label[t2-1]):\n",
    "                distances[t1][t2] = distances[t1 - 1][t2 - 1]\n",
    "            else:\n",
    "                a = distances[t1][t2 - 1]\n",
    "                b = distances[t1 - 1][t2]\n",
    "                c = distances[t1 - 1][t2 - 1]\n",
    "                \n",
    "                if (a <= b and a <= c):\n",
    "                    distances[t1][t2] = a + 1\n",
    "                elif (b <= a and b <= c):\n",
    "                    distances[t1][t2] = b + 1\n",
    "                else:\n",
    "                    distances[t1][t2] = c + 1\n",
    "\n",
    "    return distances[len(pred)][len(label)] / len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False) # labels are already grouped as they should be\n",
    "\n",
    "    err = cer(pred_str, label_str)\n",
    "\n",
    "    return {\"cer\": err}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "label_str = 'aab'\n",
    "pred_str = 'aaac'\n",
    "print(cer(pred_str, label_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCTC.from_pretrained(\n",
    "    PRE_TRAINED_ID, \n",
    "    pad_token_id=processor.tokenizer.pad_token_id, \n",
    "    gradient_checkpointing=True,\n",
    "    ctc_zero_infinity = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "old_weights = model.lm_head.weight.t().detach()\n",
    "old_bias = model.lm_head.bias.detach()\n",
    "new_layer = torch.nn.Linear(model.lm_head.in_features, len(processor.tokenizer))\n",
    "new_layer.weight.data[:model.lm_head.out_features, :] = old_weights.t()\n",
    "new_layer.bias.data[:model.lm_head.out_features] = old_bias\n",
    "model.lm_head = new_layer\n",
    "model.config.vocab_size = len(processor.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "max_class_id = 0\n",
    "for label_sequence in train_ds_prepared['labels']:\n",
    "    for class_id in label_sequence:\n",
    "        max_class_id = max(max_class_id, class_id)\n",
    "assert max_class_id == model.config.vocab_size - 1\n",
    "\n",
    "for label_sequence in train_ds_prepared['labels']:\n",
    "    for class_id in label_sequence:\n",
    "        assert class_id >= 0 and class_id <  model.config.vocab_size\n",
    "\n",
    "for label_sequence in train_ds_prepared['labels']:\n",
    "    assert  len(label_sequence) < 1024\n",
    "    assert len(label_sequence) >= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=OUTPUT_DIR,\n",
    "#     group_by_length=True,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     gradient_accumulation_steps=2,\n",
    "#     eval_strategy=\"steps\",\n",
    "#     num_train_epochs=50,\n",
    "#     fp16=False, # set to true when training with CUDA?\n",
    "#     save_steps=500,\n",
    "#     eval_steps=500,\n",
    "#     logging_steps=500,\n",
    "#     learning_rate=3e-4,\n",
    "#     gradient_checkpointing=True,\n",
    "#     warmup_steps=1000,\n",
    "#     save_total_limit=2,\n",
    "#     load_best_model_at_end=True,\n",
    "# )\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    num_train_epochs=1,\n",
    "    fp16=False,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    learning_rate=3e-4,\n",
    "    warmup_steps=0,\n",
    "    save_total_limit=3,\n",
    "    gradient_checkpointing=True,\n",
    "    load_best_model_at_end=True,\n",
    "    max_grad_norm=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_ds_prepared.select(range(100)),\n",
    "    # train_dataset=train_ds_prepared,\n",
    "    eval_dataset=test_ds_prepared.select(range(100)),\n",
    "    # eval_dataset=test_ds_prepared,\n",
    "    processing_class=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]/Users/alex/Desktop/CS/Startups/Koel/ML/venv/lib/python3.8/site-packages/torch/nn/functional.py:2705: UserWarning: The operator 'aten::_ctc_loss' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  return torch.ctc_loss(\n",
      "/Users/alex/Desktop/CS/Startups/Koel/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "                                             \n",
      "100%|██████████| 7/7 [01:08<00:00,  9.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 68.7438, 'train_samples_per_second': 1.455, 'train_steps_per_second': 0.102, 'train_loss': 7413.9765625, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7, training_loss=7413.9765625, metrics={'train_runtime': 68.7438, 'train_samples_per_second': 1.455, 'train_steps_per_second': 0.102, 'total_flos': 1.5305473943388e+16, 'train_loss': 7413.9765625, 'epoch': 1.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True) # debug NaNs, disable for real training to improve performance\n",
    "trainer.train() # resume_from_checkpoint=True to resume training if training was interrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/Users/alex/Desktop/CS/Startups/Koel/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "100%|██████████| 13/13 [00:17<00:00,  1.34s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3396.741943359375,\n",
       " 'eval_cer': 1.0,\n",
       " 'eval_runtime': 26.6438,\n",
       " 'eval_samples_per_second': 3.753,\n",
       " 'eval_steps_per_second': 0.488,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_model = AutoModelForCTC.from_pretrained(PRE_TRAINED_ID).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = AutoModelForCTC.from_pretrained(os.path.join(OUTPUT_DIR, 'checkpoint-7'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for nan weights and replace with old values if any\n",
    "for name, param in finetuned_model.named_parameters():\n",
    "    if torch.isnan(param).any():\n",
    "        print(name)\n",
    "        param.data = old_model.state_dict()[name].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are different, training did something\n"
     ]
    }
   ],
   "source": [
    "# compare weights\n",
    "is_diff = False\n",
    "for name, param in finetuned_model.named_parameters():\n",
    "    if not torch.equal(param, old_model.state_dict()[name].to(finetuned_model.device)):\n",
    "        is_diff = True\n",
    "        break\n",
    "print(\"Weights are different, training did something\" if is_diff else \"Weights are the same, training did nothing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, wav_file):\n",
    "    speech = audio_file_to_array(wav_file)\n",
    "    input_values = processor(speech, sampling_rate=16000, return_tensors=\"pt\").input_values.type(torch.float32).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    return processor.decode(predicted_ids[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Transcription: h ɛ l ɛ l ɛ s ɛ z ɛ k t ɹ k ɛ n f iː\n"
     ]
    }
   ],
   "source": [
    "print('Checkpoint Transcription:', predict(finetuned_model, os.path.join('..', 'data', 'alexIsConfused.wav')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Fine-Tune Transcription: h ɛ l ɛ l ɛ s ɛ z ɛ k t ɹ k ɛ n f iː\n"
     ]
    }
   ],
   "source": [
    "print('Best Fine-Tune Transcription:', predict(trainer.model, os.path.join('..', 'data', 'alexIsConfused.wav')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Transcription: h ɛ l oʊ æ l ɪ k s ɪ z ɛ k s t ɾ ɹ ə k ə n f iː l ʊ s t\n"
     ]
    }
   ],
   "source": [
    "print('Old Transcription:', predict(old_model, os.path.join('..', 'data', 'alexIsConfused.wav')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
