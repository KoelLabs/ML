{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import zipfile\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "# run as much as possible accelerated by apple silicon, fall back to cpu if not possible\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from scripts.audio import audio_file_to_array\n",
    "from scripts.ipa import timit2ipa\n",
    "from scripts.ipa import filter_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('..')\n",
    "from scripts.eval_tests.panphon_model_eval import panphon_model_eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set espeak library path for macOS\n",
    "if sys.platform == \"darwin\":\n",
    "    from phonemizer.backend.espeak.wrapper import EspeakWrapper\n",
    "\n",
    "    _ESPEAK_LIBRARY = \"/opt/homebrew/Cellar/espeak/1.48.04_1/lib/libespeak.1.1.48.dylib\"\n",
    "    EspeakWrapper.set_library(_ESPEAK_LIBRARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCTC, AutoTokenizer, AutoFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = os.path.join('..', 'models', 'timit-xlsr-finetune-B')\n",
    "PRE_TRAINED_ID = \"facebook/wav2vec2-lv-60-espeak-cv-ft\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit = zipfile.ZipFile('../.data/TIMIT.zip', 'r')\n",
    "timit_files = timit.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = list(set(map(lambda x: x.split('.')[0], filter(lambda x: x.startswith('data/TRAIN'), timit_files))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_HEADER_SIZE = 44\n",
    "def zipped_wav_to_array(filename):\n",
    "    with timit.open(filename) as wav_file:\n",
    "        return np.frombuffer(wav_file.read(), dtype=np.int16)[WAV_HEADER_SIZE//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>ipa</th>\n",
       "      <th>phoneme_starts</th>\n",
       "      <th>phoneme_ends</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>ðəbɔstnbæleɪoʊvəkeɪmðɛɹfʌndiŋʃɔɹɾiddʒ</td>\n",
       "      <td>[0, 2152, 2398, 2837, 4270, 4501, 6840, 7394, ...</td>\n",
       "      <td>[2152, 2398, 2837, 4270, 4501, 6840, 7394, 787...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>flaɪiŋstændaɪkinbipɹækəklʔifjuwɑntuseɪvmʌni</td>\n",
       "      <td>[0, 2680, 3480, 4135, 5723, 6600, 7320, 9080, ...</td>\n",
       "      <td>[2680, 3480, 4135, 5723, 6600, 7320, 9080, 960...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>ʃihædjɹdɑɹksutʔinɡɹisiwɔʃwɔɾɹʔɔljɪɹ</td>\n",
       "      <td>[0, 2400, 4470, 5480, 6600, 8898, 9566, 10151,...</td>\n",
       "      <td>[2400, 4470, 5480, 6600, 8898, 9566, 10151, 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>hɛlpsɛləbɹeɪtjɹbɹʌðɹsiksɛs</td>\n",
       "      <td>[0, 4280, 4920, 6146, 6760, 7951, 10760, 12320...</td>\n",
       "      <td>[4280, 4920, 6146, 6760, 7951, 10760, 12320, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>eɪvimæθjusʔɪtsdzisɡʌstiŋnəweɪjɹʔɑlisʔidɪŋ</td>\n",
       "      <td>[0, 2300, 2760, 4971, 5890, 7060, 8981, 12040,...</td>\n",
       "      <td>[2300, 2760, 4971, 5890, 7060, 8981, 12040, 13...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               audio  \\\n",
       "0  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "1  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "2  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "3  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "4  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "\n",
       "                                           ipa  \\\n",
       "0        ðəbɔstnbæleɪoʊvəkeɪmðɛɹfʌndiŋʃɔɹɾiddʒ   \n",
       "1  flaɪiŋstændaɪkinbipɹækəklʔifjuwɑntuseɪvmʌni   \n",
       "2          ʃihædjɹdɑɹksutʔinɡɹisiwɔʃwɔɾɹʔɔljɪɹ   \n",
       "3                   hɛlpsɛləbɹeɪtjɹbɹʌðɹsiksɛs   \n",
       "4    eɪvimæθjusʔɪtsdzisɡʌstiŋnəweɪjɹʔɑlisʔidɪŋ   \n",
       "\n",
       "                                      phoneme_starts  \\\n",
       "0  [0, 2152, 2398, 2837, 4270, 4501, 6840, 7394, ...   \n",
       "1  [0, 2680, 3480, 4135, 5723, 6600, 7320, 9080, ...   \n",
       "2  [0, 2400, 4470, 5480, 6600, 8898, 9566, 10151,...   \n",
       "3  [0, 4280, 4920, 6146, 6760, 7951, 10760, 12320...   \n",
       "4  [0, 2300, 2760, 4971, 5890, 7060, 8981, 12040,...   \n",
       "\n",
       "                                        phoneme_ends  \n",
       "0  [2152, 2398, 2837, 4270, 4501, 6840, 7394, 787...  \n",
       "1  [2680, 3480, 4135, 5723, 6600, 7320, 9080, 960...  \n",
       "2  [2400, 4470, 5480, 6600, 8898, 9566, 10151, 11...  \n",
       "3  [4280, 4920, 6146, 6760, 7951, 10760, 12320, 1...  \n",
       "4  [2300, 2760, 4971, 5890, 7060, 8981, 12040, 13...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, ensure that the simplify_timit function is available and works\n",
    "def simplify_timit(phoneme):\n",
    "    # Substitute the phoneme based on the dictionary\n",
    "    substitution_dict = {\n",
    "        'ɾ̃': 'ɾ',  # Replace nasalized flap with plain flap\n",
    "        'ŋ̍': 'ŋ',  # Remove syllabic marker from 'ŋ̍'\n",
    "        'ə̥': 'ə',  # Remove voiceless marker from 'ə̥'\n",
    "        'ɝ': 'ɹ',   # Simplify rhotacized schwa to 'ɹ'\n",
    "        'ɚ': 'ɹ',   # Simplify rhotacized schwa to 'ɹ'\n",
    "        'l̩': 'l',   # Remove syllabic marker from 'l̩'\n",
    "        'm̩': 'm',   # Remove syllabic marker from 'm̩'\n",
    "        'n̩': 'n',   # Remove syllabic marker from 'n̩'\n",
    "        '̩': '',     # Remove syllabic marker\n",
    "        'ʉ': 'u',    # Replace high central rounded vowel with high back rounded vowel\n",
    "        'ɨ': 'i',    # Replace high central unrounded vowel with high front unrounded vowel\n",
    "        ' ': '',     # Remove nasalization marker\n",
    "        'ɦ': 'h',    # Replace voiceless glottal fricative with voiceless glottal fricative\n",
    "        '͡': '', # Simplify affricate to single character\n",
    "        # Add other necessary substitutions if needed\n",
    "    }\n",
    "    # Apply the substitution for the phoneme\n",
    "    return substitution_dict.get(phoneme, phoneme)  # Return simplified phoneme or the original if no replacement\n",
    "\n",
    "def remove_stress_mark(text):\n",
    "    \"\"\"\n",
    "    Removes the combining double inverted breve (͡) from text.\n",
    "    \n",
    "    Args:\n",
    "        text: String or iterable containing IPA symbols\n",
    "        \n",
    "    Returns:\n",
    "        Text with stress marks removed\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return text.replace('͡', '')\n",
    "    else:\n",
    "        raise TypeError(\"Input must be string, set, or list\")\n",
    "    \n",
    "# Updated timit_file_to_dict to use the simplified IPA phonemes\n",
    "def timit_file_to_dict(filename):\n",
    "    with timit.open(filename + '.PHN') as phn_file:\n",
    "        timestamped_phonemes = []\n",
    "        for line in phn_file.read().decode('utf-8').split('\\n'):\n",
    "            if line == '':\n",
    "                continue\n",
    "            start, end, phoneme = line.split()\n",
    "            \n",
    "            # Convert to IPA first and then simplify the phoneme\n",
    "            ipa_phonemes = timit2ipa(phoneme, \"eng\")\n",
    "            \n",
    "            # Now filter only the necessary characters (this simplifies the phoneme to the basic form)\n",
    "            cleaned_ipa = filter_chars(simplify_timit(ipa_phonemes), filter_type=\"letters\")\n",
    "            cleaned_ipa = remove_stress_mark(cleaned_ipa)\n",
    "            timestamped_phonemes.append((cleaned_ipa, int(start), int(end)))\n",
    "\n",
    "    return {'timestamped_phonemes': timestamped_phonemes, 'wav_filename': filename + '.WAV'}\n",
    "\n",
    "# Updated files_to_df to create DataFrame with cleaned-up phonemes\n",
    "def files_to_df(files):\n",
    "    records = []\n",
    "    for filename in files:\n",
    "        parsed = timit_file_to_dict(filename)\n",
    "        parsed['audio'] = zipped_wav_to_array(parsed['wav_filename'])\n",
    "        del parsed['wav_filename']\n",
    "        parsed['ipa'] = \"\".join(phoneme for phoneme, _, _ in parsed['timestamped_phonemes'])\n",
    "        parsed['phoneme_starts'] = [start for _, start, _ in parsed['timestamped_phonemes']]\n",
    "        parsed['phoneme_ends'] = [end for _, _, end in parsed['timestamped_phonemes']]\n",
    "        del parsed['timestamped_phonemes']\n",
    "        records.append(parsed)\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Call files_to_df for your test files\n",
    "train_df = files_to_df(training_files)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Up Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_audio(row):\n",
    "    # Access the phoneme_starts column and get the last value of the list\n",
    "    end = row['phoneme_starts'][-1]\n",
    "    # Access the phoneme_ends column and get the first value of the list\n",
    "    start = row['phoneme_ends'][0]\n",
    "    # Crop the audio from start to end\n",
    "    # note that start and end are in samples, not seconds\n",
    "    cropped_audio = row['audio'][start:end]\n",
    "    \n",
    "    return cropped_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAGdCAYAAAAWp6lMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABRjUlEQVR4nO3deVxU5f4H8M+wzADCsMgmCoqiICqSKEguaRJo1I2ye92umVmmYamYW5naimmLVi63W1fr/srtllauESqkoiaKggu5ES4MuDEDqKzP7w/j5AiyDDMMHj7v12teMnO+c84zx4H5zDnPeR6FEEKAiIiISMYszN0AIiIiIlNj4CEiIiLZY+AhIiIi2WPgISIiItlj4CEiIiLZY+AhIiIi2WPgISIiItlj4CEiIiLZszJ3A5qCiooKXLp0CQ4ODlAoFOZuDhEREdWBEAIFBQXw8vKChUXNx3AYeABcunQJ3t7e5m4GERERGeD8+fNo06ZNjTUMPAAcHBwA3N5harXazK0hIiKiutDpdPD29pY+x2vCwANIp7HUajUDDxER0X2mLt1R2GmZiIiIZI+Bh4iIiGSPgYeIiIhkj4GHiIiIZI+Bh4iIiGSPgYeIiIhkj4GHiIiIZI+Bh4iIiGSPgYeIiIhkj4GHiIiIZI+Bh4iIiGSPgYeIiIhkj4HHxPJ0t9DznQTsPnXF3E0hIiJqthh4TCz0vURcKSzBP7/cb+6mEBERNVsMPERERCR7DDxEREQkeww8REREJHsMPERERCR7DDxEREQkewYHnuXLlyMoKAhqtRpqtRrh4eHYunWrtPzWrVuIjY1Fy5YtYW9vj6FDhyI3N1dvHdnZ2YiOjoadnR3c3d0xffp0lJWV6dXs2rULPXr0gEqlgp+fH1atWlWlLUuXLkW7du1gY2ODsLAwHDhwwNCXRURERDJkcOBp06YNFixYgNTUVBw8eBAPP/wwnnjiCRw7dgwAMHXqVPz0009Yv349kpKScOnSJTz11FPS88vLyxEdHY2SkhLs3bsXX331FVatWoW5c+dKNefOnUN0dDQGDhyItLQ0TJkyBc8//zy2b98u1axduxZxcXGYN28eDh06hO7duyMqKgp5eXmGvjSjuXD9hrmbQERERAAgjMjZ2Vl88cUXIj8/X1hbW4v169dLy06cOCEAiJSUFCGEEFu2bBEWFhZCo9FINcuXLxdqtVoUFxcLIYSYMWOG6NKli942hg0bJqKioqT7oaGhIjY2VrpfXl4uvLy8RHx8fJ3brdVqBQCh1Wrr94JrseNkrmg7c5N0IyIiIuOpz+e3UfrwlJeXY82aNSgqKkJ4eDhSU1NRWlqKiIgIqSYgIAA+Pj5ISUkBAKSkpKBbt27w8PCQaqKioqDT6aSjRCkpKXrrqKypXEdJSQlSU1P1aiwsLBARESHVVKe4uBg6nU7vRkRERPLVoMCTnp4Oe3t7qFQqTJgwARs2bEBgYCA0Gg2USiWcnJz06j08PKDRaAAAGo1GL+xULq9cVlONTqfDzZs3ceXKFZSXl1dbU7mO6sTHx8PR0VG6eXt7G/T6iYiI6P7QoMDj7++PtLQ07N+/HxMnTsSYMWNw/PhxY7XNZGbPng2tVivdzp8/b+4mERERkQlZNeTJSqUSfn5+AICQkBD89ttvWLJkCYYNG4aSkhLk5+frHeXJzc2Fp6cnAMDT07PK1VSVV3HdWXP3lV25ublQq9WwtbWFpaUlLC0tq62pXEd1VCoVVCqVYS+aiIiI7jtGHYenoqICxcXFCAkJgbW1NRITE6VlmZmZyM7ORnh4OAAgPDwc6enpeldTJSQkQK1WIzAwUKq5cx2VNZXrUCqVCAkJ0aupqKhAYmKiVENERERk8BGe2bNnY8iQIfDx8UFBQQG+/fZb7Nq1C9u3b4ejoyPGjRuHuLg4uLi4QK1W4+WXX0Z4eDh69+4NAIiMjERgYCBGjx6NhQsXQqPRYM6cOYiNjZWOvkyYMAGfffYZZsyYgeeeew47duzAunXrsHnzZqkdcXFxGDNmDHr27InQ0FAsXrwYRUVFGDt2bAN3DREREcmFwYEnLy8PzzzzDHJycuDo6IigoCBs374djzzyCADg448/hoWFBYYOHYri4mJERUVh2bJl0vMtLS2xadMmTJw4EeHh4WjRogXGjBmDt956S6rx9fXF5s2bMXXqVCxZsgRt2rTBF198gaioKKlm2LBhuHz5MubOnQuNRoPg4GBs27atSkdmIiIiar4UQghh7kaYm06ng6OjI7RaLdRqtdHWuzMzD2NX/ibdz1oQbbR1ExERNXf1+fzmXFpEREQkeww8REREJHsMPERERCR7DDxEREQkeww8REREJHsMPERERCR7DDxEREQkeww8REREJHsMPERERCR7DDwmVFJWYe4mEBERERh4TEp7o9TcTSAiIiIw8BAREVEzwMBDREREssfAQ0RERLLHwENERESyx8BDREREssfAQ0RERLLHwENERESyx8BDREREssfAQ0RERLLHwENERESyx8BDREREssfAQ0RERLLHwENERESyx8BDREREssfAQ0RERLLHwENERESyx8BDREREssfAQ0RERLLHwGNKCnM3gIiIiAAGHiIiImoGGHiIiIhI9hh4TMjb2c7cTSAiIiIw8JiUtSU78RARETUFDDxEREQkeww8REREJHsGB574+Hj06tULDg4OcHd3R0xMDDIzM/VqBgwYAIVCoXebMGGCXk12djaio6NhZ2cHd3d3TJ8+HWVlZXo1u3btQo8ePaBSqeDn54dVq1ZVac/SpUvRrl072NjYICwsDAcOHDD0pREREZHMGBx4kpKSEBsbi3379iEhIQGlpaWIjIxEUVGRXt0LL7yAnJwc6bZw4UJpWXl5OaKjo1FSUoK9e/fiq6++wqpVqzB37lyp5ty5c4iOjsbAgQORlpaGKVOm4Pnnn8f27dulmrVr1yIuLg7z5s3DoUOH0L17d0RFRSEvL8/Ql0dEREQyohBCCGOs6PLly3B3d0dSUhL69+8P4PYRnuDgYCxevLja52zduhWPPfYYLl26BA8PDwDAihUrMHPmTFy+fBlKpRIzZ87E5s2bkZGRIT1v+PDhyM/Px7Zt2wAAYWFh6NWrFz777DMAQEVFBby9vfHyyy9j1qxZtbZdp9PB0dERWq0WarW6IbtBz8Gsa3h6RYp0P2tBtNHWTURE1NzV5/PbaH14tFotAMDFxUXv8W+++Qaurq7o2rUrZs+ejRs3bkjLUlJS0K1bNynsAEBUVBR0Oh2OHTsm1UREROitMyoqCikpt4NESUkJUlNT9WosLCwQEREh1dytuLgYOp1O72YKCl6kRURE1CRYGWMlFRUVmDJlCvr06YOuXbtKj48cORJt27aFl5cXjh49ipkzZyIzMxPff/89AECj0eiFHQDSfY1GU2ONTqfDzZs3cf36dZSXl1dbc/LkyWrbGx8fjzfffLNhL7oOjHPsjIiIiBrKKIEnNjYWGRkZ2L17t97j48ePl37u1q0bWrVqhUGDBuHMmTPo0KGDMTZtkNmzZyMuLk66r9Pp4O3tbbb2EBERkWk1OPBMmjQJmzZtQnJyMtq0aVNjbVhYGADg9OnT6NChAzw9PatcTZWbmwsA8PT0lP6tfOzOGrVaDVtbW1haWsLS0rLamsp13E2lUkGlUtX9RRIREdF9zeA+PEIITJo0CRs2bMCOHTvg6+tb63PS0tIAAK1atQIAhIeHIz09Xe9qqoSEBKjVagQGBko1iYmJeutJSEhAeHg4AECpVCIkJESvpqKiAomJiVINERERNW8GH+GJjY3Ft99+ix9++AEODg5SnxtHR0fY2trizJkz+Pbbb/Hoo4+iZcuWOHr0KKZOnYr+/fsjKCgIABAZGYnAwECMHj0aCxcuhEajwZw5cxAbGysdgZkwYQI+++wzzJgxA8899xx27NiBdevWYfPmzVJb4uLiMGbMGPTs2ROhoaFYvHgxioqKMHbs2IbsGyIiIpILYSAA1d5WrlwphBAiOztb9O/fX7i4uAiVSiX8/PzE9OnThVar1VtPVlaWGDJkiLC1tRWurq5i2rRporS0VK9m586dIjg4WCiVStG+fXtpG3f69NNPhY+Pj1AqlSI0NFTs27evzq9Fq9UKAFXa1lC/nbsq2s7cJN2IiIjIeOrz+W20cXjuZxyHh4iI6P5jlnF4iIiIiJoqBh4T4sCDRERETQMDj0kx8RARETUFDDxEREQkeww8JtXs+4MTERE1CQw8REREJHsMPERERCR7DDxEREQkeww8REREJHsMPERERCR7DDxEREQkeww8JsWBB4mIiJoCBh4iIiKSPQYeIiIikj0GHpPiSMtERERNAQMPERERyR4DDxEREckeAw8RERHJHgMPERERyR4DDxEREckeAw8RERHJHgOPSXGkZSIioqaAgceEFMw7RERETQIDDxEREckeAw8RERHJHgOPCQnOLEFERNQkMPAQERGR7DHwEBERkewx8BAREZHsMfAQERGR7DHwmBDH4SEiImoaGHiIiIhI9hh4iIiISPYYeIiIiEj2GHiIiIhI9hh4TIgjLRMRETUNBgee+Ph49OrVCw4ODnB3d0dMTAwyMzP1am7duoXY2Fi0bNkS9vb2GDp0KHJzc/VqsrOzER0dDTs7O7i7u2P69OkoKyvTq9m1axd69OgBlUoFPz8/rFq1qkp7li5dinbt2sHGxgZhYWE4cOCAoS+NiIiIZMbgwJOUlITY2Fjs27cPCQkJKC0tRWRkJIqKiqSaqVOn4qeffsL69euRlJSES5cu4amnnpKWl5eXIzo6GiUlJdi7dy+++uorrFq1CnPnzpVqzp07h+joaAwcOBBpaWmYMmUKnn/+eWzfvl2qWbt2LeLi4jBv3jwcOnQI3bt3R1RUFPLy8gx9eURERCQnwkjy8vIEAJGUlCSEECI/P19YW1uL9evXSzUnTpwQAERKSooQQogtW7YICwsLodFopJrly5cLtVotiouLhRBCzJgxQ3Tp0kVvW8OGDRNRUVHS/dDQUBEbGyvdLy8vF15eXiI+Pr5ObddqtQKA0Gq19XzVNTuYdU20nblJuhEREZHx1Ofz22h9eLRaLQDAxcUFAJCamorS0lJERERINQEBAfDx8UFKSgoAICUlBd26dYOHh4dUExUVBZ1Oh2PHjkk1d66jsqZyHSUlJUhNTdWrsbCwQEREhFRzt+LiYuh0Or0bERERyZdRAk9FRQWmTJmCPn36oGvXrgAAjUYDpVIJJycnvVoPDw9oNBqp5s6wU7m8cllNNTqdDjdv3sSVK1dQXl5ebU3lOu4WHx8PR0dH6ebt7W3YC68FR1omIiJqGowSeGJjY5GRkYE1a9YYY3UmN3v2bGi1Wul2/vx5k2yHeYeIiKhpsGroCiZNmoRNmzYhOTkZbdq0kR739PRESUkJ8vPz9Y7y5ObmwtPTU6q5+2qqyqu47qy5+8qu3NxcqNVq2NrawtLSEpaWltXWVK7jbiqVCiqVyrAXTERERPcdg4/wCCEwadIkbNiwATt27ICvr6/e8pCQEFhbWyMxMVF6LDMzE9nZ2QgPDwcAhIeHIz09Xe9qqoSEBKjVagQGBko1d66jsqZyHUqlEiEhIXo1FRUVSExMlGqIiIioeTP4CE9sbCy+/fZb/PDDD3BwcJD6yzg6OsLW1haOjo4YN24c4uLi4OLiArVajZdffhnh4eHo3bs3ACAyMhKBgYEYPXo0Fi5cCI1Ggzlz5iA2NlY6AjNhwgR89tlnmDFjBp577jns2LED69atw+bNm6W2xMXFYcyYMejZsydCQ0OxePFiFBUVYezYsQ3ZN0RERCQXhl4KBqDa28qVK6Wamzdvipdeekk4OzsLOzs78eSTT4qcnBy99WRlZYkhQ4YIW1tb4erqKqZNmyZKS0v1anbu3CmCg4OFUqkU7du319tGpU8//VT4+PgIpVIpQkNDxb59++r8Wkx1WXrqH7wsnYiIyFTq8/mtEIITIOh0Ojg6OkKr1UKtVhttvYeyr+OpZXul+1kLoo22biIiouauPp/fnEuLiIiIZI+Bh4iIiGSPgceEOA4PERFR08DAQ0RERLLHwENERESyx8BDREREssfAQ0RERLLHwENERESyx8BDREREssfAY0LNfghrIiKiJoKBh4iIiGSPgYeIiIhkj4HHhDjSMhERUdPAwENERESyx8BjQgoFj/EQERE1BQw8REREJHsMPERERCR7DDxEREQkeww8REREJHsMPCYkBMdaJiIiagoYeIiIiEj2GHiIiIhI9hh4TIjj8BARETUNDDxEREQkeww8REREJHsMPERERCR7DDxEREQkeww8REREJHsMPERERCR7DDxEREQkeww8REREJHsMPERERCR7DDxEREQkeww8JsSJJYiIiJoGBh4iIiKSPYMDT3JyMh5//HF4eXlBoVBg48aNesufffZZKBQKvdvgwYP1aq5du4ZRo0ZBrVbDyckJ48aNQ2FhoV7N0aNH0a9fP9jY2MDb2xsLFy6s0pb169cjICAANjY26NatG7Zs2WLoyyIiIiIZMjjwFBUVoXv37li6dOk9awYPHoycnBzptnr1ar3lo0aNwrFjx5CQkIBNmzYhOTkZ48ePl5brdDpERkaibdu2SE1NxaJFizB//nx8/vnnUs3evXsxYsQIjBs3DocPH0ZMTAxiYmKQkZFh6EsjIiIimVEIIUSDV6JQYMOGDYiJiZEee/bZZ5Gfn1/lyE+lEydOIDAwEL/99ht69uwJANi2bRseffRRXLhwAV5eXli+fDlef/11aDQaKJVKAMCsWbOwceNGnDx5EgAwbNgwFBUVYdOmTdK6e/fujeDgYKxYsaJO7dfpdHB0dIRWq4VarTZgD1TvyPl8PLF0j3Q/a0G00dZNRETU3NXn89ukfXh27doFd3d3+Pv7Y+LEibh69aq0LCUlBU5OTlLYAYCIiAhYWFhg//79Uk3//v2lsAMAUVFRyMzMxPXr16WaiIgIve1GRUUhJSXlnu0qLi6GTqfTuxEREZF8mSzwDB48GF9//TUSExPx/vvvIykpCUOGDEF5eTkAQKPRwN3dXe85VlZWcHFxgUajkWo8PDz0airv11ZTubw68fHxcHR0lG7e3t4Ne7FERETUpFmZasXDhw+Xfu7WrRuCgoLQoUMH7Nq1C4MGDTLVZutk9uzZiIuLk+7rdDqThJ4GnyskIiIio2i0y9Lbt28PV1dXnD59GgDg6emJvLw8vZqysjJcu3YNnp6eUk1ubq5eTeX92moql1dHpVJBrVbr3YiIiEi+Gi3wXLhwAVevXkWrVq0AAOHh4cjPz0dqaqpUs2PHDlRUVCAsLEyqSU5ORmlpqVSTkJAAf39/ODs7SzWJiYl620pISEB4eLipXxIRERHdJwwOPIWFhUhLS0NaWhoA4Ny5c0hLS0N2djYKCwsxffp07Nu3D1lZWUhMTMQTTzwBPz8/REVFAQA6d+6MwYMH44UXXsCBAwewZ88eTJo0CcOHD4eXlxcAYOTIkVAqlRg3bhyOHTuGtWvXYsmSJXqnoyZPnoxt27bhww8/xMmTJzF//nwcPHgQkyZNasBuISIiIlkRBtq5c6fA7W4qercxY8aIGzduiMjISOHm5iasra1F27ZtxQsvvCA0Go3eOq5evSpGjBgh7O3thVqtFmPHjhUFBQV6NUeOHBF9+/YVKpVKtG7dWixYsKBKW9atWyc6deoklEql6NKli9i8eXO9XotWqxUAhFarrf+OqMHh7Oui7cxN0o2IiIiMpz6f30YZh+d+Z6pxeNLO5yOG4/AQERGZRJMZh4eIiIioKWDgISIiItlj4CEiIiLZY+AhIiIi2WPgMSH2ByciImoaGHiIiIhI9hh4iIiISPYYeExIoVCYuwlEREQEBh4iIiJqBhh4iIiISPYYeIiIiEj2GHiIiIhI9hh4iIiISPYYeIiIiEj2GHiIiIhI9hh4TIhTSxARETUNDDxEREQkeww8REREJHsMPERERCR7DDxEREQkeww8REREJHsMPERERCR7DDxEREQkeww8REREJHsMPERERCR7DDwm5GSnNHcTiIiICAw8JuXr2sLcTSAiIiIw8BAREVEzwMBDREREssfAQ0RERLLHwENERESyx8BDREREssfAQ0RERLLHwENERESyx8BDREREssfAQ0RERLJncOBJTk7G448/Di8vLygUCmzcuFFvuRACc+fORatWrWBra4uIiAicOnVKr+batWsYNWoU1Go1nJycMG7cOBQWFurVHD16FP369YONjQ28vb2xcOHCKm1Zv349AgICYGNjg27dumHLli2GviwiIiKSIYMDT1FREbp3746lS5dWu3zhwoX45JNPsGLFCuzfvx8tWrRAVFQUbt26JdWMGjUKx44dQ0JCAjZt2oTk5GSMHz9eWq7T6RAZGYm2bdsiNTUVixYtwvz58/H5559LNXv37sWIESMwbtw4HD58GDExMYiJiUFGRoahL42IiIjkRhgBALFhwwbpfkVFhfD09BSLFi2SHsvPzxcqlUqsXr1aCCHE8ePHBQDx22+/STVbt24VCoVCXLx4UQghxLJly4Szs7MoLi6WambOnCn8/f2l+//4xz9EdHS0XnvCwsLEiy++WOf2a7VaAUBotdo6P6eu2s7cJN2IiIjIeOrz+W2SPjznzp2DRqNBRESE9JijoyPCwsKQkpICAEhJSYGTkxN69uwp1URERMDCwgL79++Xavr37w+l8q9Zx6OiopCZmYnr169LNXdup7KmcjvVKS4uhk6n07sRERGRfJkk8Gg0GgCAh4eH3uMeHh7SMo1GA3d3d73lVlZWcHFx0aupbh13buNeNZXLqxMfHw9HR0fp5u3tXd+XSERERPeRZnmV1uzZs6HVaqXb+fPnzd0kIiIiMiGTBB5PT08AQG5urt7jubm50jJPT0/k5eXpLS8rK8O1a9f0aqpbx53buFdN5fLqqFQqqNVqvRsRERHJl0kCj6+vLzw9PZGYmCg9ptPpsH//foSHhwMAwsPDkZ+fj9TUVKlmx44dqKioQFhYmFSTnJyM0tJSqSYhIQH+/v5wdnaWau7cTmVN5XaIiIiIDA48hYWFSEtLQ1paGoDbHZXT0tKQnZ0NhUKBKVOm4J133sGPP/6I9PR0PPPMM/Dy8kJMTAwAoHPnzhg8eDBeeOEFHDhwAHv27MGkSZMwfPhweHl5AQBGjhwJpVKJcePG4dixY1i7di2WLFmCuLg4qR2TJ0/Gtm3b8OGHH+LkyZOYP38+Dh48iEmTJhm+V4iIiEheDL0UbOfOnQJAlduYMWOEELcvTX/jjTeEh4eHUKlUYtCgQSIzM1NvHVevXhUjRowQ9vb2Qq1Wi7Fjx4qCggK9miNHjoi+ffsKlUolWrduLRYsWFClLevWrROdOnUSSqVSdOnSRWzevLler4WXpRMREd1/6vP5rRBCCDPmrSZBp9PB0dERWq3W6P152s3aLP2ctSDaqOsmIiJqzurz+d0sr9IiIiKi5oWBh4iIiGSPgYeIiIhkj4GHiIiIZI+Bh4iIiGSPgYeIiIhkj4GHiIiIZI+Bh4iIiGTPytwNaE7WHMgGAPi526NnOxczt4aIiKj5YOBpRLO+TwcAKBTAvtmD4KG2MXOLiIiImgee0mpEEZ09oLS0gBDA9Rsl5m4OERFRs8HA04i+GNMTalseVCMiImpsDDxEREQkeww8REREJHsMPERERCR7DDxEREQkeww8REREJHsMPERERCR7DDxEREQkeww8REREJHsMPERERCR7DDxEREQkeww8REREJHsMPERERCR7DDxEREQkeww8REREJHsMPERERCR7DDxEREQkeww8REREJHsMPERERCR7DDxEREQkeww8REREJHsMPERERCR7DDxEREQkeww8REREJHsMPERERCR7Jg088+fPh0Kh0LsFBARIy2/duoXY2Fi0bNkS9vb2GDp0KHJzc/XWkZ2djejoaNjZ2cHd3R3Tp09HWVmZXs2uXbvQo0cPqFQq+Pn5YdWqVaZ8WQ2kMHcDjEoIYe4mEBER1crkR3i6dOmCnJwc6bZ7925p2dSpU/HTTz9h/fr1SEpKwqVLl/DUU09Jy8vLyxEdHY2SkhLs3bsXX331FVatWoW5c+dKNefOnUN0dDQGDhyItLQ0TJkyBc8//zy2b99u6pfW7K1IOoMebyfgcPZ1czel0RUVl2HvmSsor2DgIyK6H1iZfANWVvD09KzyuFarxZdffolvv/0WDz/8MABg5cqV6Ny5M/bt24fevXvj559/xvHjx/HLL7/Aw8MDwcHBePvttzFz5kzMnz8fSqUSK1asgK+vLz788EMAQOfOnbF79258/PHHiIqKMvXLk6XisnIMXb4XjrbWeDXSHw/4OOst/z23AAfOXcOCrScBAE8u24sP/t4dT4e0MUdzG015hYClxe0jdM+uPIDfsq5jepQ/Ygf6mbllRERUG5Mf4Tl16hS8vLzQvn17jBo1CtnZ2QCA1NRUlJaWIiIiQqoNCAiAj48PUlJSAAApKSno1q0bPDw8pJqoqCjodDocO3ZMqrlzHZU1leuoTnFxMXQ6nd6N/pJwPBcZF3XYc/oqnly2F7m6W9Ky4rJyRH6cjDkbM/Se8+r6I7haWNzYTW0UGu0tzPzfUXR4bQv6LNiBkLcT8FvW7aNaK5LO4PUN6fhvSpZ5G0lERDUy6RGesLAwrFq1Cv7+/sjJycGbb76Jfv36ISMjAxqNBkqlEk5OTnrP8fDwgEajAQBoNBq9sFO5vHJZTTU6nQ43b96Era1tlXbFx8fjzTffNNbLlJ27T9OEvZcIlZUFWjvbYtbggHs8C3jmPwew+ZV+pm5eo3ty2R7kaG+Hvov5N/WWFdwqwzf7b4d4BxtrxDzQutHbR0REtTNp4BkyZIj0c1BQEMLCwtC2bVusW7eu2iDSWGbPno24uDjpvk6ng7e3t9nacz8oLqvA2ctFGP/f1HvWHLskryNlaefz4WRrLYWd2kxZm4a2Le2qnAIkIiLza9TL0p2cnNCpUyecPn0anp6eKCkpQX5+vl5Nbm6u1OfH09OzylVblfdrq1Gr1fcMVSqVCmq1Wu9GdKfz124gZukeDPhgV72e9+SyvTiYdc00jSIiIoM1auApLCzEmTNn0KpVK4SEhMDa2hqJiYnS8szMTGRnZyM8PBwAEB4ejvT0dOTl5Uk1CQkJUKvVCAwMlGruXEdlTeU6qH7+uFqEyWvSzN0Mszt9udDg5z69IgWX7jr1RURE5mXSwPPqq68iKSkJWVlZ2Lt3L5588klYWlpixIgRcHR0xLhx4xAXF4edO3ciNTUVY8eORXh4OHr37g0AiIyMRGBgIEaPHo0jR45g+/btmDNnDmJjY6FSqQAAEyZMwNmzZzFjxgycPHkSy5Ytw7p16zB16lRTvjTZeuun4+ZuQpOgqeNprHt57NPdtRcBWPfbeew9fQVXC4ux+WgOSsoqGrRdIiKqnkn78Fy4cAEjRozA1atX4ebmhr59+2Lfvn1wc3MDAHz88cewsLDA0KFDUVxcjKioKCxbtkx6vqWlJTZt2oSJEyciPDwcLVq0wJgxY/DWW29JNb6+vti8eTOmTp2KJUuWoE2bNvjiiy94SbqBSsr5gbs1PQezv09v0DquFZXUWpNxUYsZ3x3Ve+yVQR0R90inBm2bqDZCCCgU8hoElag2Jg08a9asqXG5jY0Nli5diqVLl96zpm3bttiyZUuN6xkwYAAOHz5sUBupebtaWIx3t5zA8F4+CPV1AQAs3XW6UbZ94XrV016fJJ5Cu5Z2eKqHvMc0IvOZvOYwjpzPx7Yp/WFjbWnu5hA1GpMPPEjNy94zV/BgB1dzN6PO5v90HD8duYTvD13E6492RniHlsi4aN6rzeLWHUH2tRsY4O+OfyefRe8OLTGkqydc7VVmbRfJww9plwAAAW9sQwulJULauSAm2Ishm2SPgcdM5DoF1ch/70fWgmhzN6PO/rhaJP387pYTRl13SVkFlFaGdZNb/MspLP7lFABgc3oO3tiYgV2vDkBZhYCfu70xm0nNWFFJOZJ/v4zk3y9Do7sF1xYqDAxwh5sDwzXJD2dLJzKRAYt2Gnd9H+xCxEdJ2JmZV3sx0V1ulpTjkY+S7rl84bZMzPjuKJ5esbcRW0XUeBh4qFkzZbfNSzVc6aW9UYq3Nxl2Rdz3hy4a2iRqxn48chGn8mofbuGPqzcaoTVEjY+Bh5o1c51ZfGvT8SrTVNTVT0cu4bvUC0j9gwMcUt1V1OPNvv/sVdM1hMhMGHio2Uo5cxVHL2jNsu3M3IZ1jJ62/giGLk9B1MfJepO7UvUu5t/EaxvScboORzgIGPb5PnM3gcjoGHio2Rrxb9P/UR/5730oNeHYRpm5BVi0PdNk65eLF746iG/3Z+PJZXvM3RQiMhMGHpIIIXC5oLjB69l4mH1MKu09cxW/HM+t8rjCiL2Hdp+6gqLiMqOtT46O59w+olZwi/uprj5NPIUyDkRKMsLAQ5KPEn7HSU1Bg9czZW1awxsjI9WNXi2M2HtIo7uF4TwFUWd7T18xdxMAAJcLinH8knnHfKrJhwm/Y93BC+ZuBpHRMPCQ5NMdjTPCMBlf+kUt/pV0xtzNuC+M/GK/uZsAAOj17i949JNfcTqv4V8y6sKQY4pnGzCJLlFTw8BDZGKr9mbpnXI6nH3dJKM5x289iQvXTXNJcV7BLTzznwMYu/IA5v2QASHXkTMbQWFxGeb+kCHdP5h1vVG2y/8xau4YeIhM7HB2vt4ozk8uM93Abn3f34mDWdeMHkje3XwCyb9fxs7My/gq5Q/8dDTHqOs3pc+Tm9aRr8UJv+PrlD/M3QyiZoeBh6gRJJ6o2nHZVJ5ekYJn/nPAqOu8e/b3V1YfRrtZmzFt3RF8ufsctDdKjbo9Y3pvy0lzN0HyUcLv+GL3Ob3HZn2fjp+PaUx+1MyQU1o8KkRywsBD1AhydcVYsPUkSsoa56qXX081Tsfc7w5dwNubjmPa+iONsr372ZXCYnySeKraZeP/m4rBi3816RAGRM0dAw+ZxNcpWeZuQpOzIukMOs3Z2mjb67dwBzIualFRnyF2q/Fb1rVaA9SOk413BMsYfjxyqcH7pa6uF5XgP7vPoec7v9RYl5lbgAPnOHq2uV0tLMYPaRcx5j8H8GniKdwsKTd3k8hIGHgamcKUkzc1IXN/OGbuJjR756/dxGOf7kbEx0kNOmXy9xUpRm5Zw9woKcPCbSdx9EK+wet4ZfVhtH9tS6Ncoh61OBlv1XHetKbWF/zL3eewJf3+6a9lDH//Vwomr0lD0u+X8WHC7+g8dxt+y2IQlQMGHhM7F/8oPh8dgoSp/c3dFGqmzl4uwvj/puKVNWk4lVuA89eMfyVXY35OL0k8hWW7zuBvnzV81ORnV/1mhBbdW9Lvl5FnhME8zemlbw6ZuwkmJ4TAH1eLIITA2ctFVZa/s/kEr0yUAStzN0DuFAoFIrt4mrsZRPjpyCX8dOQSAOCXuIfg525f63POXan6x9/cTuYYb9waU44kfOR8PsYYufO4oYQQOGKmeePuB//+9Sze23ISVhbVH4I/cj4f4746iGWjeuBg1nWcyNFh1+956OPnij4dXNHd26lxG0wGYeAhaoYiPkpCDx8nPNqtFZ7v177amoyLWjz26e46rU8I4L8pWRgd3s6IraxefU4Lrzt4vsblFQKIW5eG+X/rArWNdQNbpi/jUtMJGBsOX8TqA9nmbkaTtWDr7Sv5ymro17XjZB4C3tim99ie01cBZCJrQbQpm0dGwlNaRM3Uoex8vLP5RLV9YTYfzalz2Kn0RhPstzXjf0drrfn+0EUEzf8ZulvGvbTemPOlNdT3hzi/XU0UDexc+eupyygsLuNVdk0cAw9RM/e3z/bgm/1/4EbJX6NBx37bdPttmCpGPPPlAVy/a7yhhmguFyjIQUP/q0Z/eQBd521HnwU7jNIeMg0GHiLC6xsyEDh3O0rKKu45VkxT0dBv4/eSdj4fD7ydgB/SGn405HJBMTYcrv96Vu3NQq7uVoO3fzdjTlYrR8Z6S93vHdTljoGHmiVTXKkkB53mbMVHCb8b/Pz/pV5AcZlpxy0x9YGTyWvS8HED9gEAPL1ir0Fj6vxyIhf/+FfTGgagOTDm6cfBi5ORcuaq0dZHxsPAQwCAfWeN/wv65k9Nr09HpX4Ld5q7CbL06vojWLrTtHNXNcapoiWJpxoUiv+4ap7nkoGM+J46qSnAiH/vM94KyWgYeAgAMPxz4/+CrtyThbwC4x+ep6Yt6ffL5m6CUdws5Qi7zYUpMvT3hy6YYK3UEAw8ZFJl5ew7QMbWOL2BDR1njgPU3V8OZ19HsQnmuItbdwSZGuONGUUNx8BDRPetmiZjfW/LiQatO2bpnnqf1qqoEHhy2d4GbdcUOB/UvZny/+uS9maVxy7m3zTpgJeVtqTnYKMBHefljIGHTIrfdZshEx/huLNT9L2uaDqdV4jPk882aDs3S8sx87vax/HR2+7lQqSdz2/Qdk0h45KuUbf3W9Y1XLjOvkh3+vHIJcz9IQN9FuzAuK8OmnRbJWUVeOmbQ5iyNs2oQy3c7xh4iMiojlzQ1qvD+q169pW587L0e2Wrw9nX67XOe8m/Ub/BCO8xM4H5NeI3j+OXdPj7ihT0fX8ntmVomvXRpcq3Q17BLbyy+jC+TvkDwO1+boMXJ+PL3eewfNcZo58GLav46whS0R3jazV3DDzU7PAbj+mt3JNVp7p/J59FwBvbkHgit87rri1THMq+jul1GGG5Lur7MWSqMYLuF+ev3cC09Uek+xP+LxWd527DB9szUVHDtA3GlH+jpMn0o6pshe5m1eB8UlOAtzcdx/vbTmJLuqbG07P1caOkDL+cyPurDU1jVzQJDDzU7IS+94u5m9AsJJ7IrfWD590/+9m8eseHZG3uzBRf7K562irheN3DU21O5Ogw/8dj+HZ/Nk7nFdZab9FEA09jDTw46MMknMipevrss52n0f61LVi267RJtpujvYlHPkpCu1mbEfxWAuZszKhSU1EhsDMzD5cbcXDA5N8vI093q9b3Rey3hxD63i8NHsPqRkkZAuduxyurDzdoPXLFyUPJpHJ1t9DaybbB63l/20ks33UGw3p6Y85jneFgYw3tjVKoba3q/a26lFeONYpxXx3EJyMewN+6e+k9fq2oBAu2nsCh7HzpsdJyASFEnf4v76z4OuUPPNfHF+1cW/y1LiNfcbNqb5be/Xef7IqnQ9pAZWVZpdayqQaeBr7lt2XkYHDXVrXWldTSGXfhtkyczi3EWzFdYa8yzsfP6C/349dTV/Qe+2Z/Nvr63Z7F3NrSAtsyciAAzP3hGOxVVsh4M8oo267Nyj1ZWLknC852tU9Mm3+jFPN/PI6HOrnWaV/fray8AoFzt1d5/FL+TXi72NV7fXLEwEMm9dSyvfhxUh9cKSxGv45usLY07KDi8l23B7Nbe/A81t41A3a7lnawVVrBTmmJhwPc4evaAo8Eehi8LTKeV1YfhoudEn07ukqPzf/xGH48ckmvrrC4DJNWH8bSkT1qXefdoWj1gWzMfrSzdD/VSP137uX1DRl4fUMGFg4NwuXCYhQVl6GHjzO6tXHEtRvGOV06/uuDWPHPEFg0kU5BE/7vEFo52sDf0wHuDioM9HfHAH932Cqrhr7afH/4Ir4/fBFPBHth7mOBaGmvalDb7g47lSZ+U/18cIXFZXUO18ZyvY59wVYfyMbqA9kGzb6uu1V9X51hn+/D56NDMMDfHUqr5v03kYFHZq4VlcDBxqpJfdj/7bM9AAAPtQr7X4uo9/NrOy2SdcfItKl//PVht31Kf/h7OtR7e2Rc//xyP+Kf6oZhPb1RcKsMW9Jzqq3bfDQHS0fWvr67P6a+TvlDL/Aca6QrkmbU8wqu+vj5eC4OZF1D7/YtG7Se4rJyCGGcPss52lvI0d6+Km7dwduD6r3+aGc82aM1XA0ILT+kXcIPaZcw0N8NUV084a5WIaStCxxtaz8aAtzuCPzD4Uu1F1aj/Wtb8PE/ghHzQGuDnt8UPb3i3pfXj/9vKoDbfxM7uLWA7lYZXFooG6tpTQYDz32sqLgMNtaWsPzzW2D21Rvov2gnOrdSY+vkfmZuXVW5umIM+1cK1r4YXmNdXsEtzP4uHUcuaNHetQV0t+p3pUylqMXJ+P6lB9HDx9mg55PxzP4+HbO/TzfKuu7+Yn73iMjG6vxpbtUNhnejpAwpZ66ij58rbKxrProihEDI27/gZmk5yk3UYfjdLSewYNtJ7Hp1gMGnTXZmXsbOzL9G544J9oJPyxZwc1AhV3sL/+jpDbWtFQ6cu4Y2znbo5GEPK0sLhL6baHC7hQCmrE1DsLeTwetoSsrKK3D2clGtdVGLk/GAjxMOZ+dj+5T+aO/Wokl9OTY1WQWepUuXYtGiRdBoNOjevTs+/fRThIaGmrtZJpF/owTBbyUgwNMB377QG/vOXpU6VVbXabCp2H/uGq4WFlc5jC2EQMrZqxj57/16j18pbFgHw6eW7cWvMwbyHLaMFBVX7dh57JIWahtrtHFueH+xpqLijiObN0rK8N2hi1i64zQ0d4w99PPU/ujk8ddRzMpTNReu30Df9xtnvrjyCoF+C3capa8eAGxM0z9q89lO/Y7O1pYKLBn+gFG2NeCDXUZZj7GN+c8BRAR6YHTvtnWq93t9a53XffjPvnNRi5MBAJ1bqfH9xAcNOj15v5FN4Fm7di3i4uKwYsUKhIWFYfHixYiKikJmZibc3d3N3Tyj23369nnrk5oCjPpiP07k6KAy8PxsQ2eGrq+Z36XjizE99R6bszED3+zPNsn2KicKfWlAB/yzjn9AyDxulZbXeuQipZqJbqM/2Q0ACPV1MUm7zOE/u89h/NcHa+xkH/lxMl7s3x7/umOQxVcjO+GDnxv3dxq4PYJwYygtF3jpHv1z5CLp98tI+v0yRob6wEJR83AHqX9ca9C2TuTo0HnuNiwcGgQ/D3sEtXaElUyP+ihEUxmwoIHCwsLQq1cvfPbZZwCAiooKeHt74+WXX8asWbNqfK5Op4OjoyO0Wi3UarVJ29nr3V9wuaAYWyf3Q+dWhm9rS3pOjb/0x9+KwqE/8hHW/vYHwIXrN+Hr2gJnLxdCoVDA17UFrhWVQHez1Gzfcr5+LhT9O7kBANrN2myWNlDTc2RuJBxruKqF7xVqLhxsrNC9jRM+G/kAkn6/jExNAWIH+qGFygpZV4pM9rf70W6e2JKuwdKRPaCyskCwj5NB/bQaQ30+v2UReEpKSmBnZ4f//e9/iImJkR4fM2YM8vPz8cMPP+jVFxcXo7j4r1MlOp0O3t7ejRJ4Kv9Yt3dtgYBWDtLlorc7FgrpZ+B2R8O//neE3mBSRHLXtbUabZzsICCkjrfGHGOHiEzP1V6JBzu4wtrSAp6OKkyPCjDq+usTeGRxSuvKlSsoLy+Hh4eH3uMeHh44efJklfr4+Hi8+eabjdU8Pe1dW+DslSLpRkTVy7ioQ8bFptsfjYhqd6WwRBqGooNbC6MHnvqQReCpr9mzZyMuLk66X3mEpzF8PCwYa347jwBPB+lqEwUgXXqi+OtHKKDQq3ln8wkUFpehtZNto50vJzKXNx4LlPqlKRS3fx/2nL6Czfe4rJ2Imp7JgzpCbWuN0vIKONiYN3LIIvC4urrC0tISubn6h7tzc3Ph6elZpV6lUkGlMs/5yO7eTuhu4KWQw0N9pJ9vlZYj4I1t96w9/lYU7JR1++/dmZmHsSt/M6hNDfHm37pgzIPtAABT1hyucnWGKTz1QGt8f/iiybdDhqtp0LWRYT7YzD481Ez8EvcQ1LZWcHewAQDcLCnXu5rq8+QzeG9L1bMYDdHJwx6ejrb44O9BcLFTyqoDsywCj1KpREhICBITE6U+PBUVFUhMTMSkSZPM2zgTsbG2RNL0AbBQKODtYocbJWUoqxAImv/z7eXVDHt/L0GtHU3VzGpV1yl18fAHMKRbK7z45wBZxjImvC0GdfZAXz9XadTaw+fzcY6nExudn7s9YoK9aryC6OTbgxu0jawF0fCfs7XaMWzkxs1BhQOvDUJBcZn0e79j2kN486fjSPr9ci3PNq7+ndwwrKc3Yr9t2NVTMwcHwF5liaguniguq0ArRxtYWVqgokLgalEJisvK0cb59hATcu68vuKfIejb0bXK9Bt3Xzo+vn8HhPm2xBNL9zR4m/MfD8TQkDZwsKnbwI/3I1kEHgCIi4vDmDFj0LNnT4SGhmLx4sUoKirC2LFjzd00k2nb8q/5gyqP5hydHwlLhaJeQ9I3doK/1xU4UV08cfiNR/DA2wl6j/89pA3Wp1645/qiunhg5uAAWFtawM1BVetlzTumPQTf2Vvq33AyyPoJ4WjtZAuvP8dpCe/QEkOXp1SpO/Peo9Igmg1x4LUIdH/r5wavpza927sg/0YpOnk4oGtrNYLaOGH45/uMtv5fZwzE5vQcDOvpjZul5Xh9Qzp2Zl5G19Zq/DSpr3SpstrGWu+o2FfPhaK0vALbj2kw6VvTTSLpaq/CjCh/hLV3kf4WxX5b9+crrSzwQj9fxAS3RkePmkdEt7BQwM1B/6i8g40VCu4xnUJdvfKwHz7ZYZoJTe+UPj8S3ebX/T2psrao81xj9T1j8PeQNnhlUEe8tiEdv566gnF9ffHao52N8rvX1Mkm8AwbNgyXL1/G3LlzodFoEBwcjG3btlXpyCx3agPSeV2HcjfEiFAfrD5Q9/F1nFsoqz2lEdLWGbO+T8dDndyw8OkgWChu929ysLGqdhLHmjTmHDrN3TfPh6FXO/2xcULaumDJ8GBMX39UmmzSQgGj/cF1tLPGkXmR6P5mw0PPqrG9sPfMVRy/pMNrj3ZG51YONb5/vhzTE+O+Otjg7QJAG2dbTHioAwDAGcDKsXUfRNXa0gKPBXkZLfD8NKkvMi5pEeztBJcWSpRVCIMHGhwU4I7Zj3aGt4ttvX9377R7xsM4fP46njXwdPzumQPRxtnO5IHnb9296n3UxL+WAHi3fz/TEy98Xfv7buWzvTAw4Pa4dP8dF1avbciBbAIPAEyaNEm2p7DuR0fmRcLR1lov8Kwd39ugdQ0P9cHTIW1kdT5ZzsJ8XbD6hd73PNL4RHBrPNqtFf64WoQfj+Tg8aD6zw5dycpCAecWSix8Okh6zNHWGk+HtMH/ajgyWJuXH/bDgD8nyayrQZ2N8wVryfBgowTzg3MicDqvEKsPZOOHBvSR69bGEd3aNOzUt0IBLHq6O54OadOg9VRytLOu1//NnQyZnNMQo8J88O6T3epc//7Qbghp6ywdDa2rRwI9MMDfDbsy730q01hHUO9nsgo8ZDh3BxXyCho2jcPdrC1v/3I918cX/9lzDv8dF4qwBkyGyLBz/1gzvnetH9jWlhbwc3dA3CMNm+B11pAAjOvrW2V7H/y9e4MCjzk9EWycSS1d7VVwtVdhTT2OsjZEJw97/J5bWOXxMeFt8cZjgWb5HXa0tYb25u35+E68NbhRp1Coa2YdEeqDlwZ0aNAUOKvG3j6V2bGaaSYOvD6o2YcdAOAnCAEAfnq5r9HXWfkLNvfxQKTPj0S/jm5G3wY1TaY8bdjDx0n6+cO/d8ezD7aTzWnKFkpLJE57yOjrdbJrnJmxv3ouFC8+1F6638VLjVVje+HNJ7o2etiZ+1ggDr3xiHTkz9e1RbVh54V+viZrgwJ/vS+/ei4UgwL+OiL1UCc3LBkejIVPB2He44FGme+vuolAd0x7SLrKq7njER4CAHiojf8LYWXx1y+fnHv+k747/6ibQqhvSxz6cwLEoUY6PXI3D7UKz/45ZEJj+mfvtujgZt/o2zWWVo62mD2kMwZ38UR5hUDPdqaf2yxp+gA8tGiX3mMPdXLDc31vB5nIQA9serkvfF1bVPNs4LVHO+N4jg57Tledo62h2rv9tc2HOrnhoU5u+GpvFtq7tWi0L4CtHOUzoW5DMfCQyfAIavMTHdQK7w8Nqr2wASYP6ggHGys8Emic/jIxwV6Ie8QfHo4qrNh1Fu1c7fC37l4GHzU68PoghL6baFhjTPQ709gzCD3g49xo26ruqszPnwmRflYoFOhaw9AbCoUCbiaYJ+qVh/2qnax4TCMG6Rf6+TaLWdDriqe0yCR+nTFQNqcZqO5G9PKp8+W0hrJVWiJ2oB861fNKlntZPPwB+LS0g8rKEpMjOuKJ4NYNeu+6O9hggH/TOn3bkLgza4j5pgKoCw+1Dfzc/zoqljonot5Xfxk7Dvb1c0VcpH+1p5hMbe5jgQCA5/+83Jz+wiM8ZBLGOB9N958g78YdxLI2PXycpNNf1Rnbp51JtvvxP4LxYUIm/m9f/ToLW5joS4KhB3j8PRykS+Obsp8m9UXnubdHnm9hQOBu6AGwcX19Ma6vL7ycbJF99QZaOZmvz8xzfX0xto98+rUZEwMPNUtWFgqUVTTuYf6mrr1bCywd2QOdPBxgoQD+uHoDaw+eh4UCOHpBi19PXanx+enzI5tcX611L4bDr5qrVob19MaEAR3QrqVpgrlzCyXG9+tQ78Dj27L6fiYNJYx+DKNpsVVa4sdJfQBUf4qrNjXtHUsLBcpr+Vvxxp9HVQDAx0Tvqfpg2KkeT2lRs/Tz1P7mbkKjevNvXfDdxPB7LndpocSOaQPQuZUalhYKKBQKtHNtgZmDAzA9KgAP16EjclMLO8C9hzJ4/+kg+Lq2MOkHQ0U9DxtMiehosk7Yhmb7+ykoBbVxQlAbJ4OeW1Mfp/2vDUIXL7WBraKmhEd4qFlqb4YrYZYMD0YPH2f0W7izUbf733Gh0hUhv8T1R8LxPPTr6IrHPt0NABjS1RMLnqq5o3Ft33AXPFX3wdWai/qOYD4lopOJWgKMfbAdvt3fOGPx3I/u9e7+v3FhcLVXYdPLt6fyKCmrQPpFLc5eLkTyqSvYe/oKerZrvA7a1DAMPCSxUBj+TfB+ZG2pQGl5473gysHk1o7vjWFGnHPpXqZH+ePZB9vp9Wnwc3eAn/vtzr4/TeqL5FOX8UK/9lBa1Xywt3LCxurMie6M4aE+xmm0CSwd2UNvUsvoBozqXB/OLZRY+WwvjF1V+9QH30180KRt6ejhYND77u75q+Rq2iOdsONEHm6Wlus93rejK4C/ThEprSwQ0tYZIW2d8fee3hBC8PTRfYSntEhia8C57/uZwlTXAFfjzuH0w9q3xLCe3ibd3kOd3PDSgA41duDs1sYRsQP9ag07wO0JWl+NrP4IxJ1XyDRF0UGt9AbDWzqyR6Nte2AdTgU+3t0LIW1Nf5SgcoLhe1k1thdeGvBXB2VvF1uTDzHQVLR3s0f6/EjMHFy/K9IYdu4vPMJDkmZ0cOe2Rvpb1bmVGu/EdNV7rL79O+rju4kPGv0DVKFQYNLDHfHvX89Jw/QDt09lPdSpaV2CXZ0pgzrBxU5ptLmu6mP2kADEbz2JpSN7IKy9C176v0M4kHVNWt5Y41XV1B/nkUAP9O/ohgH+7mjlaINrRaWYHNGxcRrWRFhZWmDigA4ouFWKZbvO4NFunuZuEhkZAw9JGnlsMrNrrO9mEZ3dq1w5Yqpd/e9nepr0aMHOVwfgX8ln8K+ks2jtZNukT2XdyVZpiRfNdHn1iw91wOjwttIRli+f7YkhS37Fhes34eagwquR/o3Sjnv9fo8Jb4s3n/grkI8Ob9co7Wmq4h7phIcD3Bs8WSo1PQw8JPlnbx/8+9dzDV7Pe/WYHdiczHk0OqiNo9Entpwx2B8RnU07rYNLCyVmDQ7A40FeaHePofqpqjtPJznYWEsDc5q7D0jblnZ6YYduH+lpjCkxqPGxDw9JpkcFGKU/xsiw++Nbf2Op7uNsZKgPRlcz7HxDvDTAr1E+PCuH6jf1iMpyVvn/1Jhhp7oDPP83LqzRtk9kbgw8JFFaWaDfn1clNAf16c/R2skWffxaGrSdJ3tUHVvFytICcx67Pey7g03Dg8M3z/ODi2p291gzrwzqyBHRqVnhVzTS05z68cQ/1Q2//n4Zultl0mO927tg+agQjP/vQfyWdR3B3k744O9B0qXcf1wtqjIzc02OzI2Eo13147GorCxx4q3BsLAA/OdsM/h1RHdrhT5+zSeoknHw+iJqbhh4qNlS21hjbB9fLEk8BQDIWhAtLVv3Yni1pxvatmyB/44LxegvD9RpG/cKO5UqZzKu63gtlVRWFiguq8DEAR0woX/Tn+uIzO/u7zIdPZr2cAJExsbAQ3osG+sa2Saupr4V/Tq6YWNsH8Qs3WO07Q0McMeReZF44euDuPjn1Ttp5/OrrX2okxs+fyYEpeWC/Wiozu48ejv/8UBEd2ucARiJmgr+tSQ9zS3v2CkNG2wx2NsJGW9G4Zkv99c4G3d9ONpaY92Lf813lVdwC6O/OIDM3AIAtyf3fLBDS0we1AkqK0sw61B93Pll5tk+vmZsCZF58E8m6WlOfXgA4J+92yLxRB4eCaz/gHT2KissfDoIER8lV7t856sDGtQ2dwcbbJvSD1+n/IFALzV68VJZaoCg1o54sENLtHayNXdTiMyCgYeatRYqK6ybcO9ZxGvT3tUe4e1bIuXs1SrLfI0wTo1CocCYB9s1eD1EFhYKfPtCb3M3g8hseFk66blfRs5tKiwsFFg9vjfOxT9q7qYQEVENGHhIj5+7Pb6baPgRj+ZKoVBgzXh+eyYiaqp4SouqqGmG7dp8OuIBI7bk/tK7fUucfHswViSdQYQZJqkkIqJ7Y+Aho3q8u5e5m2BWNtaWmBLRydzNICKiu/CUFlXh7czh5omISF4YeKiKFior/PZ6hLmbQUREZDQMPFQtNweVuZtARERkNAw8REREJHsMPERERCR7DDxEREQkeww8dE/bpvQzdxOIiIiMgoGH7inAUw0/d3tzN4OIiKjBGHioRt9NfLDOtSPDOA8XERE1TQw8VCNHW2tYKP66v76GmcXfjenaCC0iIiKqP5MFnnbt2kGhUOjdFixYoFdz9OhR9OvXDzY2NvD29sbChQurrGf9+vUICAiAjY0NunXrhi1btugtF0Jg7ty5aNWqFWxtbREREYFTp06Z6mU1e73auVT7uKfaBgqFotplRERE5mbSIzxvvfUWcnJypNvLL78sLdPpdIiMjETbtm2RmpqKRYsWYf78+fj888+lmr1792LEiBEYN24cDh8+jJiYGMTExCAjI0OqWbhwIT755BOsWLEC+/fvR4sWLRAVFYVbt26Z8qU1K/95thesLRX44O/d71mz9kXOFE5ERE2XQgghTLHidu3aYcqUKZgyZUq1y5cvX47XX38dGo0GSqUSADBr1ixs3LgRJ0+eBAAMGzYMRUVF2LRpk/S83r17Izg4GCtWrIAQAl5eXpg2bRpeffVVAIBWq4WHhwdWrVqF4cOH16mtOp0Ojo6O0Gq1UKvVDXjV8lVeIWD557mtK4XF6PnOL9IyHxc7JM8YaK6mERFRM1Wfz2+THuFZsGABWrZsiQceeACLFi1CWVmZtCwlJQX9+/eXwg4AREVFITMzE9evX5dqIiL053SKiopCSkoKAODcuXPQaDR6NY6OjggLC5NqqlNcXAydTqd3o5pZ3tGRx9Vef9qJx4JaNXZziIiI6sXKVCt+5ZVX0KNHD7i4uGDv3r2YPXs2cnJy8NFHHwEANBoNfH199Z7j4eEhLXN2doZGo5Eeu7NGo9FIdXc+r7qa6sTHx+PNN99s2Ats5hKm9kfiyTy0dbHDoM4etT+BiIjIjOoVeGbNmoX333+/xpoTJ04gICAAcXFx0mNBQUFQKpV48cUXER8fD5XKvBNTzp49W699Op0O3t7eZmzR/aejhwM6ejiYuxlERER1Uq/AM23aNDz77LM11rRv377ax8PCwlBWVoasrCz4+/vD09MTubm5ejWV9z09PaV/q6u5c3nlY61atdKrCQ4OvmcbVSqV2UMXERERNZ56BR43Nze4ubkZtKG0tDRYWFjA3d0dABAeHo7XX38dpaWlsLa2BgAkJCTA398fzs7OUk1iYqJex+eEhASEh98eC8bX1xeenp5ITEyUAo5Op8P+/fsxceJEg9pJRERE8mOSTsspKSlYvHgxjhw5grNnz+Kbb77B1KlT8c9//lMKMyNHjoRSqcS4ceNw7NgxrF27FkuWLNE71TR58mRs27YNH374IU6ePIn58+fj4MGDmDRpEgBAoVBgypQpeOedd/Djjz8iPT0dzzzzDLy8vBATE2OKl0ZERET3I2ECqampIiwsTDg6OgobGxvRuXNn8d5774lbt27p1R05ckT07dtXqFQq0bp1a7FgwYIq61q3bp3o1KmTUCqVokuXLmLz5s16yysqKsQbb7whPDw8hEqlEoMGDRKZmZn1aq9WqxUAhFarrf+LJSIiIrOoz+e3ycbhuZ9wHB4iIqL7T5MZh4eIiIioKWDgISIiItlj4CEiIiLZY+AhIiIi2WPgISIiItlj4CEiIiLZY+AhIiIi2WPgISIiItmr11xaclU59qJOpzNzS4iIiKiuKj+36zKGMgMPgIKCAgCAt7e3mVtCRERE9VVQUABHR8caazi1BICKigpcunQJDg4OUCgURl23TqeDt7c3zp8/z2krjIz71jS4X02H+9Z0uG9No6nvVyEECgoK4OXlBQuLmnvp8AgPAAsLC7Rp08ak21Cr1U3yzSIH3Lemwf1qOty3psN9axpNeb/WdmSnEjstExERkewx8BAREZHsMfCYmEqlwrx586BSqczdFNnhvjUN7lfT4b41He5b05DTfmWnZSIiIpI9HuEhIiIi2WPgISIiItlj4CEiIiLZY+AhIiIi2WPgMaGlS5eiXbt2sLGxQVhYGA4cOGDuJplVcnIyHn/8cXh5eUGhUGDjxo16y4UQmDt3Llq1agVbW1tERETg1KlTejXXrl3DqFGjoFar4eTkhHHjxqGwsFCv5ujRo+jXrx9sbGzg7e2NhQsXVmnL+vXrERAQABsbG3Tr1g1btmwx+uttLPHx8ejVqxccHBzg7u6OmJgYZGZm6tXcunULsbGxaNmyJezt7TF06FDk5ubq1WRnZyM6Ohp2dnZwd3fH9OnTUVZWpleza9cu9OjRAyqVCn5+fli1alWV9sjpfb98+XIEBQVJg66Fh4dj69at0nLuV+NZsGABFAoFpkyZIj3G/WuY+fPnQ6FQ6N0CAgKk5c12vwoyiTVr1gilUin+85//iGPHjokXXnhBODk5idzcXHM3zWy2bNkiXn/9dfH9998LAGLDhg16yxcsWCAcHR3Fxo0bxZEjR8Tf/vY34evrK27evCnVDB48WHTv3l3s27dP/Prrr8LPz0+MGDFCWq7VaoWHh4cYNWqUyMjIEKtXrxa2trbiX//6l1SzZ88eYWlpKRYuXCiOHz8u5syZI6ytrUV6errJ94EpREVFiZUrV4qMjAyRlpYmHn30UeHj4yMKCwulmgkTJghvb2+RmJgoDh48KHr37i0efPBBaXlZWZno2rWriIiIEIcPHxZbtmwRrq6uYvbs2VLN2bNnhZ2dnYiLixPHjx8Xn376qbC0tBTbtm2TauT2vv/xxx/F5s2bxe+//y4yMzPFa6+9JqytrUVGRoYQgvvVWA4cOCDatWsngoKCxOTJk6XHuX8NM2/ePNGlSxeRk5Mj3S5fviwtb677lYHHREJDQ0VsbKx0v7y8XHh5eYn4+HgztqrpuDvwVFRUCE9PT7Fo0SLpsfz8fKFSqcTq1auFEEIcP35cABC//fabVLN161ahUCjExYsXhRBCLFu2TDg7O4vi4mKpZubMmcLf31+6/49//ENER0frtScsLEy8+OKLRn2N5pKXlycAiKSkJCHE7f1obW0t1q9fL9WcOHFCABApKSlCiNth1MLCQmg0Gqlm+fLlQq1WS/tyxowZokuXLnrbGjZsmIiKipLuN4f3vbOzs/jiiy+4X42koKBAdOzYUSQkJIiHHnpICjzcv4abN2+e6N69e7XLmvN+5SktEygpKUFqaioiIiKkxywsLBAREYGUlBQztqzpOnfuHDQajd4+c3R0RFhYmLTPUlJS4OTkhJ49e0o1ERERsLCwwP79+6Wa/v37Q6lUSjVRUVHIzMzE9evXpZo7t1NZI5f/G61WCwBwcXEBAKSmpqK0tFTvNQcEBMDHx0dv33br1g0eHh5STVRUFHQ6HY4dOybV1LTf5P6+Ly8vx5o1a1BUVITw8HDuVyOJjY1FdHR0lX3A/dswp06dgpeXF9q3b49Ro0YhOzsbQPPerww8JnDlyhWUl5frvVkAwMPDAxqNxkytatoq90tN+0yj0cDd3V1vuZWVFVxcXPRqqlvHndu4V40c/m8qKiowZcoU9OnTB127dgVw+/UqlUo4OTnp1d69bw3dbzqdDjdv3pTt+z49PR329vZQqVSYMGECNmzYgMDAQO5XI1izZg0OHTqE+Pj4Ksu4fw0XFhaGVatWYdu2bVi+fDnOnTuHfv36oaCgoFnvV86WTiQjsbGxyMjIwO7du83dFNnw9/dHWloatFot/ve//2HMmDFISkoyd7Pue+fPn8fkyZORkJAAGxsbczdHVoYMGSL9HBQUhLCwMLRt2xbr1q2Dra2tGVtmXjzCYwKurq6wtLSs0us9NzcXnp6eZmpV01a5X2raZ56ensjLy9NbXlZWhmvXrunVVLeOO7dxr5r7/f9m0qRJ2LRpE3bu3Ik2bdpIj3t6eqKkpAT5+fl69XfvW0P3m1qthq2trWzf90qlEn5+fggJCUF8fDy6d++OJUuWcL82UGpqKvLy8tCjRw9YWVnBysoKSUlJ+OSTT2BlZQUPDw/uXyNxcnJCp06dcPr06Wb9vmXgMQGlUomQkBAkJiZKj1VUVCAxMRHh4eFmbFnT5evrC09PT719ptPpsH//fmmfhYeHIz8/H6mpqVLNjh07UFFRgbCwMKkmOTkZpaWlUk1CQgL8/f3h7Ows1dy5ncqa+/X/RgiBSZMmYcOGDdixYwd8fX31loeEhMDa2lrvNWdmZiI7O1tv36anp+sFyoSEBKjVagQGBko1Ne235vK+r6ioQHFxMfdrAw0aNAjp6elIS0uTbj179sSoUaOkn7l/jaOwsBBnzpxBq1atmvf71ixdpZuBNWvWCJVKJVatWiWOHz8uxo8fL5ycnPR6vTc3BQUF4vDhw+Lw4cMCgPjoo4/E4cOHxR9//CGEuH1ZupOTk/jhhx/E0aNHxRNPPFHtZekPPPCA2L9/v9i9e7fo2LGj3mXp+fn5wsPDQ4wePVpkZGSINWvWCDs7uyqXpVtZWYkPPvhAnDhxQsybN+++vix94sSJwtHRUezatUvvMtQbN25INRMmTBA+Pj5ix44d4uDBgyI8PFyEh4dLyysvQ42MjBRpaWli27Ztws3NrdrLUKdPny5OnDghli5dWu1lqHJ638+aNUskJSWJc+fOiaNHj4pZs2YJhUIhfv75ZyEE96ux3XmVlhDcv4aaNm2a2LVrlzh37pzYs2ePiIiIEK6uriIvL08I0Xz3KwOPCX366afCx8dHKJVKERoaKvbt22fuJpnVzp07BYAqtzFjxgghbl+a/sYbbwgPDw+hUqnEoEGDRGZmpt46rl69KkaMGCHs7e2FWq0WY8eOFQUFBXo1R44cEX379hUqlUq0bt1aLFiwoEpb1q1bJzp16iSUSqXo0qWL2Lx5s8let6lVt08BiJUrV0o1N2/eFC+99JJwdnYWdnZ24sknnxQ5OTl668nKyhJDhgwRtra2wtXVVUybNk2Ulpbq1ezcuVMEBwcLpVIp2rdvr7eNSnJ63z/33HOibdu2QqlUCjc3NzFo0CAp7AjB/Wpsdwce7l/DDBs2TLRq1UoolUrRunVrMWzYMHH69GlpeXPdrwohhDDPsSUiIiKixsE+PERERCR7DDxEREQkeww8REREJHsMPERERCR7DDxEREQkeww8REREJHsMPERERCR7DDxEREQkeww8REREJHsMPERERCR7DDxEREQkeww8REREJHv/D00mSG+XDVIAAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# graph one audio to see start signal that we will remove\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_df['audio'][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>ipa</th>\n",
       "      <th>phoneme_starts</th>\n",
       "      <th>phoneme_ends</th>\n",
       "      <th>cropped_audio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>ðəbɔstnbæleɪoʊvəkeɪmðɛɹfʌndiŋʃɔɹɾiddʒ</td>\n",
       "      <td>[0, 2152, 2398, 2837, 4270, 4501, 6840, 7394, ...</td>\n",
       "      <td>[2152, 2398, 2837, 4270, 4501, 6840, 7394, 787...</td>\n",
       "      <td>[-1, 1, 0, -2, -4, 1, -4, 6, 1, -3, 1, 3, -1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>flaɪiŋstændaɪkinbipɹækəklʔifjuwɑntuseɪvmʌni</td>\n",
       "      <td>[0, 2680, 3480, 4135, 5723, 6600, 7320, 9080, ...</td>\n",
       "      <td>[2680, 3480, 4135, 5723, 6600, 7320, 9080, 960...</td>\n",
       "      <td>[4, 17, -2, -4, -5, -1, -4, -3, 7, -10, -2, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>ʃihædjɹdɑɹksutʔinɡɹisiwɔʃwɔɾɹʔɔljɪɹ</td>\n",
       "      <td>[0, 2400, 4470, 5480, 6600, 8898, 9566, 10151,...</td>\n",
       "      <td>[2400, 4470, 5480, 6600, 8898, 9566, 10151, 11...</td>\n",
       "      <td>[-4, 1, -4, -1, -3, -7, 0, 3, 4, -2, -6, -3, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>hɛlpsɛləbɹeɪtjɹbɹʌðɹsiksɛs</td>\n",
       "      <td>[0, 4280, 4920, 6146, 6760, 7951, 10760, 12320...</td>\n",
       "      <td>[4280, 4920, 6146, 6760, 7951, 10760, 12320, 1...</td>\n",
       "      <td>[15, -4, -4, 15, 18, 2, 12, 16, -11, -21, 2, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>eɪvimæθjusʔɪtsdzisɡʌstiŋnəweɪjɹʔɑlisʔidɪŋ</td>\n",
       "      <td>[0, 2300, 2760, 4971, 5890, 7060, 8981, 12040,...</td>\n",
       "      <td>[2300, 2760, 4971, 5890, 7060, 8981, 12040, 13...</td>\n",
       "      <td>[7, 3, 5, 6, 6, 6, 5, 4, 4, 3, 8, 4, 6, 1, 4, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               audio  \\\n",
       "0  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "1  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "2  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "3  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "4  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "\n",
       "                                           ipa  \\\n",
       "0        ðəbɔstnbæleɪoʊvəkeɪmðɛɹfʌndiŋʃɔɹɾiddʒ   \n",
       "1  flaɪiŋstændaɪkinbipɹækəklʔifjuwɑntuseɪvmʌni   \n",
       "2          ʃihædjɹdɑɹksutʔinɡɹisiwɔʃwɔɾɹʔɔljɪɹ   \n",
       "3                   hɛlpsɛləbɹeɪtjɹbɹʌðɹsiksɛs   \n",
       "4    eɪvimæθjusʔɪtsdzisɡʌstiŋnəweɪjɹʔɑlisʔidɪŋ   \n",
       "\n",
       "                                      phoneme_starts  \\\n",
       "0  [0, 2152, 2398, 2837, 4270, 4501, 6840, 7394, ...   \n",
       "1  [0, 2680, 3480, 4135, 5723, 6600, 7320, 9080, ...   \n",
       "2  [0, 2400, 4470, 5480, 6600, 8898, 9566, 10151,...   \n",
       "3  [0, 4280, 4920, 6146, 6760, 7951, 10760, 12320...   \n",
       "4  [0, 2300, 2760, 4971, 5890, 7060, 8981, 12040,...   \n",
       "\n",
       "                                        phoneme_ends  \\\n",
       "0  [2152, 2398, 2837, 4270, 4501, 6840, 7394, 787...   \n",
       "1  [2680, 3480, 4135, 5723, 6600, 7320, 9080, 960...   \n",
       "2  [2400, 4470, 5480, 6600, 8898, 9566, 10151, 11...   \n",
       "3  [4280, 4920, 6146, 6760, 7951, 10760, 12320, 1...   \n",
       "4  [2300, 2760, 4971, 5890, 7060, 8981, 12040, 13...   \n",
       "\n",
       "                                       cropped_audio  \n",
       "0  [-1, 1, 0, -2, -4, 1, -4, 6, 1, -3, 1, 3, -1, ...  \n",
       "1  [4, 17, -2, -4, -5, -1, -4, -3, 7, -10, -2, -1...  \n",
       "2  [-4, 1, -4, -1, -3, -7, 0, 3, 4, -2, -6, -3, 1...  \n",
       "3  [15, -4, -4, 15, 18, 2, 12, 16, -11, -21, 2, 1...  \n",
       "4  [7, 3, 5, 6, 6, 6, 5, 4, 4, 3, 8, 4, 6, 1, 4, ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the process_row function to each row in the DataFrame\n",
    "train_df['cropped_audio'] = train_df.apply(crop_audio, axis=1) \n",
    "train_df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa523203d60>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize again, much better!\n",
    "plt.plot(train_df['cropped_audio'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ipa</th>\n",
       "      <th>audio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ðəbɔstnbæleɪoʊvəkeɪmðɛɹfʌndiŋʃɔɹɾiddʒ</td>\n",
       "      <td>[-1, 1, 0, -2, -4, 1, -4, 6, 1, -3, 1, 3, -1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flaɪiŋstændaɪkinbipɹækəklʔifjuwɑntuseɪvmʌni</td>\n",
       "      <td>[4, 17, -2, -4, -5, -1, -4, -3, 7, -10, -2, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ʃihædjɹdɑɹksutʔinɡɹisiwɔʃwɔɾɹʔɔljɪɹ</td>\n",
       "      <td>[-4, 1, -4, -1, -3, -7, 0, 3, 4, -2, -6, -3, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hɛlpsɛləbɹeɪtjɹbɹʌðɹsiksɛs</td>\n",
       "      <td>[15, -4, -4, 15, 18, 2, 12, 16, -11, -21, 2, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eɪvimæθjusʔɪtsdzisɡʌstiŋnəweɪjɹʔɑlisʔidɪŋ</td>\n",
       "      <td>[7, 3, 5, 6, 6, 6, 5, 4, 4, 3, 8, 4, 6, 1, 4, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           ipa  \\\n",
       "0        ðəbɔstnbæleɪoʊvəkeɪmðɛɹfʌndiŋʃɔɹɾiddʒ   \n",
       "1  flaɪiŋstændaɪkinbipɹækəklʔifjuwɑntuseɪvmʌni   \n",
       "2          ʃihædjɹdɑɹksutʔinɡɹisiwɔʃwɔɾɹʔɔljɪɹ   \n",
       "3                   hɛlpsɛləbɹeɪtjɹbɹʌðɹsiksɛs   \n",
       "4    eɪvimæθjusʔɪtsdzisɡʌstiŋnəweɪjɹʔɑlisʔidɪŋ   \n",
       "\n",
       "                                               audio  \n",
       "0  [-1, 1, 0, -2, -4, 1, -4, 6, 1, -3, 1, 3, -1, ...  \n",
       "1  [4, 17, -2, -4, -5, -1, -4, -3, 7, -10, -2, -1...  \n",
       "2  [-4, 1, -4, -1, -3, -7, 0, 3, 4, -2, -6, -3, 1...  \n",
       "3  [15, -4, -4, 15, 18, 2, 12, 16, -11, -21, 2, 1...  \n",
       "4  [7, 3, 5, 6, 6, 6, 5, 4, 4, 3, 8, 4, 6, 1, 4, ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_df comprised of just cropped_audio and phonemes\n",
    "train_df = train_df.drop(columns=['phoneme_starts', 'phoneme_ends', 'audio'])\n",
    "# rename cropped audio to audio\n",
    "train_df = train_df.rename(columns={'cropped_audio': 'audio'})\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend Phoneme Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_vocab = set(\"\".join(train_df['ipa']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'æ', 'ɡ', 'ɛ', 't', 'ʊ', 'ə', 'h', 'o', 'z', 'd', 's', 'n', 'ʒ', 'b', 'ɑ', 'ɹ', 'ŋ', 'ɪ', 'ʃ', 'ʔ', 'a', 'i', 'e', 'v', 'θ', 'ɔ', 'f', 'm', 'ð', 'j', 'u', 'w', 'k', 'ɾ', 'p', 'l', 'ʌ'}\n"
     ]
    }
   ],
   "source": [
    "print(timit_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<s>': 1, '<pad>': 0, '</s>': 2, '<unk>': 3, 'n': 4, 's': 5, 't': 6, 'ə': 7, 'l': 8, 'a': 9, 'i': 10, 'k': 11, 'd': 12, 'm': 13, 'ɛ': 14, 'ɾ': 15, 'e': 16, 'ɪ': 17, 'p': 18, 'o': 19, 'ɐ': 20, 'z': 21, 'ð': 22, 'f': 23, 'j': 24, 'v': 25, 'b': 26, 'ɹ': 27, 'ʁ': 28, 'ʊ': 29, 'iː': 30, 'r': 31, 'w': 32, 'ʌ': 33, 'u': 34, 'ɡ': 35, 'æ': 36, 'aɪ': 37, 'ʃ': 38, 'h': 39, 'ɔ': 40, 'ɑː': 41, 'ŋ': 42, 'ɚ': 43, 'eɪ': 44, 'β': 45, 'uː': 46, 'y': 47, 'ɑ̃': 48, 'oʊ': 49, 'ᵻ': 50, 'eː': 51, 'θ': 52, 'aʊ': 53, 'ts': 54, 'oː': 55, 'ɔ̃': 56, 'ɣ': 57, 'ɜ': 58, 'ɑ': 59, 'dʒ': 60, 'əl': 61, 'x': 62, 'ɜː': 63, 'ç': 64, 'ʒ': 65, 'tʃ': 66, 'ɔː': 67, 'ɑːɹ': 68, 'ɛ̃': 69, 'ʎ': 70, 'ɔːɹ': 71, 'ʋ': 72, 'aː': 73, 'ɕ': 74, 'œ': 75, 'ø': 76, 'oːɹ': 77, 'ɲ': 78, 'yː': 79, 'ʔ': 80, 'iə': 81, 'i5': 82, 's.': 83, 'tɕ': 84, '??': 85, 'nʲ': 86, 'ɛː': 87, 'œ̃': 88, 'ɭ': 89, 'ɔø': 90, 'ʑ': 91, 'tʲ': 92, 'ɨ': 93, 'ɛɹ': 94, 'ts.': 95, 'rʲ': 96, 'ɪɹ': 97, 'ɭʲ': 98, 'i.5': 99, 'ɔɪ': 100, 'q': 101, 'sʲ': 102, 'u5': 103, 'ʊɹ': 104, 'iɜ': 105, 'a5': 106, 'iɛ5': 107, 'øː': 108, 'ʕ': 109, 'ja': 110, 'əɜ': 111, 'th': 112, 'ɑ5': 113, 'oɪ': 114, 'dʲ': 115, 'ə5': 116, 'tɕh': 117, 'ts.h': 118, 'mʲ': 119, 'ɯ': 120, 'dʑ': 121, 'vʲ': 122, 'e̞': 123, 'tʃʲ': 124, 'ei5': 125, 'o5': 126, 'onɡ5': 127, 'ɑu5': 128, 'iɑ5': 129, 'ai5': 130, 'aɪɚ': 131, 'kh': 132, 'ə1': 133, 'ʐ': 134, 'i2': 135, 'ʉ': 136, 'ħ': 137, 't[': 138, 'aɪə': 139, 'ʲ': 140, 'ju': 141, 'ə2': 142, 'u2': 143, 'oɜ': 144, 'pː': 145, 'iɛɜ': 146, 'ou5': 147, 'y5': 148, 'uɜ': 149, 'tː': 150, 'uo5': 151, 'd[': 152, 'uoɜ': 153, 'tsh': 154, 'ɑɜ': 155, 'ɵ': 156, 'i̪5': 157, 'uei5': 158, 'ɟ': 159, 'aɜ': 160, 'ɑɨ': 161, 'i.ɜ': 162, 'eʊ': 163, 'o2': 164, 'ɐ̃': 165, 'ä': 166, 'pʲ': 167, 'kʲ': 168, 'n̩': 169, 'ɒ': 170, 'ph': 171, 'ɑu2': 172, 'uɨ': 173, 'əɪ': 174, 'ɫ': 175, 'ɬ': 176, 'yɜ': 177, 'bʲ': 178, 'ɑ2': 179, 's̪': 180, 'aiɜ': 181, 'χ': 182, 'ɐ̃ʊ̃': 183, '1': 184, 'ə4': 185, 'yæɜ': 186, 'a2': 187, 'ɨː': 188, 't̪': 189, 'iouɜ': 190, 'ũ': 191, 'onɡɜ': 192, 'aɨ': 193, 'iɛ2': 194, 'ɔɨ': 195, 'ɑuɜ': 196, 'o̞': 197, 'ei2': 198, 'iou2': 199, 'c': 200, 'kː': 201, 'y2': 202, 'ɖ': 203, 'oe': 204, 'dˤ': 205, 'yɛɜ': 206, 'əʊ': 207, 'S': 208, 'ɡʲ': 209, 'onɡ2': 210, 'u\"': 211, 'eiɜ': 212, 'ʈ': 213, 'ɯᵝ': 214, 'iou5': 215, 'dZ': 216, 'r̝̊': 217, 'i.2': 218, 'tS': 219, 's^': 220, 'ʝ': 221, 'yə5': 222, 'iɑɜ': 223, 'uə5': 224, 'pf': 225, 'ɨu': 226, 'iɑ2': 227, 'ou2': 228, 'ər2': 229, 'fʲ': 230, 'ai2': 231, 'r̝': 232, 'uəɜ': 233, 'ɳ': 234, 'əɨ': 235, 'ua5': 236, 'uɪ': 237, 'ɽ': 238, 'bː': 239, 'yu5': 240, 'uo2': 241, 'yɛ5': 242, 'l̩': 243, 'ɻ': 244, 'ərɜ': 245, 'ʂ': 246, 'i̪2': 247, 'ouɜ': 248, 'uaɜ': 249, 'a.': 250, 'a.ː': 251, 'yæ5': 252, 'dː': 253, 'r̩': 254, 'ee': 255, 'ɪu': 256, 'ər5': 257, 'i̪ɜ': 258, 'æi': 259, 'u:': 260, 'i.ː': 261, 't^': 262, 'o1': 263, 'ɪ^': 264, 'ai': 265, 'ueiɜ': 266, 'æː': 267, 'ɛɪ': 268, 'eə': 269, 'i.': 270, 'ɴ': 271, 'ie': 272, 'ua2': 273, 'ɑ1': 274, 'o4': 275, 'tʃː': 276, 'o:': 277, 'ɑ:': 278, 'u1': 279, 'N': 280, 'i̪1': 281, 'au': 282, 'yæ2': 283, 'u.': 284, 'qː': 285, 'yəɜ': 286, 'y:': 287, 'kʰ': 288, 'tʃʰ': 289, 'iʊ': 290, 'sx': 291, 'õ': 292, 'uo': 293, 'tʰ': 294, 'uai5': 295, 'bʰ': 296, 'u.ː': 297, 'uə2': 298, 'ʊə': 299, 'd^': 300, 's̪ː': 301, 'yiɜ': 302, 'dʰ': 303, 'r.': 304, 'oe:': 305, 'i1': 306, 'ɟː': 307, 'yu2': 308, 'nʲʲ': 309, 'i̪4': 310, 'uei2': 311, 'tsʲ': 312, 'ɸ': 313, 'ĩ': 314, 'ɑ4': 315, 't̪ː': 316, 'eɑ': 317, 'u4': 318, 'e:': 319, 'tsː': 320, 'ʈʰ': 321, 'ɡʰ': 322, 'ɯɯ': 323, 'dʒʲ': 324, 'ʂʲ': 325, 'X': 326, 'ɵː': 327, 'uaiɜ': 328, 'tɕʲ': 329, 'ã': 330, 't^ː': 331, 'ẽː': 332, 'yɛ2': 333, 'cː': 334, 'i.1': 335, 'ɛʊ': 336, 'dˤdˤ': 337, 'dʒː': 338, 'i4': 339, 'ɡː': 340, 'yi': 341, 'ɕʲ': 342, 'ɟʰ': 343, 'pʰ': 344, 'dʑʲ': 345, 'yuɜ': 346, 'ua1': 347, 'ua4': 348, 'æiː': 349, 'ɐɐ': 350, 'ui': 351, 'iou1': 352, 'ʊː': 353, 'a1': 354, 'iou4': 355, 'cʰ': 356, 'iɛ1': 357, 'yə2': 358, 'ɖʰ': 359, 'ẽ': 360, 'ʒʲ': 361, 'ää': 362, 'ər4': 363, 'iːː': 364, 'ɪː': 365, 'iɑ1': 366, 'ər1': 367, 'œː': 368, 'øi': 369, 'ɪuː': 370, 'cʰcʰ': 371, 'əː1': 372, 'iː1': 373, 'ũ': 374, 'kʰː': 375, 'o̞o̞': 376, 'xʲ': 377, 'ou1': 378, 'iɛ4': 379, 'e̞e̞': 380, 'y1': 381, 'dzː': 382, 'dʲʲ': 383, 'dʰː': 384, 'ɯᵝɯᵝ': 385, 'lː': 386, 'uo1': 387, 'i.4': 388, 'i:': 389, 'yɛ5ʲ': 390, 'a4': 391}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_ID)\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "additional_vocab = timit_vocab.difference(set(vocab.keys()) | {' '})\n",
    "tokenizer.add_tokens(list(additional_vocab))\n",
    "print(additional_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(PRE_TRAINED_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to update the feature extractor since it has been pretrained on 16kHz audio which matches the TIMIT dataset.\n",
    "\n",
    "For datasets with different sampling rates, the feature extractor should be updated or the audio resampled (easier).\n",
    "\n",
    "This is also where code to add extra features (such as conditioning on speaker's native language etc.) would be added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEW FINE-TUNE CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maruna-sri\u001b[0m (\u001b[33maruna-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/arunasrivastava/ML/notebooks/wandb/run-20241129_044725-g5eqlkdd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aruna-team/xlsr-phoneme-prediction/runs/g5eqlkdd' target=\"_blank\">usual-violet-67</a></strong> to <a href='https://wandb.ai/aruna-team/xlsr-phoneme-prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aruna-team/xlsr-phoneme-prediction' target=\"_blank\">https://wandb.ai/aruna-team/xlsr-phoneme-prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aruna-team/xlsr-phoneme-prediction/runs/g5eqlkdd' target=\"_blank\">https://wandb.ai/aruna-team/xlsr-phoneme-prediction/runs/g5eqlkdd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading pretrained model and processor...\n",
      "INFO:__main__:Initial vocabulary size: 41\n",
      "INFO:__main__:Reduced vocabulary size: 41\n",
      "INFO:__main__:Old weights shape: torch.Size([41, 1024])\n",
      "INFO:__main__:New layer weight shape: torch.Size([41, 1024])\n",
      "INFO:__main__:Special token <pad> verified with id 0\n",
      "INFO:__main__:Special token <unk> verified with id 1\n",
      "INFO:__main__:Special token </s> verified with id 2\n",
      "INFO:__main__:Special token <s> verified with id 3\n",
      "INFO:__main__:Splitting dataset into train and evaluation...\n",
      "Map:   0%|          | 0/3696 [00:00<?, ? examples/s]/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 3696/3696 [01:34<00:00, 38.95 examples/s]\n",
      "Map: 100%|██████████| 924/924 [00:24<00:00, 37.76 examples/s]\n",
      "INFO:__main__:Sample input_values shape: (160000,)\n",
      "INFO:__main__:Sample labels shape: (35,)\n",
      "INFO:__main__:Starting training...\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/trainer.py:3347: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
      "\tsave_steps: 500 (from args) != 100 (from trainer_state.json)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/trainer.py:3026: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3201' max='9240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3201/9240 14:11 < 2:51:44, 0.59 it/s, Epoch 3.46/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>105.738300</td>\n",
       "      <td>121.364586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>57.969700</td>\n",
       "      <td>87.509377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>80.344100</td>\n",
       "      <td>87.647171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>52.737200</td>\n",
       "      <td>83.651855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='116' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 66/116 01:00 < 00:46, 1.07 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForCTC, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union\n",
    "import logging\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "NEW_MODEL_ID = \"/home/arunasrivastava/ML/notebooks/results\"\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Constants\n",
    "SAMPLING_RATE = 16000\n",
    "MAX_AUDIO_LENGTH = 160000  # 10 seconds at 16kHz\n",
    "MAX_LABEL_LENGTH = 100\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    \"\"\"\n",
    "    processor: AutoProcessor\n",
    "    padding: Union[bool, str] = \"longest\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Get max length of all input_values in batch\n",
    "        max_length = max(len(feature[\"input_values\"]) for feature in features)\n",
    "        \n",
    "        # Pad input_values\n",
    "        padded_inputs = []\n",
    "        attention_mask = []\n",
    "        for feature in features:\n",
    "            input_length = len(feature[\"input_values\"])\n",
    "            padding_length = max_length - input_length\n",
    "            \n",
    "            # Convert to tensor and ensure correct shape\n",
    "            if isinstance(feature[\"input_values\"], list):\n",
    "                input_values = torch.tensor(feature[\"input_values\"])\n",
    "            else:\n",
    "                input_values = feature[\"input_values\"]\n",
    "            \n",
    "            # Remove any extra dimensions and ensure it's 1D\n",
    "            input_values = input_values.squeeze()\n",
    "            \n",
    "            # Pad with zeros\n",
    "            if padding_length > 0:\n",
    "                padded_input = torch.nn.functional.pad(input_values, (0, padding_length))\n",
    "                attention_mask.append(torch.cat([torch.ones(input_length), torch.zeros(padding_length)]))\n",
    "            else:\n",
    "                padded_input = input_values\n",
    "                attention_mask.append(torch.ones(input_length))\n",
    "            \n",
    "            padded_inputs.append(padded_input)\n",
    "\n",
    "        # Stack all padded inputs ensuring correct dimensions [batch_size, sequence_length]\n",
    "        batch = {\n",
    "            \"input_values\": torch.stack(padded_inputs),\n",
    "            \"attention_mask\": torch.stack(attention_mask)\n",
    "        }\n",
    "\n",
    "        # Process labels\n",
    "        with self.processor.as_target_processor():\n",
    "            label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # Replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "def prepare_model_and_processor(timit_vocab):\n",
    "    \"\"\"\n",
    "    Prepare the model and processor with reduced vocabulary.\n",
    "    \"\"\"\n",
    "    logger.info(\"Loading pretrained model and processor...\")\n",
    "    \n",
    "    # Load processor and model\n",
    "    processor = AutoProcessor.from_pretrained(NEW_MODEL_ID)\n",
    "    model = AutoModelForCTC.from_pretrained(NEW_MODEL_ID)\n",
    "    \n",
    "    # Store the old vocabulary before we modify anything\n",
    "    old_vocab = processor.tokenizer.get_vocab()\n",
    "    \n",
    "    # Verify initial setup\n",
    "    initial_vocab_size = len(old_vocab)\n",
    "    logger.info(f\"Initial vocabulary size: {initial_vocab_size}\")\n",
    "    \n",
    "    # Create reduced vocabulary with special tokens preserved\n",
    "    special_tokens = processor.tokenizer.special_tokens_map\n",
    "    special_token_values = set(special_tokens.values())\n",
    "    \n",
    "    # Ensure we keep special tokens\n",
    "    timit_vocab_with_special = timit_vocab | special_token_values\n",
    "    \n",
    "    # Create new vocab with special tokens first, then regular tokens\n",
    "    special_token_dict = {token: idx for idx, token in enumerate(special_token_values)}\n",
    "    next_idx = len(special_token_dict)\n",
    "    \n",
    "    regular_token_dict = {token: idx + next_idx \n",
    "                         for idx, token in enumerate(timit_vocab - special_token_values)}\n",
    "    \n",
    "    reduced_vocab = {**special_token_dict, **regular_token_dict}\n",
    "    \n",
    "    # Create new tokenizer config\n",
    "    tokenizer_config = {\n",
    "        \"vocab\": reduced_vocab,\n",
    "        \"pad_token\": processor.tokenizer.pad_token,\n",
    "        \"word_delimiter_token\": processor.tokenizer.word_delimiter_token,\n",
    "        \"bos_token\": processor.tokenizer.bos_token,\n",
    "        \"eos_token\": processor.tokenizer.eos_token,\n",
    "        \"unk_token\": processor.tokenizer.unk_token,\n",
    "        \"word_delimiter_token\": processor.tokenizer.word_delimiter_token,\n",
    "    }\n",
    "    \n",
    "    # Create new tokenizer with reduced vocabulary\n",
    "    from transformers import Wav2Vec2CTCTokenizer\n",
    "    import json\n",
    "    import tempfile\n",
    "    \n",
    "    # Save vocab to temporary file\n",
    "    with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:\n",
    "        json.dump(reduced_vocab, f)\n",
    "        vocab_path = f.name\n",
    "    \n",
    "    # Create new tokenizer\n",
    "    new_tokenizer = Wav2Vec2CTCTokenizer(\n",
    "        vocab_path,\n",
    "        **{k: v for k, v in tokenizer_config.items() if k != \"vocab\"}\n",
    "    )\n",
    "    \n",
    "    # Update processor with new tokenizer\n",
    "    processor.tokenizer = new_tokenizer\n",
    "    \n",
    "    # Clean up temporary file\n",
    "    os.remove(vocab_path)\n",
    "    \n",
    "    # Verify vocabulary size\n",
    "    assert len(processor.tokenizer.get_vocab()) == len(reduced_vocab), \"Vocabulary size mismatch\"\n",
    "    logger.info(f\"Reduced vocabulary size: {len(reduced_vocab)}\")\n",
    "    \n",
    "    # Get the weights from the original model\n",
    "    old_weights = model.lm_head.weight.data\n",
    "    old_bias = model.lm_head.bias.data\n",
    "    \n",
    "    # Create new layer with correct dimensions\n",
    "    new_layer = torch.nn.Linear(model.lm_head.in_features, len(processor.tokenizer))\n",
    "    \n",
    "    # Verify dimensions before transfer\n",
    "    logger.info(f\"Old weights shape: {old_weights.shape}\")\n",
    "    logger.info(f\"New layer weight shape: {new_layer.weight.shape}\")\n",
    "    \n",
    "    # Initialize new weights to zero\n",
    "    new_layer.weight.data.zero_()\n",
    "    new_layer.bias.data.zero_()\n",
    "    \n",
    "    # Get new vocabulary\n",
    "    new_vocab = processor.tokenizer.get_vocab()\n",
    "    \n",
    "    # Transfer weights and bias for tokens that exist in both vocabularies\n",
    "    for token, new_idx in new_vocab.items():\n",
    "        if token in old_vocab:\n",
    "            old_idx = old_vocab[token]\n",
    "            new_layer.weight.data[new_idx, :] = old_weights[old_idx, :]\n",
    "            new_layer.bias.data[new_idx] = old_bias[old_idx]\n",
    "    \n",
    "    model.lm_head = new_layer\n",
    "    model.config.vocab_size = len(processor.tokenizer)\n",
    "    \n",
    "    # Verify the setup with special handling for special tokens\n",
    "    special_tokens_set = set(special_tokens.values())\n",
    "    sample_regular_tokens = list(reduced_vocab.keys() - special_tokens_set)[:5]\n",
    "    \n",
    "    # First verify regular tokens\n",
    "    for token in sample_regular_tokens:\n",
    "        encoded = processor.tokenizer(token)\n",
    "        decoded = processor.tokenizer.decode(encoded.input_ids)\n",
    "        assert token in decoded, f\"Token verification failed for {token}\"\n",
    "    \n",
    "    # Then verify special tokens separately\n",
    "    for token in special_tokens_set:\n",
    "        # Get the ID for this special token\n",
    "        token_id = processor.tokenizer.convert_tokens_to_ids(token)\n",
    "        # Verify the ID exists and is within range\n",
    "        assert 0 <= token_id < len(processor.tokenizer), f\"Invalid ID for special token {token}\"\n",
    "        # Verify we can encode and decode without error\n",
    "        encoded = processor.tokenizer(token, return_tensors='pt')\n",
    "        _ = processor.tokenizer.decode(encoded.input_ids[0].tolist())\n",
    "        logger.info(f\"Special token {token} verified with id {token_id}\")\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "\n",
    "def preprocess_audio(audio_input):\n",
    "    \"\"\"\n",
    "    Load and preprocess audio to match expected format.\n",
    "    \n",
    "    Args:\n",
    "        audio_input: Can be a file path (str), numpy array, list, or tensor\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Processed audio as float32 numpy array\n",
    "    \"\"\"\n",
    "    # Handle different input types\n",
    "    if isinstance(audio_input, str):  # File path\n",
    "        waveform, sample_rate = torchaudio.load(audio_input)\n",
    "        audio_numpy = waveform.squeeze().numpy()\n",
    "        \n",
    "    elif isinstance(audio_input, list):  # List input\n",
    "        audio_numpy = np.array(audio_input, dtype=np.float32)\n",
    "        \n",
    "    elif isinstance(audio_input, np.ndarray):  # Already numpy array\n",
    "        audio_numpy = audio_input\n",
    "        \n",
    "    elif isinstance(audio_input, torch.Tensor):  # Torch tensor\n",
    "        audio_numpy = audio_input.numpy()\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported audio type: {type(audio_input)}\")\n",
    "    \n",
    "    # Ensure float32 dtype\n",
    "    audio_numpy = audio_numpy.astype(np.float32)\n",
    "    \n",
    "    # Ensure 1D array\n",
    "    if len(audio_numpy.shape) > 1:\n",
    "        audio_numpy = np.mean(audio_numpy, axis=0)\n",
    "    \n",
    "    # Normalize audio\n",
    "    if np.abs(audio_numpy).max() > 1:\n",
    "        audio_numpy = audio_numpy / np.abs(audio_numpy).max()\n",
    "    \n",
    "    # Trim or pad to max length\n",
    "    if len(audio_numpy) > MAX_AUDIO_LENGTH:\n",
    "        audio_numpy = audio_numpy[:MAX_AUDIO_LENGTH]\n",
    "    elif len(audio_numpy) < MAX_AUDIO_LENGTH:\n",
    "        padding = np.zeros(MAX_AUDIO_LENGTH - len(audio_numpy), dtype=np.float32)\n",
    "        audio_numpy = np.concatenate([audio_numpy, padding])\n",
    "    \n",
    "    return audio_numpy\n",
    "\n",
    "def prepare_datasets(train_df, processor):\n",
    "    \"\"\"\n",
    "    Split and prepare datasets for training and evaluation.\n",
    "    \"\"\"\n",
    "    logger.info(\"Splitting dataset into train and evaluation...\")\n",
    "    \n",
    "    train_data, eval_data = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    def process_data(batch):\n",
    "        # Preprocess audio\n",
    "        audio = preprocess_audio(batch[\"audio\"])\n",
    "        \n",
    "        # Ensure audio is 1D\n",
    "        audio = audio.squeeze()\n",
    "        \n",
    "        # Process audio with the processor\n",
    "        inputs = processor(\n",
    "            audio, \n",
    "            sampling_rate=SAMPLING_RATE, \n",
    "            return_tensors=None,\n",
    "            padding=False\n",
    "        )\n",
    "        \n",
    "        # Ensure input_values is 1D\n",
    "        input_values = np.squeeze(inputs[\"input_values\"])\n",
    "        \n",
    "        # Tokenize IPA\n",
    "        with processor.as_target_processor():\n",
    "            labels = processor(batch[\"ipa\"]).input_ids\n",
    "        \n",
    "        return {\n",
    "            \"input_values\": input_values,\n",
    "            \"labels\": labels,\n",
    "            \"audio\": audio  # Keep the audio in the processed data\n",
    "        }\n",
    "    \n",
    "    # Convert DataFrames to Datasets\n",
    "    train_dataset = Dataset.from_pandas(train_data)\n",
    "    eval_dataset = Dataset.from_pandas(eval_data)\n",
    "    \n",
    "    # Apply processing\n",
    "    train_dataset = train_dataset.map(\n",
    "        process_data,\n",
    "        remove_columns=[col for col in train_dataset.column_names if col != \"audio\"]  # Keep audio column\n",
    "    )\n",
    "    eval_dataset = eval_dataset.map(\n",
    "        process_data,\n",
    "        remove_columns=[col for col in eval_dataset.column_names if col != \"audio\"]  # Keep audio column\n",
    "    )\n",
    "    \n",
    "    # Debug: Check shapes\n",
    "    sample_item = train_dataset[0]\n",
    "    logger.info(f\"Sample input_values shape: {np.array(sample_item['input_values']).shape}\")\n",
    "    logger.info(f\"Sample labels shape: {np.array(sample_item['labels']).shape}\")\n",
    "    \n",
    "    return train_dataset, eval_dataset\n",
    "\n",
    "def verify_dimensions(batch, logger):\n",
    "    \"\"\"\n",
    "    Debug helper to verify tensor dimensions\n",
    "    \"\"\"\n",
    "    logger.info(\"Batch dimensions:\")\n",
    "    for key, value in batch.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            logger.info(f\"{key}: {value.shape}\")\n",
    "\n",
    "def train_model(model, train_dataset, eval_dataset, processor, output_dir=\"./results\"):\n",
    "    \"\"\"\n",
    "    Train the model using CTC loss.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting training...\")\n",
    "    \n",
    "    # training_args = TrainingArguments(\n",
    "    #     output_dir=output_dir,\n",
    "    #     per_device_train_batch_size=1,\n",
    "    #     gradient_accumulation_steps=4,\n",
    "    #     learning_rate=1e-4,\n",
    "    #     num_train_epochs=3,\n",
    "    #     logging_dir='./logs',\n",
    "    #     logging_steps=10,\n",
    "    #     # Increase save_steps to reduce disk usage\n",
    "    #     save_steps=500,  # Changed from 100\n",
    "    #     eval_steps=100,\n",
    "    #     eval_strategy=\"steps\",\n",
    "    #     load_best_model_at_end=True,\n",
    "    #     # Add save_safetensors=True for more reliable saving\n",
    "    #     save_safetensors=True,\n",
    "    # )\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        # Decrease learning rate since loss is high\n",
    "        learning_rate=1e-5,  # Changed from 1e-4\n",
    "        \n",
    "        # Increase batch size for better stability\n",
    "        per_device_train_batch_size=1,  # Changed from 1\n",
    "        gradient_accumulation_steps=4,   # Changed from 4 (maintains effective batch size of 16)\n",
    "        \n",
    "        # Add warmup steps\n",
    "        warmup_steps=500,\n",
    "        \n",
    "        # Add weight decay for regularization\n",
    "        weight_decay=0.01,\n",
    "        \n",
    "        # Increase training time\n",
    "        num_train_epochs=10,  # Changed from 3\n",
    "        \n",
    "        # Keep other parameters\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        save_steps=500,\n",
    "        eval_steps=100,\n",
    "        eval_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_safetensors=True,\n",
    "        \n",
    "        # Add early stopping\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "    )\n",
    "    data_collator = DataCollatorCTCWithPadding(processor=processor)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,  # Include the eval dataset here\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(resume_from_checkpoint=\"./results/checkpoint-2700\")\n",
    "\n",
    "    # Save the final model and processor\n",
    "    model.save_pretrained(output_dir)\n",
    "    processor.save_pretrained(output_dir)\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "def main(train_df, timit_vocab):\n",
    "    \"\"\"\n",
    "    Main training pipeline.\n",
    "    \"\"\"\n",
    "    # Initialize wandb for tracking\n",
    "    wandb.init(project=\"xlsr-phoneme-prediction\")\n",
    "    \n",
    "    # Prepare model and processor\n",
    "    model, processor = prepare_model_and_processor(timit_vocab)\n",
    "    \n",
    "    # Get the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)  # Ensure model is on GPU\n",
    "    \n",
    "    # Prepare train and eval datasets\n",
    "    train_dataset, eval_dataset = prepare_datasets(train_df, processor)\n",
    "    \n",
    "    # Train model\n",
    "    model, processor = train_model(model, train_dataset, eval_dataset, processor)\n",
    "    \n",
    "    ## Final verification\n",
    "    logger.info(\"Running inference on sample...\")\n",
    "    sample_audio = train_dataset[0][\"audio\"]\n",
    "    inputs = processor(sample_audio, return_tensors=\"pt\", padding=True)\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Modified decoding to remove special tokens properly\n",
    "    predicted_tokens = processor.batch_decode(predicted_ids)\n",
    "    # Clean up the prediction by removing special tokens and extra spaces\n",
    "    cleaned_prediction = (predicted_tokens[0]\n",
    "                        .replace('<s>', '')\n",
    "                        .replace('</s>', '')\n",
    "                        .replace('<pad>', '')\n",
    "                        .replace('<unk>', '')\n",
    "                        .strip())\n",
    "    \n",
    "    logger.info(f\"Sample raw prediction: {predicted_tokens}\")\n",
    "    logger.info(f\"Sample cleaned prediction: {cleaned_prediction}\")\n",
    "    \n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(train_df, timit_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processor vocab: {'</s>': 1, '<pad>': 3, '<s>': 0, '<unk>': 2, 'a': 11, 'b': 14, 'd': 17, 'e': 22, 'f': 23, 'h': 31, 'i': 24, 'j': 35, 'k': 26, 'l': 38, 'm': 28, 'n': 20, 'o': 5, 'p': 13, 's': 9, 't': 32, 'u': 21, 'v': 37, 'w': 19, 'z': 7, 'æ': 16, 'ð': 29, 'ŋ': 33, 'ɑ': 40, 'ɔ': 15, 'ə': 12, 'ɛ': 30, 'ɡ': 39, 'ɪ': 10, 'ɹ': 8, 'ɾ': 34, 'ʃ': 18, 'ʊ': 36, 'ʌ': 6, 'ʒ': 25, 'ʔ': 4, 'θ': 27}\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"/home/arunasrivastava/ML/notebooks/results\")\n",
    "print(f\"processor vocab: {processor.tokenizer.get_vocab()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab = processor.tokenizer.get_vocab()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    batch[\"input_values\"] = processor(batch[\"audio\"], sampling_rate=feature_extractor.sampling_rate).input_values\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"ipa\"]).input_ids\n",
    "    print(\"Tokenized IPA:\", batch['labels'])\n",
    "    # convert the batch['labels'] back to IPA\n",
    "    decoded_ipa = [processor.tokenizer.decode(label, skip_special_tokens=True) for label in batch['labels']]\n",
    "    print(\"Decoded IPA (skipping special tokens):\", decoded_ipa)\n",
    "    print(\"Length of tokenized IPA:\", len(batch['labels']))\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test prepare_dataset on just the furst row of the train_ds\n",
    "\n",
    "ds_test = prepare_dataset(train_ds[0])\n",
    "print(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_prepared = train_ds.map(prepare_dataset, batch_size=8, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds_prepared = test_ds.map(prepare_dataset, batch_size=8, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_prepared.to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = processor.tokenizer.get_vocab()\n",
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first row of train ipa\n",
    "print(len(train_ds_prepared[0]['ipa']))\n",
    "print(train_ds_prepared[0]['ipa'])\n",
    "# print ipa mapped to the vocab numbers\n",
    "print(train_ds_prepared[0]['labels'])\n",
    "\n",
    "print(len(train_ds_prepared[0]['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, processor: AutoProcessor, padding=True, max_length=None, max_length_labels=None, pad_to_multiple_of=None, pad_to_multiple_of_labels=None):\n",
    "        self.processor = processor\n",
    "        self.padding = padding\n",
    "        self.max_length = max_length\n",
    "        self.max_length_labels = max_length_labels\n",
    "        self.pad_to_multiple_of = pad_to_multiple_of\n",
    "        self.pad_to_multiple_of_labels = pad_to_multiple_of_labels\n",
    "\n",
    "    def __call__(self, features: \"list[dict[str, list[int] | torch.Tensor]]\") -> \"dict[str, torch.Tensor]\":\n",
    "        vocab = processor.tokenizer.get_vocab()  # For most processors, this works\n",
    "        print(\"MF VOCAB\", vocab)\n",
    "\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        print(\"input-features pt after processor.pad\", batch[\"input_values\"].shape)\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "        print(\"labels-features pt after processor.pad\", labels_batch[\"input_ids\"].shape)\n",
    "        # Loop through each sequence in the batch and decode\n",
    "        for i, input_ids in enumerate(labels_batch[\"input_ids\"]):\n",
    "            decoded_text = processor.decode(input_ids.tolist(), skip_special_tokens=False)\n",
    "            print(f\"Decoded text for sample {i}: {decoded_text}\")\n",
    "     \n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        print(\"labels: \", labels)\n",
    "        print(\"labels-features pt after masked_fill\", labels.shape)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming train_ds is your dataset\n",
    "sample = train_ds_prepared[0]  # Grab the first row from the dataset\n",
    "# print the first row's phonemes\n",
    "print(\"Phonemes:\", sample[\"ipa\"])\n",
    "# Create an instance of the DataCollatorCTCWithPadding class\n",
    "processor = AutoProcessor.from_pretrained(PRE_TRAINED_ID)  # Replace with your processor\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor)\n",
    "\n",
    "# Manually test the data collator with the sample\n",
    "features = [sample]\n",
    "batch = data_collator(features)\n",
    "\n",
    "# Print the batch to inspect the results\n",
    "print(\"Batch:\", batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cer(pred, label):\n",
    "    distances = np.zeros((len(pred) + 1, len(label) + 1))\n",
    "\n",
    "    for t1 in range(len(pred) + 1):\n",
    "        distances[t1][0] = t1\n",
    "\n",
    "    for t2 in range(len(label) + 1):\n",
    "        distances[0][t2] = t2\n",
    "        \n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "    \n",
    "    for t1 in range(1, len(pred) + 1):\n",
    "        for t2 in range(1, len(label) + 1):\n",
    "            if (pred[t1-1] == label[t2-1]):\n",
    "                distances[t1][t2] = distances[t1 - 1][t2 - 1]\n",
    "            else:\n",
    "                a = distances[t1][t2 - 1]\n",
    "                b = distances[t1 - 1][t2]\n",
    "                c = distances[t1 - 1][t2 - 1]\n",
    "                \n",
    "                if (a <= b and a <= c):\n",
    "                    distances[t1][t2] = a + 1\n",
    "                elif (b <= a and b <= c):\n",
    "                    distances[t1][t2] = b + 1\n",
    "                else:\n",
    "                    distances[t1][t2] = c + 1\n",
    "\n",
    "    return distances[len(pred)][len(label)] / len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False) # labels are already grouped as they should be\n",
    "\n",
    "    # Call panphon_model_eval with label and predictedipa\n",
    "    results = panphon_model_eval(label_str, pred_str)\n",
    "\n",
    "    # Output results\n",
    "    print(\"Evaluation Results:\")\n",
    "    print(f\"Feature edit distance: {results['feature_dist']}\")\n",
    "    print(f\"Weighted feature edit distance: {results['weighted_feature_dist']}\")\n",
    "    print(f\"Hamming distance: {results['hamming_feature_dist']}\")\n",
    "    print(f\"CER: {results['cer_score']}\")\n",
    "\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCTC.from_pretrained(\n",
    "    PRE_TRAINED_ID, \n",
    "    pad_token_id=processor.tokenizer.pad_token_id, \n",
    "    # gradient_checkpointing=True,\n",
    "    ctc_zero_infinity = True,\n",
    "    attention_dropout=0.1,\n",
    "    layerdrop=0.0,\n",
    "    feat_proj_dropout=0.0,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "old_weights = model.lm_head.weight.t().detach()\n",
    "old_bias = model.lm_head.bias.detach()\n",
    "new_layer = torch.nn.Linear(model.lm_head.in_features, len(processor.tokenizer))\n",
    "new_layer.weight.data[:model.lm_head.out_features, :] = old_weights.t()\n",
    "new_layer.bias.data[:model.lm_head.out_features] = old_bias\n",
    "model.lm_head = new_layer\n",
    "model.config.vocab_size = len(processor.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "max_class_id = 0\n",
    "for label_sequence in train_ds_prepared['labels']:\n",
    "    for class_id in label_sequence:\n",
    "        max_class_id = max(max_class_id, class_id)\n",
    "assert max_class_id == model.config.vocab_size - 1\n",
    "\n",
    "for label_sequence in train_ds_prepared['labels']:\n",
    "    for class_id in label_sequence:\n",
    "        assert class_id >= 0 and class_id <  model.config.vocab_size\n",
    "\n",
    "for label_sequence in train_ds_prepared['labels']:\n",
    "    assert  len(label_sequence) < 1024\n",
    "    assert len(label_sequence) >= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"timit-finetune-a100\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    eval_strategy=\"steps\",\n",
    "    num_train_epochs=5,\n",
    "    fp16=True,\n",
    "    save_steps=50,\n",
    "    eval_steps=50,\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=1000,\n",
    "    save_total_limit=3,\n",
    "    gradient_checkpointing=True,\n",
    "    load_best_model_at_end=True,\n",
    "    max_grad_norm=0.5,\n",
    "    report_to=\"wandb\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_ds_prepared.select(range(10)),\n",
    "    # train_dataset=train_ds_prepared,\n",
    "    eval_dataset=test_ds_prepared.select(range(10)),\n",
    "    # eval_dataset=test_ds_prepared,\n",
    "    processing_class=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(False) # debug NaNs, disable for real training to improve performance\n",
    "# model.config.ctc_zero_infinity = True \n",
    "#resume_from_checkpoint=True\n",
    "trainer.train() # resume_from_checkpoint=True to resume training if training was interrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "eval_results = trainer.evaluate()\n",
    "wandb.log(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_model = AutoModelForCTC.from_pretrained(PRE_TRAINED_ID).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = AutoModelForCTC.from_pretrained(os.path.join(OUTPUT_DIR, 'checkpoint-100'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for nan weights and replace with old values if any\n",
    "for name, param in finetuned_model.named_parameters():\n",
    "    if torch.isnan(param).any():\n",
    "        print(name)\n",
    "        param.data = old_model.state_dict()[name].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare weights\n",
    "is_diff = False\n",
    "for name, param in finetuned_model.named_parameters():\n",
    "    if not torch.equal(param, old_model.state_dict()[name].to(finetuned_model.device)):\n",
    "        is_diff = True\n",
    "        break\n",
    "print(\"Weights are different, training did something\" if is_diff else \"Weights are the same, training did nothing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, wav_file):\n",
    "    speech = audio_file_to_array(wav_file)\n",
    "    input_values = processor(speech, sampling_rate=16000, return_tensors=\"pt\").input_values.type(torch.float32).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    return processor.decode(predicted_ids[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Checkpoint Transcription:', predict(finetuned_model, os.path.join('..', 'data', 'alexIsConfused.wav')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Fine-Tune Transcription:', predict(trainer.model, os.path.join('..', 'data', 'alexIsConfused.wav')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Fine-Tune Transcription:', predict(trainer.model, os.path.join('..', 'data', 'alexIsConfused.wav')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Old Transcription:', predict(old_model, os.path.join('..', 'data', 'alexIsConfused.wav')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "\n",
    "# sweep_config = {\n",
    "#     'method': 'bayes',  # Or 'grid' or 'random'\n",
    "#     'metric': {'name': 'loss', 'goal': 'minimize'},\n",
    "#     'parameters': {\n",
    "#         'learning_rate': {'min': 1e-8, 'max': 5e-6, 'distribution': 'log_uniform_values'},\n",
    "#         'per_device_train_batch_size': {'values': [4, 8, 16]},\n",
    "#         'num_train_epochs': {'values': [3, 5, 10]},\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep_config, project=\"timit-finetune-hyperparam\")\n",
    "\n",
    "# from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# def train():\n",
    "#     # Initialize a new wandb run\n",
    "#     wandb.init()\n",
    "\n",
    "#     # Set up training arguments, pulling values from wandb.config\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=OUTPUT_DIR,\n",
    "#         group_by_length=True,\n",
    "#         per_device_train_batch_size=wandb.config.per_device_train_batch_size,\n",
    "#         gradient_accumulation_steps=4,\n",
    "#         eval_strategy=\"steps\",\n",
    "#         num_train_epochs=wandb.config.num_train_epochs,\n",
    "#         fp16=True,\n",
    "#         save_steps=50,\n",
    "#         eval_steps=50,\n",
    "#         logging_steps=10,\n",
    "#         learning_rate=wandb.config.learning_rate,\n",
    "#         warmup_steps=1000,\n",
    "#         save_total_limit=3,\n",
    "#         gradient_checkpointing=True,\n",
    "#         load_best_model_at_end=True,\n",
    "#         max_grad_norm=1.0,\n",
    "#         report_to=\"wandb\"\n",
    "#     )\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         data_collator=data_collator,\n",
    "#         args=training_args,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#         train_dataset=train_ds_prepared,\n",
    "#         eval_dataset=test_ds_prepared,\n",
    "#         processing_class=processor.feature_extractor,\n",
    "#     )\n",
    "\n",
    "#     # Run training and automatically log results to W&B\n",
    "#     trainer.train()\n",
    "\n",
    "#     # Ensure to log any additional evaluation metrics\n",
    "#     eval_metrics = trainer.evaluate()\n",
    "#     wandb.log(eval_metrics)\n",
    "\n",
    "\n",
    "\n",
    "# wandb.agent(sweep_id, train, count=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# import optuna\n",
    "# from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# # Initialize wandb\n",
    "# wandb.init(project=\"hyperparam\")\n",
    "\n",
    "# def model_init():\n",
    "#     return model\n",
    "\n",
    "# def objective(trial):\n",
    "#     # Define the hyperparameters to tune\n",
    "#     learning_rate = trial.suggest_float(\"learning_rate\", 1e-8, 1e-7, log=True)\n",
    "#     per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [2, 4, 8])\n",
    "#     num_train_epochs = trial.suggest_int(\"num_train_epochs\", 5, 10, 15)\n",
    "\n",
    "#     # Update training arguments with dynamic hyperparameters\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=OUTPUT_DIR,\n",
    "#         run_name=\"hyperparam-searchC\",  # Set a unique run name\n",
    "#         group_by_length=True,\n",
    "#         per_device_train_batch_size=per_device_train_batch_size,\n",
    "#         gradient_accumulation_steps=4,\n",
    "#         evaluation_strategy=\"steps\",\n",
    "#         num_train_epochs=num_train_epochs,\n",
    "#         fp16=True,\n",
    "#         save_steps=100,\n",
    "#         eval_steps=100,\n",
    "#         logging_steps=100,\n",
    "#         learning_rate=learning_rate,\n",
    "#         warmup_steps=1000,\n",
    "#         save_total_limit=3,\n",
    "#         gradient_checkpointing=True,\n",
    "#         load_best_model_at_end=True,\n",
    "#         max_grad_norm=1.0,\n",
    "#         report_to=\"wandb\"  # Report to WandB\n",
    "#     )\n",
    "\n",
    "#     # Set up the trainer with the training arguments and datasets\n",
    "#     trainer = Trainer(\n",
    "#         model_init=model_init,\n",
    "#         args=training_args,\n",
    "#         data_collator=data_collator,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#         train_dataset=train_ds_prepared.select(range(100)),  # Use a subset for faster experimentation\n",
    "#         eval_dataset=test_ds_prepared.select(range(100)),\n",
    "#         processing_class=processor.feature_extractor,\n",
    "#     )\n",
    "\n",
    "#     # Disable anomaly detection for better performance\n",
    "#     torch.autograd.set_detect_anomaly(False)\n",
    "#     # model.config.ctc_zero_infinity = True  # Uncomment if needed for CTC models\n",
    "\n",
    "#     # Train the model for this trial\n",
    "#     result = trainer.train()  # Set resume_from_checkpoint=True if resuming is required\n",
    "\n",
    "#     # Return the final training loss for Optuna to minimize\n",
    "#     return result.training_loss\n",
    "\n",
    "# # Perform hyperparameter search with updated search space\n",
    "# study = optuna.create_study(direction=\"minimize\")\n",
    "# study.optimize(objective, n_trials=10)\n",
    "\n",
    "# # Print the best hyperparameters\n",
    "# print(\"Best hyperparameters: \", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
