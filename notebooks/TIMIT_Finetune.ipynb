{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import zipfile\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "# run as much as possible accelerated by apple silicon, fall back to cpu if not possible\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from scripts.audio import audio_file_to_array\n",
    "from scripts.ipa import timit2ipa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('..')\n",
    "from scripts.eval_tests.panphon_model_eval import panphon_model_eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set espeak library path for macOS\n",
    "if sys.platform == \"darwin\":\n",
    "    from phonemizer.backend.espeak.wrapper import EspeakWrapper\n",
    "\n",
    "    _ESPEAK_LIBRARY = \"/opt/homebrew/Cellar/espeak/1.48.04_1/lib/libespeak.1.1.48.dylib\"\n",
    "    EspeakWrapper.set_library(_ESPEAK_LIBRARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCTC, AutoTokenizer, AutoFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = os.path.join('..', 'models', 'timit-xlsr-finetune-B')\n",
    "PRE_TRAINED_ID = \"facebook/wav2vec2-lv-60-espeak-cv-ft\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit = zipfile.ZipFile('../.data/TIMIT.zip', 'r')\n",
    "timit_files = timit.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = list(set(map(lambda x: x.split('.')[0], filter(lambda x: x.startswith('data/TRAIN'), timit_files))))\n",
    "test_files = list(set(map(lambda x: x.split('.')[0], filter(lambda x: x.startswith('data/TEST'), timit_files))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def timit_file_to_dict(filename):\n",
    "    with timit.open(filename + '.PHN') as phn_file:\n",
    "        timestamped_phonemes = []\n",
    "        for line in phn_file.read().decode('utf-8').split('\\n'):\n",
    "            if line == '':\n",
    "                continue\n",
    "            start, end, phoneme = line.split()\n",
    "            timestamped_phonemes.append((timit2ipa(phoneme, \"eng\"), int(start), int(end)))\n",
    "    return {'timestamped_phonemes': timestamped_phonemes, 'wav_filename': filename + '.WAV'}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_HEADER_SIZE = 44\n",
    "def zipped_wav_to_array(filename):\n",
    "    with timit.open(filename) as wav_file:\n",
    "        return np.frombuffer(wav_file.read(), dtype=np.int16)[WAV_HEADER_SIZE//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timestamped_phonemes': [('', 0, 2212), ('aʊ', 2212, 4344), ('ɹ', 4344, 5597), ('ɚ', 5597, 6560), ('s', 6560, 8610), ('i', 8610, 10790), ('v', 10790, 11009), ('d', 11009, 11600), ('', 11600, 11840), ('ɨ', 11840, 12800), ('d', 12800, 13590), ('dʒ', 13590, 14260), ('ɔɪ', 14260, 16520), ('ɾ̃', 16520, 16920), ('ə', 16920, 17720), ('p', 17720, 18780), ('', 18780, 20020), ('ɔɪ', 20020, 22064), ('m', 22064, 22987), ('ɨ', 22987, 24244), ('n', 24244, 24539), ('t', 24539, 25520), ('ɨ', 25520, 26280), ('n', 26280, 27280), ('ə', 27280, 27920), ('b', 27920, 28890), ('', 28890, 29060), ('aɪ', 29060, 31749), ('ɑ', 31749, 32835), ('l', 32835, 33378), ('ə', 33378, 34360), ('d', 34360, 34930), ('dʒ', 34930, 36010), ('i', 36010, 37389), ('ɪ', 37389, 38480), ('n', 38480, 39480), ('i', 39480, 40850), ('ɛ', 40850, 42119), ('n', 42119, 42504), ('d', 42504, 42825), ('dʒ', 42825, 43702), ('ɨ', 43702, 44162), ('n', 44162, 44880), ('ɪ', 44880, 45791), ('ɚ', 45791, 47337), ('ŋ', 47337, 48084), ('d', 48084, 48603), ('', 48603, 48840), ('ə̥', 48840, 49103), ('p', 49103, 49680), ('', 49680, 50210), ('ɑ', 50210, 51399), ('ɹ', 51399, 52120), ('ʔ', 52120, 53360), ('m', 53360, 53705), ('n̩', 53705, 54295), ('t', 54295, 55300), ('', 55300, 55586), ('s', 55586, 56900), ('', 56900, 59120)], 'wav_filename': 'data/TRAIN/DR3/MLNS0/SX327.WAV'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parsed_0 = timit_file_to_dict(training_files[0])\n",
    "print(parsed_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_to_df(files):\n",
    "    records = []\n",
    "    for filename in files:\n",
    "        parsed = timit_file_to_dict(filename)\n",
    "        parsed['audio'] = zipped_wav_to_array(parsed['wav_filename'])\n",
    "        del parsed['wav_filename']\n",
    "        parsed['ipa'] = \"\".join(phoneme for phoneme, _, _ in parsed['timestamped_phonemes'])\n",
    "        parsed['phoneme_starts'] = [start for _, start, _ in parsed['timestamped_phonemes']]\n",
    "        parsed['phoneme_ends'] = [end for _, _, end in parsed['timestamped_phonemes']]\n",
    "        del parsed['timestamped_phonemes']\n",
    "        records.append(parsed)\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>ipa</th>\n",
       "      <th>phoneme_starts</th>\n",
       "      <th>phoneme_ends</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>aʊɹɚsivdɨddʒɔɪɾ̃əpɔɪmɨntɨnəbaɪɑləddʒiɪniɛnddʒɨ...</td>\n",
       "      <td>[0, 2212, 4344, 5597, 6560, 8610, 10790, 11009...</td>\n",
       "      <td>[2212, 4344, 5597, 6560, 8610, 10790, 11009, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>ɑdleɪztɑpɹaɪɔɹɾiɔŋgɛɾɨŋɨzbaɪkfɪkst</td>\n",
       "      <td>[0, 2190, 3250, 5556, 7160, 8350, 9126, 10680,...</td>\n",
       "      <td>[2190, 3250, 5556, 7160, 8350, 9126, 10680, 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>ʃiɦɪddʒʌmpəweɪfɹm̩hɨʃaɪtʌttʃlaɪkɨkætkn̩fɹʌntɨd...</td>\n",
       "      <td>[0, 2260, 4010, 4469, 4804, 5720, 6410, 7260, ...</td>\n",
       "      <td>[2260, 4010, 4469, 4804, 5720, 6410, 7260, 827...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>ðəmɔɪʃɝʔɪnmaɪaɪzʔɨzfɹʌmʔaɪdɹɑfsnɑtfɹəmtiɨz</td>\n",
       "      <td>[0, 2731, 3264, 3610, 4982, 7464, 9818, 11134,...</td>\n",
       "      <td>[2731, 3264, 3610, 4982, 7464, 9818, 11134, 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>mɑaɪdɪlmɔɹɾ̃ɨŋbɨgɪzwəθhɑʔkɔfi</td>\n",
       "      <td>[0, 1921, 2480, 4302, 6320, 7160, 7350, 8814, ...</td>\n",
       "      <td>[1921, 2480, 4302, 6320, 7160, 7350, 8814, 980...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               audio  \\\n",
       "0  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "1  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "2  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "3  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "4  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "\n",
       "                                                 ipa  \\\n",
       "0  aʊɹɚsivdɨddʒɔɪɾ̃əpɔɪmɨntɨnəbaɪɑləddʒiɪniɛnddʒɨ...   \n",
       "1                 ɑdleɪztɑpɹaɪɔɹɾiɔŋgɛɾɨŋɨzbaɪkfɪkst   \n",
       "2  ʃiɦɪddʒʌmpəweɪfɹm̩hɨʃaɪtʌttʃlaɪkɨkætkn̩fɹʌntɨd...   \n",
       "3         ðəmɔɪʃɝʔɪnmaɪaɪzʔɨzfɹʌmʔaɪdɹɑfsnɑtfɹəmtiɨz   \n",
       "4                      mɑaɪdɪlmɔɹɾ̃ɨŋbɨgɪzwəθhɑʔkɔfi   \n",
       "\n",
       "                                      phoneme_starts  \\\n",
       "0  [0, 2212, 4344, 5597, 6560, 8610, 10790, 11009...   \n",
       "1  [0, 2190, 3250, 5556, 7160, 8350, 9126, 10680,...   \n",
       "2  [0, 2260, 4010, 4469, 4804, 5720, 6410, 7260, ...   \n",
       "3  [0, 2731, 3264, 3610, 4982, 7464, 9818, 11134,...   \n",
       "4  [0, 1921, 2480, 4302, 6320, 7160, 7350, 8814, ...   \n",
       "\n",
       "                                        phoneme_ends  \n",
       "0  [2212, 4344, 5597, 6560, 8610, 10790, 11009, 1...  \n",
       "1  [2190, 3250, 5556, 7160, 8350, 9126, 10680, 11...  \n",
       "2  [2260, 4010, 4469, 4804, 5720, 6410, 7260, 827...  \n",
       "3  [2731, 3264, 3610, 4982, 7464, 9818, 11134, 12...  \n",
       "4  [1921, 2480, 4302, 6320, 7160, 7350, 8814, 980...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = files_to_df(training_files)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>ipa</th>\n",
       "      <th>phoneme_starts</th>\n",
       "      <th>phoneme_ends</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>sɝvðɨkoʊlslɔʔæftɚaɪʔædðiɔɪl</td>\n",
       "      <td>[0, 2330, 4280, 6560, 7000, 7770, 8760, 9700, ...</td>\n",
       "      <td>[2330, 4280, 6560, 7000, 7770, 8760, 9700, 104...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>ʃiɦɛdjɚdɑɹksʉtʔɨngɹiziwɑʃwɔɾɚɔljɪɝ</td>\n",
       "      <td>[0, 2370, 4804, 5765, 6600, 8280, 8650, 9060, ...</td>\n",
       "      <td>[2370, 4804, 5765, 6600, 8280, 8650, 9060, 941...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>maɪkl̩ kʌlɝdðɨbɛdɹʉmwɔlwɨθkɹeɪɑnz</td>\n",
       "      <td>[0, 1960, 3000, 5240, 6040, 6684, 7644, 8840, ...</td>\n",
       "      <td>[1960, 3000, 5240, 6040, 6684, 7644, 8840, 102...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>lændpɛɹɨnthʊdʔɔɹgɨnɪzeɪʃɨnzpɚmoʊtɝθkɨnɹoʊl</td>\n",
       "      <td>[0, 2250, 3950, 5206, 7720, 8680, 9080, 9720, ...</td>\n",
       "      <td>[2250, 3950, 5206, 7720, 8680, 9080, 9720, 108...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>hɛlpɹɛgɨpɪkəpɛkəvpɨteɪɾoʊz</td>\n",
       "      <td>[0, 2680, 4000, 5266, 6040, 8170, 8820, 9880, ...</td>\n",
       "      <td>[2680, 4000, 5266, 6040, 8170, 8820, 9880, 120...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               audio  \\\n",
       "0  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "1  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "2  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "3  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "4  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "\n",
       "                                          ipa  \\\n",
       "0                 sɝvðɨkoʊlslɔʔæftɚaɪʔædðiɔɪl   \n",
       "1          ʃiɦɛdjɚdɑɹksʉtʔɨngɹiziwɑʃwɔɾɚɔljɪɝ   \n",
       "2           maɪkl̩ kʌlɝdðɨbɛdɹʉmwɔlwɨθkɹeɪɑnz   \n",
       "3  lændpɛɹɨnthʊdʔɔɹgɨnɪzeɪʃɨnzpɚmoʊtɝθkɨnɹoʊl   \n",
       "4                  hɛlpɹɛgɨpɪkəpɛkəvpɨteɪɾoʊz   \n",
       "\n",
       "                                      phoneme_starts  \\\n",
       "0  [0, 2330, 4280, 6560, 7000, 7770, 8760, 9700, ...   \n",
       "1  [0, 2370, 4804, 5765, 6600, 8280, 8650, 9060, ...   \n",
       "2  [0, 1960, 3000, 5240, 6040, 6684, 7644, 8840, ...   \n",
       "3  [0, 2250, 3950, 5206, 7720, 8680, 9080, 9720, ...   \n",
       "4  [0, 2680, 4000, 5266, 6040, 8170, 8820, 9880, ...   \n",
       "\n",
       "                                        phoneme_ends  \n",
       "0  [2330, 4280, 6560, 7000, 7770, 8760, 9700, 104...  \n",
       "1  [2370, 4804, 5765, 6600, 8280, 8650, 9060, 941...  \n",
       "2  [1960, 3000, 5240, 6040, 6684, 7644, 8840, 102...  \n",
       "3  [2250, 3950, 5206, 7720, 8680, 9080, 9720, 108...  \n",
       "4  [2680, 4000, 5266, 6040, 8170, 8820, 9880, 120...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = files_to_df(test_files)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Up Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_audio(row):\n",
    "    # Access the phoneme_starts column and get the last value of the list\n",
    "    end = row['phoneme_starts'][-1]\n",
    "    # Access the phoneme_ends column and get the first value of the list\n",
    "    start = row['phoneme_ends'][0]\n",
    "    # Crop the audio from start to end\n",
    "    # note that start and end are in samples, not seconds\n",
    "    cropped_audio = row['audio'][start:end]\n",
    "    \n",
    "    return cropped_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# graph one audio to see start signal that we will remove\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_df['audio'][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>ipa</th>\n",
       "      <th>phoneme_starts</th>\n",
       "      <th>phoneme_ends</th>\n",
       "      <th>cropped_audio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>aʊɹɚsivdɨddʒɔɪɾ̃əpɔɪmɨntɨnəbaɪɑləddʒiɪniɛnddʒɨ...</td>\n",
       "      <td>[0, 2212, 4344, 5597, 6560, 8610, 10790, 11009...</td>\n",
       "      <td>[2212, 4344, 5597, 6560, 8610, 10790, 11009, 1...</td>\n",
       "      <td>[4, 0, 0, 3, 1, -2, 0, 2, 1, 4, 2, 4, 1, 3, 4,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>ɑdleɪztɑpɹaɪɔɹɾiɔŋgɛɾɨŋɨzbaɪkfɪkst</td>\n",
       "      <td>[0, 2190, 3250, 5556, 7160, 8350, 9126, 10680,...</td>\n",
       "      <td>[2190, 3250, 5556, 7160, 8350, 9126, 10680, 11...</td>\n",
       "      <td>[3, 4, 6, 1, 5, 0, 3, 2, 5, 3, 1, 4, 3, 4, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>ʃiɦɪddʒʌmpəweɪfɹm̩hɨʃaɪtʌttʃlaɪkɨkætkn̩fɹʌntɨd...</td>\n",
       "      <td>[0, 2260, 4010, 4469, 4804, 5720, 6410, 7260, ...</td>\n",
       "      <td>[2260, 4010, 4469, 4804, 5720, 6410, 7260, 827...</td>\n",
       "      <td>[-2, 2, 2, 0, -1, 0, -2, -1, 0, 3, 0, 0, 2, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>ðəmɔɪʃɝʔɪnmaɪaɪzʔɨzfɹʌmʔaɪdɹɑfsnɑtfɹəmtiɨz</td>\n",
       "      <td>[0, 2731, 3264, 3610, 4982, 7464, 9818, 11134,...</td>\n",
       "      <td>[2731, 3264, 3610, 4982, 7464, 9818, 11134, 12...</td>\n",
       "      <td>[0, 1, -2, 4, 0, -1, -5, -3, -4, -3, 0, -3, -2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>mɑaɪdɪlmɔɹɾ̃ɨŋbɨgɪzwəθhɑʔkɔfi</td>\n",
       "      <td>[0, 1921, 2480, 4302, 6320, 7160, 7350, 8814, ...</td>\n",
       "      <td>[1921, 2480, 4302, 6320, 7160, 7350, 8814, 980...</td>\n",
       "      <td>[2, 1, 3, 1, 2, 2, 2, 2, 0, 1, -2, 1, 3, 4, 2,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               audio  \\\n",
       "0  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "1  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "2  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "3  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "4  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "\n",
       "                                                 ipa  \\\n",
       "0  aʊɹɚsivdɨddʒɔɪɾ̃əpɔɪmɨntɨnəbaɪɑləddʒiɪniɛnddʒɨ...   \n",
       "1                 ɑdleɪztɑpɹaɪɔɹɾiɔŋgɛɾɨŋɨzbaɪkfɪkst   \n",
       "2  ʃiɦɪddʒʌmpəweɪfɹm̩hɨʃaɪtʌttʃlaɪkɨkætkn̩fɹʌntɨd...   \n",
       "3         ðəmɔɪʃɝʔɪnmaɪaɪzʔɨzfɹʌmʔaɪdɹɑfsnɑtfɹəmtiɨz   \n",
       "4                      mɑaɪdɪlmɔɹɾ̃ɨŋbɨgɪzwəθhɑʔkɔfi   \n",
       "\n",
       "                                      phoneme_starts  \\\n",
       "0  [0, 2212, 4344, 5597, 6560, 8610, 10790, 11009...   \n",
       "1  [0, 2190, 3250, 5556, 7160, 8350, 9126, 10680,...   \n",
       "2  [0, 2260, 4010, 4469, 4804, 5720, 6410, 7260, ...   \n",
       "3  [0, 2731, 3264, 3610, 4982, 7464, 9818, 11134,...   \n",
       "4  [0, 1921, 2480, 4302, 6320, 7160, 7350, 8814, ...   \n",
       "\n",
       "                                        phoneme_ends  \\\n",
       "0  [2212, 4344, 5597, 6560, 8610, 10790, 11009, 1...   \n",
       "1  [2190, 3250, 5556, 7160, 8350, 9126, 10680, 11...   \n",
       "2  [2260, 4010, 4469, 4804, 5720, 6410, 7260, 827...   \n",
       "3  [2731, 3264, 3610, 4982, 7464, 9818, 11134, 12...   \n",
       "4  [1921, 2480, 4302, 6320, 7160, 7350, 8814, 980...   \n",
       "\n",
       "                                       cropped_audio  \n",
       "0  [4, 0, 0, 3, 1, -2, 0, 2, 1, 4, 2, 4, 1, 3, 4,...  \n",
       "1  [3, 4, 6, 1, 5, 0, 3, 2, 5, 3, 1, 4, 3, 4, 5, ...  \n",
       "2  [-2, 2, 2, 0, -1, 0, -2, -1, 0, 3, 0, 0, 2, 1,...  \n",
       "3  [0, 1, -2, 4, 0, -1, -5, -3, -4, -3, 0, -3, -2...  \n",
       "4  [2, 1, 3, 1, 2, 2, 2, 2, 0, 1, -2, 1, 3, 4, 2,...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the process_row function to each row in the DataFrame\n",
    "train_df['cropped_audio'] = train_df.apply(crop_audio, axis=1) \n",
    "train_df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>ipa</th>\n",
       "      <th>phoneme_starts</th>\n",
       "      <th>phoneme_ends</th>\n",
       "      <th>cropped_audio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>sɝvðɨkoʊlslɔʔæftɚaɪʔædðiɔɪl</td>\n",
       "      <td>[0, 2330, 4280, 6560, 7000, 7770, 8760, 9700, ...</td>\n",
       "      <td>[2330, 4280, 6560, 7000, 7770, 8760, 9700, 104...</td>\n",
       "      <td>[1, 0, -2, -2, 6, 5, 1, 5, 6, 6, 6, 6, 7, 5, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>ʃiɦɛdjɚdɑɹksʉtʔɨngɹiziwɑʃwɔɾɚɔljɪɝ</td>\n",
       "      <td>[0, 2370, 4804, 5765, 6600, 8280, 8650, 9060, ...</td>\n",
       "      <td>[2370, 4804, 5765, 6600, 8280, 8650, 9060, 941...</td>\n",
       "      <td>[-1, 0, -1, -2, 0, -1, -2, 3, 0, 2, 3, -2, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>maɪkl̩ kʌlɝdðɨbɛdɹʉmwɔlwɨθkɹeɪɑnz</td>\n",
       "      <td>[0, 1960, 3000, 5240, 6040, 6684, 7644, 8840, ...</td>\n",
       "      <td>[1960, 3000, 5240, 6040, 6684, 7644, 8840, 102...</td>\n",
       "      <td>[4, 2, 3, 3, 1, -1, -1, 3, 3, -1, -3, 0, -1, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>lændpɛɹɨnthʊdʔɔɹgɨnɪzeɪʃɨnzpɚmoʊtɝθkɨnɹoʊl</td>\n",
       "      <td>[0, 2250, 3950, 5206, 7720, 8680, 9080, 9720, ...</td>\n",
       "      <td>[2250, 3950, 5206, 7720, 8680, 9080, 9720, 108...</td>\n",
       "      <td>[-1, 2, 1, 2, 3, 2, 5, 4, 0, 4, 2, 1, 4, 5, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>hɛlpɹɛgɨpɪkəpɛkəvpɨteɪɾoʊz</td>\n",
       "      <td>[0, 2680, 4000, 5266, 6040, 8170, 8820, 9880, ...</td>\n",
       "      <td>[2680, 4000, 5266, 6040, 8170, 8820, 9880, 120...</td>\n",
       "      <td>[3, 2, 3, 0, 1, 5, 0, 1, 3, 3, 2, 3, 1, 0, 4, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               audio  \\\n",
       "0  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "1  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "2  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "3  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "4  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "\n",
       "                                          ipa  \\\n",
       "0                 sɝvðɨkoʊlslɔʔæftɚaɪʔædðiɔɪl   \n",
       "1          ʃiɦɛdjɚdɑɹksʉtʔɨngɹiziwɑʃwɔɾɚɔljɪɝ   \n",
       "2           maɪkl̩ kʌlɝdðɨbɛdɹʉmwɔlwɨθkɹeɪɑnz   \n",
       "3  lændpɛɹɨnthʊdʔɔɹgɨnɪzeɪʃɨnzpɚmoʊtɝθkɨnɹoʊl   \n",
       "4                  hɛlpɹɛgɨpɪkəpɛkəvpɨteɪɾoʊz   \n",
       "\n",
       "                                      phoneme_starts  \\\n",
       "0  [0, 2330, 4280, 6560, 7000, 7770, 8760, 9700, ...   \n",
       "1  [0, 2370, 4804, 5765, 6600, 8280, 8650, 9060, ...   \n",
       "2  [0, 1960, 3000, 5240, 6040, 6684, 7644, 8840, ...   \n",
       "3  [0, 2250, 3950, 5206, 7720, 8680, 9080, 9720, ...   \n",
       "4  [0, 2680, 4000, 5266, 6040, 8170, 8820, 9880, ...   \n",
       "\n",
       "                                        phoneme_ends  \\\n",
       "0  [2330, 4280, 6560, 7000, 7770, 8760, 9700, 104...   \n",
       "1  [2370, 4804, 5765, 6600, 8280, 8650, 9060, 941...   \n",
       "2  [1960, 3000, 5240, 6040, 6684, 7644, 8840, 102...   \n",
       "3  [2250, 3950, 5206, 7720, 8680, 9080, 9720, 108...   \n",
       "4  [2680, 4000, 5266, 6040, 8170, 8820, 9880, 120...   \n",
       "\n",
       "                                       cropped_audio  \n",
       "0  [1, 0, -2, -2, 6, 5, 1, 5, 6, 6, 6, 6, 7, 5, 5...  \n",
       "1  [-1, 0, -1, -2, 0, -1, -2, 3, 0, 2, 3, -2, 4, ...  \n",
       "2  [4, 2, 3, 3, 1, -1, -1, 3, 3, -1, -3, 0, -1, 1...  \n",
       "3  [-1, 2, 1, 2, 3, 2, 5, 4, 0, 4, 2, 1, 4, 5, 3,...  \n",
       "4  [3, 2, 3, 0, 1, 5, 0, 1, 3, 3, 2, 3, 1, 0, 4, ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['cropped_audio'] = test_df.apply(crop_audio, axis=1)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5f36eb9730>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize again, much better!\n",
    "plt.plot(train_df['cropped_audio'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ipa</th>\n",
       "      <th>audio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aʊɹɚsivdɨddʒɔɪɾ̃əpɔɪmɨntɨnəbaɪɑləddʒiɪniɛnddʒɨ...</td>\n",
       "      <td>[4, 0, 0, 3, 1, -2, 0, 2, 1, 4, 2, 4, 1, 3, 4,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ɑdleɪztɑpɹaɪɔɹɾiɔŋgɛɾɨŋɨzbaɪkfɪkst</td>\n",
       "      <td>[3, 4, 6, 1, 5, 0, 3, 2, 5, 3, 1, 4, 3, 4, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ʃiɦɪddʒʌmpəweɪfɹm̩hɨʃaɪtʌttʃlaɪkɨkætkn̩fɹʌntɨd...</td>\n",
       "      <td>[-2, 2, 2, 0, -1, 0, -2, -1, 0, 3, 0, 0, 2, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ðəmɔɪʃɝʔɪnmaɪaɪzʔɨzfɹʌmʔaɪdɹɑfsnɑtfɹəmtiɨz</td>\n",
       "      <td>[0, 1, -2, 4, 0, -1, -5, -3, -4, -3, 0, -3, -2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mɑaɪdɪlmɔɹɾ̃ɨŋbɨgɪzwəθhɑʔkɔfi</td>\n",
       "      <td>[2, 1, 3, 1, 2, 2, 2, 2, 0, 1, -2, 1, 3, 4, 2,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 ipa  \\\n",
       "0  aʊɹɚsivdɨddʒɔɪɾ̃əpɔɪmɨntɨnəbaɪɑləddʒiɪniɛnddʒɨ...   \n",
       "1                 ɑdleɪztɑpɹaɪɔɹɾiɔŋgɛɾɨŋɨzbaɪkfɪkst   \n",
       "2  ʃiɦɪddʒʌmpəweɪfɹm̩hɨʃaɪtʌttʃlaɪkɨkætkn̩fɹʌntɨd...   \n",
       "3         ðəmɔɪʃɝʔɪnmaɪaɪzʔɨzfɹʌmʔaɪdɹɑfsnɑtfɹəmtiɨz   \n",
       "4                      mɑaɪdɪlmɔɹɾ̃ɨŋbɨgɪzwəθhɑʔkɔfi   \n",
       "\n",
       "                                               audio  \n",
       "0  [4, 0, 0, 3, 1, -2, 0, 2, 1, 4, 2, 4, 1, 3, 4,...  \n",
       "1  [3, 4, 6, 1, 5, 0, 3, 2, 5, 3, 1, 4, 3, 4, 5, ...  \n",
       "2  [-2, 2, 2, 0, -1, 0, -2, -1, 0, 3, 0, 0, 2, 1,...  \n",
       "3  [0, 1, -2, 4, 0, -1, -5, -3, -4, -3, 0, -3, -2...  \n",
       "4  [2, 1, 3, 1, 2, 2, 2, 2, 0, 1, -2, 1, 3, 4, 2,...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_df comprised of just cropped_audio and phonemes\n",
    "train_df = train_df.drop(columns=['phoneme_starts', 'phoneme_ends', 'audio'])\n",
    "# rename cropped audio to audio\n",
    "train_df = train_df.rename(columns={'cropped_audio': 'audio'})\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ipa</th>\n",
       "      <th>audio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sɝvðɨkoʊlslɔʔæftɚaɪʔædðiɔɪl</td>\n",
       "      <td>[1, 0, -2, -2, 6, 5, 1, 5, 6, 6, 6, 6, 7, 5, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ʃiɦɛdjɚdɑɹksʉtʔɨngɹiziwɑʃwɔɾɚɔljɪɝ</td>\n",
       "      <td>[-1, 0, -1, -2, 0, -1, -2, 3, 0, 2, 3, -2, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>maɪkl̩ kʌlɝdðɨbɛdɹʉmwɔlwɨθkɹeɪɑnz</td>\n",
       "      <td>[4, 2, 3, 3, 1, -1, -1, 3, 3, -1, -3, 0, -1, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lændpɛɹɨnthʊdʔɔɹgɨnɪzeɪʃɨnzpɚmoʊtɝθkɨnɹoʊl</td>\n",
       "      <td>[-1, 2, 1, 2, 3, 2, 5, 4, 0, 4, 2, 1, 4, 5, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hɛlpɹɛgɨpɪkəpɛkəvpɨteɪɾoʊz</td>\n",
       "      <td>[3, 2, 3, 0, 1, 5, 0, 1, 3, 3, 2, 3, 1, 0, 4, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          ipa  \\\n",
       "0                 sɝvðɨkoʊlslɔʔæftɚaɪʔædðiɔɪl   \n",
       "1          ʃiɦɛdjɚdɑɹksʉtʔɨngɹiziwɑʃwɔɾɚɔljɪɝ   \n",
       "2           maɪkl̩ kʌlɝdðɨbɛdɹʉmwɔlwɨθkɹeɪɑnz   \n",
       "3  lændpɛɹɨnthʊdʔɔɹgɨnɪzeɪʃɨnzpɚmoʊtɝθkɨnɹoʊl   \n",
       "4                  hɛlpɹɛgɨpɪkəpɛkəvpɨteɪɾoʊz   \n",
       "\n",
       "                                               audio  \n",
       "0  [1, 0, -2, -2, 6, 5, 1, 5, 6, 6, 6, 6, 7, 5, 5...  \n",
       "1  [-1, 0, -1, -2, 0, -1, -2, 3, 0, 2, 3, -2, 4, ...  \n",
       "2  [4, 2, 3, 3, 1, -1, -1, 3, 3, -1, -3, 0, -1, 1...  \n",
       "3  [-1, 2, 1, 2, 3, 2, 5, 4, 0, 4, 2, 1, 4, 5, 3,...  \n",
       "4  [3, 2, 3, 0, 1, 5, 0, 1, 3, 3, 2, 3, 1, 0, 4, ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same for test_df\n",
    "test_df = test_df.drop(columns=['phoneme_starts', 'phoneme_ends', 'audio'])\n",
    "test_df = test_df.rename(columns={'cropped_audio': 'audio'})\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend Phoneme Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_vocab = set(\"\".join(train_df['ipa'])) | set(\"\".join(test_df['ipa']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_ID)\n",
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ɦ', '̥', '̍', '̩', 'g', 'ɝ', '̃'}\n"
     ]
    }
   ],
   "source": [
    "additional_vocab = timit_vocab.difference(set(vocab.keys()) | {' '})\n",
    "tokenizer.add_tokens(list(additional_vocab))\n",
    "print(additional_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/4620 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4620/4620 [01:52<00:00, 41.12 examples/s]\n",
      "Map: 100%|██████████| 1680/1680 [00:41<00:00, 40.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# replace ' ' in ipa with tokenizer.pad_token\n",
    "train_ds = train_ds.map(lambda x: {'audio': x['audio'], 'ipa': x['ipa'].replace(' ', tokenizer.pad_token)})\n",
    "test_ds = test_ds.map(lambda x: {'audio': x['audio'], 'ipa': x['ipa'].replace(' ', tokenizer.pad_token)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(PRE_TRAINED_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to update the feature extractor since it has been pretrained on 16kHz audio which matches the TIMIT dataset.\n",
    "\n",
    "For datasets with different sampling rates, the feature extractor should be updated or the audio resampled (easier).\n",
    "\n",
    "This is also where code to add extra features (such as conditioning on speaker's native language etc.) would be added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Updated Tokenizer and Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(PRE_TRAINED_ID)\n",
    "processor.tokenizer = tokenizer\n",
    "processor.feature_extractor = feature_extractor\n",
    "processor.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    batch[\"input_values\"] = processor(batch[\"audio\"], sampling_rate=feature_extractor.sampling_rate).input_values\n",
    "    batch[\"labels\"] = processor(text=batch[\"ipa\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4620/4620 [02:06<00:00, 36.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds_prepared = train_ds.map(prepare_dataset, batch_size=8, num_proc=1, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 1680/1680 [00:19<00:00, 87.19 examples/s] \n"
     ]
    }
   ],
   "source": [
    "test_ds_prepared = test_ds.map(prepare_dataset, batch_size=8, num_proc=4, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ipa</th>\n",
       "      <th>audio</th>\n",
       "      <th>input_values</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aʊɹɚsivdɨddʒɔɪɾ̃əpɔɪmɨntɨnəbaɪɑləddʒiɪniɛnddʒɨ...</td>\n",
       "      <td>[4, 0, 0, 3, 1, -2, 0, 2, 1, 4, 2, 4, 1, 3, 4,...</td>\n",
       "      <td>[0.005778457815224606, 0.00016013673891347244,...</td>\n",
       "      <td>[20, 8, 14, 15, 43, 6, 46, 44, 6, 44, 6, 63, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ɑdleɪztɑpɹaɪɔɹɾiɔŋgɛɾɨŋɨzbaɪkfɪkst</td>\n",
       "      <td>[3, 4, 6, 1, 5, 0, 3, 2, 5, 3, 1, 4, 3, 4, 5, ...</td>\n",
       "      <td>[0.0030405941378357517, 0.0040448967068924604,...</td>\n",
       "      <td>[8, 14, 15, 43, 6, 46, 23, 37, 25, 32, 33, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ʃiɦɪddʒʌmpəweɪfɹm̩hɨʃaɪtʌttʃlaɪkɨkætkn̩fɹʌntɨd...</td>\n",
       "      <td>[-2, 2, 2, 0, -1, 0, -2, -1, 0, 3, 0, 0, 2, 1,...</td>\n",
       "      <td>[-0.004731169339948622, 0.005373915718013923, ...</td>\n",
       "      <td>[14, 38, 37, 392, 5, 13, 67, 8, 11, 36, 18, 37...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ðəmɔɪʃɝʔɪnmaɪaɪzʔɨzfɹʌmʔaɪdɹɑfsnɑtfɹəmtiɨz</td>\n",
       "      <td>[0, 1, -2, 4, 0, -1, -5, -3, -4, -3, 0, -3, -2...</td>\n",
       "      <td>[0.00046617271074254955, 0.0023691686010726097...</td>\n",
       "      <td>[14, 22, 38, 32, 41, 14, 13, 49, 18, 7, 4, 67,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mɑaɪdɪlmɔɹɾ̃ɨŋbɨgɪzwəθhɑʔkɔfi</td>\n",
       "      <td>[2, 1, 3, 1, 2, 2, 2, 2, 0, 1, -2, 1, 3, 4, 2,...</td>\n",
       "      <td>[0.002510627313697649, 0.0011226055582206742, ...</td>\n",
       "      <td>[14, 13, 8, 14, 15, 43, 6, 46, 23, 37, 25, 32,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 ipa  \\\n",
       "0  aʊɹɚsivdɨddʒɔɪɾ̃əpɔɪmɨntɨnəbaɪɑləddʒiɪniɛnddʒɨ...   \n",
       "1                 ɑdleɪztɑpɹaɪɔɹɾiɔŋgɛɾɨŋɨzbaɪkfɪkst   \n",
       "2  ʃiɦɪddʒʌmpəweɪfɹm̩hɨʃaɪtʌttʃlaɪkɨkætkn̩fɹʌntɨd...   \n",
       "3         ðəmɔɪʃɝʔɪnmaɪaɪzʔɨzfɹʌmʔaɪdɹɑfsnɑtfɹəmtiɨz   \n",
       "4                      mɑaɪdɪlmɔɹɾ̃ɨŋbɨgɪzwəθhɑʔkɔfi   \n",
       "\n",
       "                                               audio  \\\n",
       "0  [4, 0, 0, 3, 1, -2, 0, 2, 1, 4, 2, 4, 1, 3, 4,...   \n",
       "1  [3, 4, 6, 1, 5, 0, 3, 2, 5, 3, 1, 4, 3, 4, 5, ...   \n",
       "2  [-2, 2, 2, 0, -1, 0, -2, -1, 0, 3, 0, 0, 2, 1,...   \n",
       "3  [0, 1, -2, 4, 0, -1, -5, -3, -4, -3, 0, -3, -2...   \n",
       "4  [2, 1, 3, 1, 2, 2, 2, 2, 0, 1, -2, 1, 3, 4, 2,...   \n",
       "\n",
       "                                        input_values  \\\n",
       "0  [0.005778457815224606, 0.00016013673891347244,...   \n",
       "1  [0.0030405941378357517, 0.0040448967068924604,...   \n",
       "2  [-0.004731169339948622, 0.005373915718013923, ...   \n",
       "3  [0.00046617271074254955, 0.0023691686010726097...   \n",
       "4  [0.002510627313697649, 0.0011226055582206742, ...   \n",
       "\n",
       "                                              labels  \n",
       "0  [20, 8, 14, 15, 43, 6, 46, 44, 6, 44, 6, 63, 4...  \n",
       "1  [8, 14, 15, 43, 6, 46, 23, 37, 25, 32, 33, 4, ...  \n",
       "2  [14, 38, 37, 392, 5, 13, 67, 8, 11, 36, 18, 37...  \n",
       "3  [14, 22, 38, 32, 41, 14, 13, 49, 18, 7, 4, 67,...  \n",
       "4  [14, 13, 8, 14, 15, 43, 6, 46, 23, 37, 25, 32,...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_prepared.to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, processor: AutoProcessor, padding=True, max_length=None, max_length_labels=None, pad_to_multiple_of=None, pad_to_multiple_of_labels=None):\n",
    "        self.processor = processor\n",
    "        self.padding = padding\n",
    "        self.max_length = max_length\n",
    "        self.max_length_labels = max_length_labels\n",
    "        self.pad_to_multiple_of = pad_to_multiple_of\n",
    "        self.pad_to_multiple_of_labels = pad_to_multiple_of_labels\n",
    "\n",
    "    def __call__(self, features: \"list[dict[str, list[int] | torch.Tensor]]\") -> \"dict[str, torch.Tensor]\":\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cer(pred, label):\n",
    "    distances = np.zeros((len(pred) + 1, len(label) + 1))\n",
    "\n",
    "    for t1 in range(len(pred) + 1):\n",
    "        distances[t1][0] = t1\n",
    "\n",
    "    for t2 in range(len(label) + 1):\n",
    "        distances[0][t2] = t2\n",
    "        \n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "    \n",
    "    for t1 in range(1, len(pred) + 1):\n",
    "        for t2 in range(1, len(label) + 1):\n",
    "            if (pred[t1-1] == label[t2-1]):\n",
    "                distances[t1][t2] = distances[t1 - 1][t2 - 1]\n",
    "            else:\n",
    "                a = distances[t1][t2 - 1]\n",
    "                b = distances[t1 - 1][t2]\n",
    "                c = distances[t1 - 1][t2 - 1]\n",
    "                \n",
    "                if (a <= b and a <= c):\n",
    "                    distances[t1][t2] = a + 1\n",
    "                elif (b <= a and b <= c):\n",
    "                    distances[t1][t2] = b + 1\n",
    "                else:\n",
    "                    distances[t1][t2] = c + 1\n",
    "\n",
    "    return distances[len(pred)][len(label)] / len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False) # labels are already grouped as they should be\n",
    "\n",
    "    # Call panphon_model_eval with label and predictedipa\n",
    "    results = panphon_model_eval(label_str, pred_str)\n",
    "\n",
    "    # Output results\n",
    "    print(\"Evaluation Results:\")\n",
    "    print(f\"Feature edit distance: {results['feature_dist']}\")\n",
    "    print(f\"Weighted feature edit distance: {results['weighted_feature_dist']}\")\n",
    "    print(f\"Hamming distance: {results['hamming_feature_dist']}\")\n",
    "    print(f\"CER: {results['cer_score']}\")\n",
    "\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCTC.from_pretrained(\n",
    "    PRE_TRAINED_ID, \n",
    "    pad_token_id=processor.tokenizer.pad_token_id, \n",
    "    # gradient_checkpointing=True,\n",
    "    ctc_zero_infinity = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# model.resize_token_embeddings(len(tokenizer))\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m old_weights \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mt()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m      3\u001b[0m old_bias \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m      4\u001b[0m new_layer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(model\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39min_features, \u001b[38;5;28mlen\u001b[39m(processor\u001b[38;5;241m.\u001b[39mtokenizer))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "old_weights = model.lm_head.weight.t().detach()\n",
    "old_bias = model.lm_head.bias.detach()\n",
    "new_layer = torch.nn.Linear(model.lm_head.in_features, len(processor.tokenizer))\n",
    "new_layer.weight.data[:model.lm_head.out_features, :] = old_weights.t()\n",
    "new_layer.bias.data[:model.lm_head.out_features] = old_bias\n",
    "model.lm_head = new_layer\n",
    "model.config.vocab_size = len(processor.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "max_class_id = 0\n",
    "for label_sequence in train_ds_prepared['labels']:\n",
    "    for class_id in label_sequence:\n",
    "        max_class_id = max(max_class_id, class_id)\n",
    "assert max_class_id == model.config.vocab_size - 1\n",
    "\n",
    "for label_sequence in train_ds_prepared['labels']:\n",
    "    for class_id in label_sequence:\n",
    "        assert class_id >= 0 and class_id <  model.config.vocab_size\n",
    "\n",
    "for label_sequence in train_ds_prepared['labels']:\n",
    "    assert  len(label_sequence) < 1024\n",
    "    assert len(label_sequence) >= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/arunasrivastava/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/arunasrivastava/ML/notebooks/wandb/run-20241117_001603-a75ru3zx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aruna-team/timit-finetune-a100/runs/a75ru3zx' target=\"_blank\">silvery-shape-1</a></strong> to <a href='https://wandb.ai/aruna-team/timit-finetune-a100' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aruna-team/timit-finetune-a100' target=\"_blank\">https://wandb.ai/aruna-team/timit-finetune-a100</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aruna-team/timit-finetune-a100/runs/a75ru3zx' target=\"_blank\">https://wandb.ai/aruna-team/timit-finetune-a100/runs/a75ru3zx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"timit-finetune-a100\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    eval_strategy=\"steps\",\n",
    "    num_train_epochs=150,\n",
    "    fp16=True,\n",
    "    save_steps=50,\n",
    "    eval_steps=50,\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=1000,\n",
    "    save_total_limit=3,\n",
    "    gradient_checkpointing=True,\n",
    "    load_best_model_at_end=True,\n",
    "    max_grad_norm=0.5,\n",
    "    report_to=\"wandb\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_ds_prepared.select(range(200)),\n",
    "    # train_dataset=train_ds_prepared,\n",
    "    eval_dataset=test_ds_prepared.select(range(200)),\n",
    "    # eval_dataset=test_ds_prepared,\n",
    "    processing_class=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# torch.autograd.set_detect_anomaly(False) # debug NaNs, disable for real training to improve performance\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# model.config.ctc_zero_infinity = True \u001b[39;00m\n\u001b[1;32m      3\u001b[0m resume_from_checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m# resume_from_checkpoint=True to resume training if training was interrupted\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# torch.autograd.set_detect_anomaly(False) # debug NaNs, disable for real training to improve performance\n",
    "# model.config.ctc_zero_infinity = True \n",
    "resume_from_checkpoint=True\n",
    "trainer.train() # resume_from_checkpoint=True to resume training if training was interrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "eval_results = trainer.evaluate()\n",
    "wandb.log(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m old_model \u001b[38;5;241m=\u001b[39m AutoModelForCTC\u001b[38;5;241m.\u001b[39mfrom_pretrained(PRE_TRAINED_ID)\u001b[38;5;241m.\u001b[39mto(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "old_model = AutoModelForCTC.from_pretrained(PRE_TRAINED_ID).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = AutoModelForCTC.from_pretrained(os.path.join(OUTPUT_DIR, 'checkpoint-100'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for nan weights and replace with old values if any\n",
    "for name, param in finetuned_model.named_parameters():\n",
    "    if torch.isnan(param).any():\n",
    "        print(name)\n",
    "        param.data = old_model.state_dict()[name].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'old_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m is_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m finetuned_model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mequal(param, \u001b[43mold_model\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict()[name]\u001b[38;5;241m.\u001b[39mto(finetuned_model\u001b[38;5;241m.\u001b[39mdevice)):\n\u001b[1;32m      5\u001b[0m         is_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'old_model' is not defined"
     ]
    }
   ],
   "source": [
    "# compare weights\n",
    "is_diff = False\n",
    "for name, param in finetuned_model.named_parameters():\n",
    "    if not torch.equal(param, old_model.state_dict()[name].to(finetuned_model.device)):\n",
    "        is_diff = True\n",
    "        break\n",
    "print(\"Weights are different, training did something\" if is_diff else \"Weights are the same, training did nothing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, wav_file):\n",
    "    speech = audio_file_to_array(wav_file)\n",
    "    input_values = processor(speech, sampling_rate=16000, return_tensors=\"pt\").input_values.type(torch.float32).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    return processor.decode(predicted_ids[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Transcription: eɪ l ɾ t eɪ t l ɛ s m k ɛ s l p k aɪ s l æ aɪ k ɛ t n d k eɪ ʃ v b ɛ n ɛ f eɪ j b b aɪ b ɛ t iː\n"
     ]
    }
   ],
   "source": [
    "print('Checkpoint Transcription:', predict(finetuned_model, os.path.join('..', 'data', 'alexIsConfused.wav')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Fine-Tune Transcription:', predict(trainer.model, os.path.join('..', 'data', 'alexIsConfused.wav')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest Fine-Tune Transcription:\u001b[39m\u001b[38;5;124m'\u001b[39m, predict(\u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mmodel, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malexIsConfused.wav\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "print('Best Fine-Tune Transcription:', predict(trainer.model, os.path.join('..', 'data', 'alexIsConfused.wav')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Transcription: h ɛ l oʊ æ l ɪ k s ɪ z ɛ k s t ɾ ɹ ə k ə n f iː l ʊ s t\n"
     ]
    }
   ],
   "source": [
    "print('Old Transcription:', predict(old_model, os.path.join('..', 'data', 'alexIsConfused.wav')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: b2pxzie9\n",
      "Sweep URL: https://wandb.ai/aruna-team/timit-finetune-hyperparam/sweeps/b2pxzie9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: j69zyxg1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.9100852568813272e-08\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mvibrant-pyramid-9\u001b[0m at: \u001b[34mhttps://wandb.ai/aruna-team/timit-finetune/runs/h93avd58\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241115_045627-h93avd58/logs\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/arunasrivastava/ML/notebooks/wandb/run-20241115_064444-j69zyxg1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aruna-team/timit-finetune-hyperparam/runs/j69zyxg1' target=\"_blank\">decent-sweep-1</a></strong> to <a href='https://wandb.ai/aruna-team/timit-finetune-hyperparam' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/aruna-team/timit-finetune-hyperparam/sweeps/b2pxzie9' target=\"_blank\">https://wandb.ai/aruna-team/timit-finetune-hyperparam/sweeps/b2pxzie9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aruna-team/timit-finetune-hyperparam' target=\"_blank\">https://wandb.ai/aruna-team/timit-finetune-hyperparam</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/aruna-team/timit-finetune-hyperparam/sweeps/b2pxzie9' target=\"_blank\">https://wandb.ai/aruna-team/timit-finetune-hyperparam/sweeps/b2pxzie9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aruna-team/timit-finetune-hyperparam/runs/j69zyxg1' target=\"_blank\">https://wandb.ai/aruna-team/timit-finetune-hyperparam/runs/j69zyxg1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='864' max='864' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [864/864 42:42, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2126.116800</td>\n",
       "      <td>2166.366211</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1907.205700</td>\n",
       "      <td>2166.355469</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2107.934800</td>\n",
       "      <td>2166.327393</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1952.314600</td>\n",
       "      <td>2166.268555</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1939.089300</td>\n",
       "      <td>2166.171143</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>7263.468000</td>\n",
       "      <td>2166.041992</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>6529.998000</td>\n",
       "      <td>2165.866699</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>6916.693800</td>\n",
       "      <td>2165.670166</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>7391.245300</td>\n",
       "      <td>2165.394775</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6652.000800</td>\n",
       "      <td>2165.065430</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>6298.119900</td>\n",
       "      <td>2164.762207</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4996.916400</td>\n",
       "      <td>2164.319824</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>5035.049200</td>\n",
       "      <td>2163.793701</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4761.062100</td>\n",
       "      <td>2163.259033</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>5095.130500</td>\n",
       "      <td>2162.793213</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>5012.030500</td>\n",
       "      <td>2162.129395</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>3867.489800</td>\n",
       "      <td>2161.476074</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>eval/cer</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/loss</td><td>██████▇▇▇▆▆▅▄▄▃▂▁▁</td></tr><tr><td>eval/runtime</td><td>▄▃▄▂▁▃▁▁▂▃▅▂▆▅█▅█▃</td></tr><tr><td>eval/samples_per_second</td><td>▅▆▄▇█▆██▇▆▄▇▃▄▁▄▁▆</td></tr><tr><td>eval/steps_per_second</td><td>▅▆▄▇█▆██▇▆▄▇▃▄▁▄▁▆</td></tr><tr><td>eval_cer</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_runtime</td><td>▁</td></tr><tr><td>eval_samples_per_second</td><td>▁</td></tr><tr><td>eval_steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/grad_norm</td><td>▅▃▂▃▄▂▁▇▂▃▆▄▂▃▆▃▅▂▂▆▅▃▂▂▃▂▂▅▄▄▂▆▅▃▅█▃▃▂▂</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇███</td></tr><tr><td>train/loss</td><td>█▆▁▁██▅▄▂▁▂▁▅▁▂▆▄▂▃▂▇▂█▃▂▂▂▇▃▆▃▁▅▃▂▂▃▇▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2.99221</td></tr><tr><td>eval/cer</td><td>1</td></tr><tr><td>eval/loss</td><td>2161.47583</td></tr><tr><td>eval/runtime</td><td>54.255</td></tr><tr><td>eval/samples_per_second</td><td>30.965</td></tr><tr><td>eval/steps_per_second</td><td>3.871</td></tr><tr><td>eval_cer</td><td>1</td></tr><tr><td>eval_loss</td><td>2161.47583</td></tr><tr><td>eval_runtime</td><td>54.255</td></tr><tr><td>eval_samples_per_second</td><td>30.965</td></tr><tr><td>eval_steps_per_second</td><td>3.871</td></tr><tr><td>total_flos</td><td>1.1292686285818781e+18</td></tr><tr><td>train/epoch</td><td>2.99221</td></tr><tr><td>train/global_step</td><td>864</td></tr><tr><td>train/grad_norm</td><td>793.75781</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2545.9715</td></tr><tr><td>train_loss</td><td>4258.44711</td></tr><tr><td>train_runtime</td><td>2563.9146</td></tr><tr><td>train_samples_per_second</td><td>5.406</td></tr><tr><td>train_steps_per_second</td><td>0.337</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">decent-sweep-1</strong> at: <a href='https://wandb.ai/aruna-team/timit-finetune-hyperparam/runs/j69zyxg1' target=\"_blank\">https://wandb.ai/aruna-team/timit-finetune-hyperparam/runs/j69zyxg1</a><br/> View project at: <a href='https://wandb.ai/aruna-team/timit-finetune-hyperparam' target=\"_blank\">https://wandb.ai/aruna-team/timit-finetune-hyperparam</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241115_064444-j69zyxg1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 93qrmgzh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.670512733589786e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/arunasrivastava/ML/notebooks/wandb/run-20241115_073116-93qrmgzh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aruna-team/timit-finetune-hyperparam/runs/93qrmgzh' target=\"_blank\">comfy-sweep-2</a></strong> to <a href='https://wandb.ai/aruna-team/timit-finetune-hyperparam' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/aruna-team/timit-finetune-hyperparam/sweeps/b2pxzie9' target=\"_blank\">https://wandb.ai/aruna-team/timit-finetune-hyperparam/sweeps/b2pxzie9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aruna-team/timit-finetune-hyperparam' target=\"_blank\">https://wandb.ai/aruna-team/timit-finetune-hyperparam</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/aruna-team/timit-finetune-hyperparam/sweeps/b2pxzie9' target=\"_blank\">https://wandb.ai/aruna-team/timit-finetune-hyperparam/sweeps/b2pxzie9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aruna-team/timit-finetune-hyperparam/runs/93qrmgzh' target=\"_blank\">https://wandb.ai/aruna-team/timit-finetune-hyperparam/runs/93qrmgzh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='801' max='2880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 801/2880 37:44 < 1:38:13, 0.35 it/s, Epoch 2.77/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2124.236500</td>\n",
       "      <td>2160.302490</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1905.107800</td>\n",
       "      <td>2156.335449</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2103.522900</td>\n",
       "      <td>2149.275879</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1944.339100</td>\n",
       "      <td>2141.821533</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1935.538900</td>\n",
       "      <td>2135.632080</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>7223.684400</td>\n",
       "      <td>2130.099609</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>6485.108200</td>\n",
       "      <td>2125.869385</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>6841.821100</td>\n",
       "      <td>2121.018066</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>7304.120300</td>\n",
       "      <td>2116.460938</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6586.377300</td>\n",
       "      <td>2111.827148</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>6221.235200</td>\n",
       "      <td>2110.767334</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4943.890600</td>\n",
       "      <td>2104.278320</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>4976.560200</td>\n",
       "      <td>2100.485596</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4695.405900</td>\n",
       "      <td>2097.608398</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>5026.596500</td>\n",
       "      <td>2097.271240</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4933.773800</td>\n",
       "      <td>2091.371094</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    }
   ],
   "source": [
    "# import wandb\n",
    "\n",
    "# sweep_config = {\n",
    "#     'method': 'bayes',  # Or 'grid' or 'random'\n",
    "#     'metric': {'name': 'loss', 'goal': 'minimize'},\n",
    "#     'parameters': {\n",
    "#         'learning_rate': {'min': 1e-8, 'max': 5e-6, 'distribution': 'log_uniform_values'},\n",
    "#         'per_device_train_batch_size': {'values': [4, 8, 16]},\n",
    "#         'num_train_epochs': {'values': [3, 5, 10]},\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep_config, project=\"timit-finetune-hyperparam\")\n",
    "\n",
    "# from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# def train():\n",
    "#     # Initialize a new wandb run\n",
    "#     wandb.init()\n",
    "\n",
    "#     # Set up training arguments, pulling values from wandb.config\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=OUTPUT_DIR,\n",
    "#         group_by_length=True,\n",
    "#         per_device_train_batch_size=wandb.config.per_device_train_batch_size,\n",
    "#         gradient_accumulation_steps=4,\n",
    "#         eval_strategy=\"steps\",\n",
    "#         num_train_epochs=wandb.config.num_train_epochs,\n",
    "#         fp16=True,\n",
    "#         save_steps=50,\n",
    "#         eval_steps=50,\n",
    "#         logging_steps=10,\n",
    "#         learning_rate=wandb.config.learning_rate,\n",
    "#         warmup_steps=1000,\n",
    "#         save_total_limit=3,\n",
    "#         gradient_checkpointing=True,\n",
    "#         load_best_model_at_end=True,\n",
    "#         max_grad_norm=1.0,\n",
    "#         report_to=\"wandb\"\n",
    "#     )\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         data_collator=data_collator,\n",
    "#         args=training_args,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#         train_dataset=train_ds_prepared,\n",
    "#         eval_dataset=test_ds_prepared,\n",
    "#         processing_class=processor.feature_extractor,\n",
    "#     )\n",
    "\n",
    "#     # Run training and automatically log results to W&B\n",
    "#     trainer.train()\n",
    "\n",
    "#     # Ensure to log any additional evaluation metrics\n",
    "#     eval_metrics = trainer.evaluate()\n",
    "#     wandb.log(eval_metrics)\n",
    "\n",
    "\n",
    "\n",
    "# wandb.agent(sweep_id, train, count=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2u5w603x) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fresh-capybara-4</strong> at: <a href='https://wandb.ai/aruna-team/hyperparam/runs/2u5w603x' target=\"_blank\">https://wandb.ai/aruna-team/hyperparam/runs/2u5w603x</a><br/> View project at: <a href='https://wandb.ai/aruna-team/hyperparam' target=\"_blank\">https://wandb.ai/aruna-team/hyperparam</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241114_190553-2u5w603x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2u5w603x). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/arunasrivastava/ML/notebooks/wandb/run-20241114_190614-jm8ozxyq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aruna-team/hyperparam/runs/jm8ozxyq' target=\"_blank\">misty-universe-5</a></strong> to <a href='https://wandb.ai/aruna-team/hyperparam' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aruna-team/hyperparam' target=\"_blank\">https://wandb.ai/aruna-team/hyperparam</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aruna-team/hyperparam/runs/jm8ozxyq' target=\"_blank\">https://wandb.ai/aruna-team/hyperparam/runs/jm8ozxyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 19:06:16,626] A new study created in memory with name: no-name-4f36681b-7de1-48b5-82a9-e3e4641bccb6\n",
      "/var/tmp/ipykernel_7470/3929815082.py:15: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  num_train_epochs = trial.suggest_int(\"num_train_epochs\", 5, 10, 20)\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/optuna/distributions.py:704: UserWarning: The distribution is specified by [5, 10] and step=20, but the range is not divisible by `step`. It will be replaced by [5, 5].\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:47, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 19:07:09,061] Trial 0 finished with value: 4984.161979166666 and parameters: {'learning_rate': 7.687952860086006e-07, 'per_device_train_batch_size': 4, 'num_train_epochs': 5}. Best is trial 0 with value: 4984.161979166666.\n",
      "/var/tmp/ipykernel_7470/3929815082.py:15: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  num_train_epochs = trial.suggest_int(\"num_train_epochs\", 5, 10, 20)\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/optuna/distributions.py:704: UserWarning: The distribution is specified by [5, 10] and step=20, but the range is not divisible by `step`. It will be replaced by [5, 5].\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:48, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 19:08:01,943] Trial 1 finished with value: 4984.1171875 and parameters: {'learning_rate': 5.973695659770477e-07, 'per_device_train_batch_size': 4, 'num_train_epochs': 5}. Best is trial 1 with value: 4984.1171875.\n",
      "/var/tmp/ipykernel_7470/3929815082.py:15: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  num_train_epochs = trial.suggest_int(\"num_train_epochs\", 5, 10, 20)\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/optuna/distributions.py:704: UserWarning: The distribution is specified by [5, 10] and step=20, but the range is not divisible by `step`. It will be replaced by [5, 5].\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:48, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 19:08:55,255] Trial 2 finished with value: 4984.077604166667 and parameters: {'learning_rate': 1.493259017726814e-06, 'per_device_train_batch_size': 4, 'num_train_epochs': 5}. Best is trial 2 with value: 4984.077604166667.\n",
      "/var/tmp/ipykernel_7470/3929815082.py:15: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  num_train_epochs = trial.suggest_int(\"num_train_epochs\", 5, 10, 20)\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/optuna/distributions.py:704: UserWarning: The distribution is specified by [5, 10] and step=20, but the range is not divisible by `step`. It will be replaced by [5, 5].\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 00:59, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 19:09:59,119] Trial 3 finished with value: 2519.1041666666665 and parameters: {'learning_rate': 3.2132405085015246e-07, 'per_device_train_batch_size': 2, 'num_train_epochs': 5}. Best is trial 3 with value: 2519.1041666666665.\n",
      "/var/tmp/ipykernel_7470/3929815082.py:15: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  num_train_epochs = trial.suggest_int(\"num_train_epochs\", 5, 10, 20)\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/optuna/distributions.py:704: UserWarning: The distribution is specified by [5, 10] and step=20, but the range is not divisible by `step`. It will be replaced by [5, 5].\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:04, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 19:11:07,802] Trial 4 finished with value: 2519.0208333333335 and parameters: {'learning_rate': 2.0615922759934747e-06, 'per_device_train_batch_size': 2, 'num_train_epochs': 5}. Best is trial 4 with value: 2519.0208333333335.\n",
      "/var/tmp/ipykernel_7470/3929815082.py:15: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  num_train_epochs = trial.suggest_int(\"num_train_epochs\", 5, 10, 20)\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/optuna/distributions.py:704: UserWarning: The distribution is specified by [5, 10] and step=20, but the range is not divisible by `step`. It will be replaced by [5, 5].\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:41, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 19:11:55,215] Trial 5 finished with value: 9643.107291666667 and parameters: {'learning_rate': 1.0927644338378763e-07, 'per_device_train_batch_size': 8, 'num_train_epochs': 5}. Best is trial 4 with value: 2519.0208333333335.\n",
      "/var/tmp/ipykernel_7470/3929815082.py:15: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  num_train_epochs = trial.suggest_int(\"num_train_epochs\", 5, 10, 20)\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/optuna/distributions.py:704: UserWarning: The distribution is specified by [5, 10] and step=20, but the range is not divisible by `step`. It will be replaced by [5, 5].\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:48, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 19:12:48,126] Trial 6 finished with value: 4983.488020833333 and parameters: {'learning_rate': 5.935061857451671e-06, 'per_device_train_batch_size': 4, 'num_train_epochs': 5}. Best is trial 4 with value: 2519.0208333333335.\n",
      "/var/tmp/ipykernel_7470/3929815082.py:15: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  num_train_epochs = trial.suggest_int(\"num_train_epochs\", 5, 10, 20)\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/optuna/distributions.py:704: UserWarning: The distribution is specified by [5, 10] and step=20, but the range is not divisible by `step`. It will be replaced by [5, 5].\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:03, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 19:13:56,494] Trial 7 finished with value: 2518.512760416667 and parameters: {'learning_rate': 4.139150701404576e-06, 'per_device_train_batch_size': 2, 'num_train_epochs': 5}. Best is trial 7 with value: 2518.512760416667.\n",
      "/var/tmp/ipykernel_7470/3929815082.py:15: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  num_train_epochs = trial.suggest_int(\"num_train_epochs\", 5, 10, 20)\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/optuna/distributions.py:704: UserWarning: The distribution is specified by [5, 10] and step=20, but the range is not divisible by `step`. It will be replaced by [5, 5].\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:04, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 19:15:05,055] Trial 8 finished with value: 2518.04453125 and parameters: {'learning_rate': 2.0900669738601152e-07, 'per_device_train_batch_size': 2, 'num_train_epochs': 5}. Best is trial 8 with value: 2518.04453125.\n",
      "/var/tmp/ipykernel_7470/3929815082.py:15: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  num_train_epochs = trial.suggest_int(\"num_train_epochs\", 5, 10, 20)\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/optuna/distributions.py:704: UserWarning: The distribution is specified by [5, 10] and step=20, but the range is not divisible by `step`. It will be replaced by [5, 5].\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/arunasrivastava/ML/venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:41, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 19:15:52,464] Trial 9 finished with value: 9640.767708333333 and parameters: {'learning_rate': 7.006142803301166e-06, 'per_device_train_batch_size': 8, 'num_train_epochs': 5}. Best is trial 8 with value: 2518.04453125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'learning_rate': 2.0900669738601152e-07, 'per_device_train_batch_size': 2, 'num_train_epochs': 5}\n"
     ]
    }
   ],
   "source": [
    "# import wandb\n",
    "# import optuna\n",
    "# from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# # Initialize wandb\n",
    "# wandb.init(project=\"hyperparam\")\n",
    "\n",
    "# def model_init():\n",
    "#     return model\n",
    "\n",
    "# def objective(trial):\n",
    "#     # Define the hyperparameters to tune\n",
    "#     learning_rate = trial.suggest_float(\"learning_rate\", 1e-8, 1e-7, log=True)\n",
    "#     per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [2, 4, 8])\n",
    "#     num_train_epochs = trial.suggest_int(\"num_train_epochs\", 5, 10, 15)\n",
    "\n",
    "#     # Update training arguments with dynamic hyperparameters\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=OUTPUT_DIR,\n",
    "#         run_name=\"hyperparam-searchC\",  # Set a unique run name\n",
    "#         group_by_length=True,\n",
    "#         per_device_train_batch_size=per_device_train_batch_size,\n",
    "#         gradient_accumulation_steps=4,\n",
    "#         evaluation_strategy=\"steps\",\n",
    "#         num_train_epochs=num_train_epochs,\n",
    "#         fp16=True,\n",
    "#         save_steps=100,\n",
    "#         eval_steps=100,\n",
    "#         logging_steps=100,\n",
    "#         learning_rate=learning_rate,\n",
    "#         warmup_steps=1000,\n",
    "#         save_total_limit=3,\n",
    "#         gradient_checkpointing=True,\n",
    "#         load_best_model_at_end=True,\n",
    "#         max_grad_norm=1.0,\n",
    "#         report_to=\"wandb\"  # Report to WandB\n",
    "#     )\n",
    "\n",
    "#     # Set up the trainer with the training arguments and datasets\n",
    "#     trainer = Trainer(\n",
    "#         model_init=model_init,\n",
    "#         args=training_args,\n",
    "#         data_collator=data_collator,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#         train_dataset=train_ds_prepared.select(range(100)),  # Use a subset for faster experimentation\n",
    "#         eval_dataset=test_ds_prepared.select(range(100)),\n",
    "#         processing_class=processor.feature_extractor,\n",
    "#     )\n",
    "\n",
    "#     # Disable anomaly detection for better performance\n",
    "#     torch.autograd.set_detect_anomaly(False)\n",
    "#     # model.config.ctc_zero_infinity = True  # Uncomment if needed for CTC models\n",
    "\n",
    "#     # Train the model for this trial\n",
    "#     result = trainer.train()  # Set resume_from_checkpoint=True if resuming is required\n",
    "\n",
    "#     # Return the final training loss for Optuna to minimize\n",
    "#     return result.training_loss\n",
    "\n",
    "# # Perform hyperparameter search with updated search space\n",
    "# study = optuna.create_study(direction=\"minimize\")\n",
    "# study.optimize(objective, n_trials=10)\n",
    "\n",
    "# # Print the best hyperparameters\n",
    "# print(\"Best hyperparameters: \", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
