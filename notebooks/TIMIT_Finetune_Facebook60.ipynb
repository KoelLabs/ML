{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from scripts.data_loaders.common import show_sample\n",
    "\n",
    "# importing datasets from dataloader \n",
    "from scripts.data_loaders.L2ARCTIC import L2ArcticDataset, all_arctic_speaker_splits, SPEAKERS\n",
    "from scripts.data_loaders.TIMIT import TIMITDataset\n",
    "from scripts.data_loaders.PSST import PSSTDataset\n",
    "# <TODO> add buckeye and doreco \n",
    "\n",
    "from scripts.eval.evaluate import evaluate\n",
    "from scripts.eval.metrics import per, fer\n",
    "from scripts.ipa_transcription.wav2vec2 import transcribe_batch\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set espeak library path for macOS\n",
    "if sys.platform == \"darwin\":\n",
    "    from phonemizer.backend.espeak.wrapper import EspeakWrapper\n",
    "\n",
    "    _ESPEAK_LIBRARY = \"/opt/homebrew/Cellar/espeak/1.48.04_1/lib/libespeak.1.1.48.dylib\"\n",
    "    EspeakWrapper.set_library(_ESPEAK_LIBRARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PRE_TRAINED_ID = \"facebook/wav2vec2-lv-60-espeak-cv-ft\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_psst = PSSTDataset(split=\"train\", include_speaker_info=True, force_offline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_timit = TIMITDataset(split=\"train\", include_speaker_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_timit = L2ArcticDataset(include_speaker_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend Phoneme Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k', 'o', 'h', 'ŋ', 'p', 'ɡ', 'm', 'v', 'f', 'ʌ', 'e', 'ɛ', 'u', 'a', 'ɹ', 'n', 'ɑ', 'ð', 'ə', 't', 'z', 'ʒ', 'ʊ', 'ɔ', 'b', 's', 'w', 'd', 'æ', 'θ', 'ɾ', 'ʔ', 'ɪ', 'l', 'j', 'i', 'ʃ'}\n"
     ]
    }
   ],
   "source": [
    "timit_vocab = set(\"\".join(train_df['ipa']))\n",
    "print(timit_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 267, 'E': 45, 'H': 72, 'I': 20, 'O': 305, 'U': 292, '[PAD]': 310, '[UNK]': 309, '_': 182, 'a': 53, 'ã': 125, 'b': 113, 'b̪': 177, 'b̪͡v': 289, 'b͡ꞵ': 287, 'c': 25, 'cʼ': 229, 'c͡ç': 66, 'd': 176, 'd̼': 118, 'd͡z': 263, 'd͡ð': 244, 'd͡ɮ': 18, 'd͡ʑ': 94, 'd͡ʒ': 280, 'e': 175, 'ẽ': 211, 'e̞': 114, 'ẽ̞': 265, 'f': 139, 'fʼ': 249, 'h': 179, 'i': 245, 'j': 215, 'k': 282, 'kxʼ': 281, 'kǀ': 41, 'kǁ': 61, 'kǂ': 21, 'kǃ': 70, 'kʘ': 79, 'kʼ': 39, 'k̚': 89, 'k͡p': 48, 'k͡x': 198, 'l': 303, 'm': 75, 'm̥': 172, 'n': 102, 'n̥': 28, 'n̼': 169, 'o': 117, 'õ': 119, 'o̞': 62, 'õ̞': 234, 'p': 259, 'pʼ': 286, 'p̚': 149, 'p̪': 273, 'p̪͡f': 105, 'p͡f': 225, 'p͡ɸ': 103, 'q': 130, 'qǀ': 302, 'qǁ': 126, 'qǂ': 299, 'qǃ': 37, 'qʘ': 261, 'qʼ': 58, 'q͡ʡ': 127, 'q͡χʼ': 291, 'q͡ꭓ': 4, 'r': 82, 'r̥': 34, 's': 247, 'sʼ': 200, 't': 307, 'tʼ': 183, 't̚': 241, 't̪͡θʼ': 216, 't̼': 87, 't͡s': 156, 't͡sʼ': 43, 't͡ɕ': 264, 't͡ɬ': 170, 't͡ɬʼ': 164, 't͡ʃ': 91, 't͡ʃʼ': 108, 't͡θ': 140, 'u': 154, 'ũ': 81, 'v': 243, 'w': 143, 'x': 133, 'xʼ': 129, 'y': 214, 'ỹ': 277, 'z': 271, '{': 251, '}': 74, 'ã': 99, 'ä': 165, 'ä̃': 115, 'æ': 191, 'æ̃': 217, 'ç': 71, 'ð': 69, 'ð̠': 192, 'ð̼': 9, 'õ': 157, 'ø': 47, 'ø̃': 46, 'ø̞': 112, 'ø̞̃': 12, 'ħ': 224, 'ĩ': 180, 'ŋ': 213, 'ŋǀ': 231, 'ŋǁ': 304, 'ŋǂ': 301, 'ŋǃ': 284, 'ŋʘ': 168, 'ŋ̊': 27, 'ŋ͡m': 296, 'œ': 29, 'œ̃': 153, 'ũ': 31, 'ɐ': 142, 'ɐ̃': 7, 'ɑ': 49, 'ɑ̃': 50, 'ɒ': 116, 'ɒ̃': 85, 'ɓ': 178, 'ɓ̥': 13, 'ɔ': 228, 'ɔ̃': 186, 'ɕ': 220, 'ɕʼ': 36, 'ɖ': 256, 'ɖ͡ʐ': 77, 'ɗ': 93, 'ɗ̥': 270, 'ɘ': 104, 'ɘ̃': 189, 'ə': 6, 'ə̃': 294, 'ɚ': 184, 'ɛ': 293, 'ɛ̃': 98, 'ɜ': 92, 'ɜ̃': 73, 'ɝ': 196, 'ɞ': 210, 'ɞ̃': 54, 'ɟ': 68, 'ɟ͡ʝ': 86, 'ɠ': 160, 'ɠ̊': 1, 'ɡ': 252, 'ɡǀ': 19, 'ɡǁ': 226, 'ɡǂ': 131, 'ɡǃ': 152, 'ɡʘ': 162, 'ɡ̆': 201, 'ɡ͡b': 185, 'ɡ͡ɣ': 78, 'ɢ': 195, 'ɢǀ': 88, 'ɢǁ': 258, 'ɢǂ': 64, 'ɢǃ': 308, 'ɢʘ': 107, 'ɢ̆': 123, 'ɢ͡ʁ': 111, 'ɣ': 219, 'ɤ': 194, 'ɤ̃': 17, 'ɤ̞': 187, 'ɤ̞̃': 60, 'ɥ': 24, 'ɥ̊': 135, 'ɧ': 246, 'ɨ': 40, 'ɨ̃': 109, 'ɪ': 8, 'ɪ̃': 97, 'ɬ': 121, 'ɬʼ': 3, 'ɭ': 290, 'ɭ˔': 279, 'ɭ̆': 236, 'ɭ̥̆': 240, 'ɮ': 266, 'ɯ': 275, 'ɯ̃': 300, 'ɰ': 208, 'ɱ': 295, 'ɲ': 128, 'ɲ̊': 197, 'ɳ': 63, 'ɳ̊': 144, 'ɴ': 136, 'ɴǀ': 101, 'ɴǁ': 14, 'ɴǂ': 238, 'ɴǃ': 147, 'ɴʘ': 221, 'ɵ': 95, 'ɵ̃': 51, 'ɶ': 145, 'ɶ̃': 146, 'ɸ': 166, 'ɸʼ': 306, 'ɹ': 223, 'ɹ̠˔': 202, 'ɹ̠̊˔': 173, 'ɺ': 76, 'ɺ̥': 181, 'ɻ': 163, 'ɻ˔': 30, 'ɻ̊˔': 42, 'ɽ': 254, 'ɽ̊': 218, 'ɾ': 232, 'ɾ̥': 120, 'ɾ̼': 32, 'ʀ': 52, 'ʀ̥': 148, 'ʁ': 239, 'ʂ': 242, 'ʂʼ': 110, 'ʃ': 253, 'ʃʼ': 230, 'ʄ': 132, 'ʄ̊': 155, 'ʈ': 199, 'ʈʼ': 44, 'ʈ͡ʂ': 190, 'ʈ͡ʂʼ': 67, 'ʉ': 80, 'ʉ̃': 250, 'ʊ': 83, 'ʊ̃': 205, 'ʋ': 106, 'ʌ': 272, 'ʌ̃': 158, 'ʍ': 141, 'ʎ': 276, 'ʎ̆': 84, 'ʎ̝': 262, 'ʎ̝̊': 59, 'ʏ': 23, 'ʏ̃': 237, 'ʐ': 209, 'ʑ': 137, 'ʒ': 171, 'ʔ': 10, 'ʔ̞': 257, 'ʔ͡h': 203, 'ʕ': 38, 'ʙ': 274, 'ʙ̥': 167, 'ʛ': 233, 'ʛ̥': 159, 'ʜ': 298, 'ʝ': 35, 'ʟ': 150, 'ʟ̆': 248, 'ʟ̝': 33, 'ʟ̝̊': 161, 'ʟ̠': 285, 'ʡ': 174, 'ʡʼ': 5, 'ʡ̆': 22, 'ʡ͡ʜ': 56, 'ʡ͡ʢ': 255, 'ʢ': 188, 'ʰ': 206, 'ʲ': 0, 'ʷ': 288, 'ː': 235, '˞': 90, 'ˠ': 122, 'ˡ': 16, 'ˣ': 96, 'ˤ': 15, '̃': 57, '̩': 204, 'β': 269, 'θ': 283, 'θʼ': 26, 'θ̠': 100, 'θ̼': 227, 'χ': 65, 'χʼ': 260, 'ᵐ': 124, 'ᵑ': 297, 'ᶑ': 212, 'ᶑ̊': 268, 'ᶬ': 151, 'ᶮ': 55, 'ᶯ': 207, 'ᶰ': 11, 'ᶿ': 138, 'ẽ': 2, 'ⁿ': 222, 'ⱱ': 134, 'ⱱ̟': 193, 'ꞎ': 278, '<s>': 311, '</s>': 312}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_ID)\n",
    "vocab = tokenizer.get_vocab()\n",
    "# you will see how large the vocab is, we will resize our linear layer later to make it work for our smaller vocab\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens that are in timit but not in the pretrained model set()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_vocab = timit_vocab.difference(set(vocab.keys()) | {' '})\n",
    "print(\"tokens that are in timit but not in the pretrained model\", additional_vocab)\n",
    "tokenizer.add_tokens(list(additional_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(PRE_TRAINED_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to update the feature extractor since it has been pretrained on 16kHz audio which matches the TIMIT dataset.\n",
    "\n",
    "For datasets with different sampling rates, the feature extractor should be updated or the audio resampled (easier).\n",
    "\n",
    "This is also where code to add extra features (such as conditioning on speaker's native language etc.) would be added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparam search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchaudio in /home/arunasri/ML/venv/lib/python3.8/site-packages (2.4.1)\n",
      "Requirement already satisfied: torch==2.4.1 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torchaudio) (2.4.1)\n",
      "Requirement already satisfied: filelock in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1->torchaudio) (12.6.85)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from jinja2->torch==2.4.1->torchaudio) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from sympy->torch==2.4.1->torchaudio) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2 resize finetuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import logging\n",
    "import tempfile\n",
    "import wandb\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoProcessor, \n",
    "    AutoModelForCTC, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "import torchaudio\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import GPUtil\n",
    "import psutil\n",
    "\n",
    "# Constants and logging setup as before\n",
    "SAMPLING_RATE = 16000\n",
    "MAX_AUDIO_LENGTH = 160000\n",
    "MAX_LABEL_LENGTH = 100\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TrialTimeTracker:\n",
    "    def __init__(self, output_file):\n",
    "        self.output_file = output_file\n",
    "        self.current_trial = None\n",
    "        self.start_time = None\n",
    "        \n",
    "    def start_trial(self, config):\n",
    "        self.current_trial = {\n",
    "            'config': config,\n",
    "            'start_time': datetime.now().isoformat(),\n",
    "            'gpu_type': self._get_gpu_type(),\n",
    "            'timestamps': []\n",
    "        }\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def log_timestamp(self, step_name):\n",
    "        if self.current_trial:\n",
    "            gpu_metrics = self._get_gpu_metrics()\n",
    "            timestamp = {\n",
    "                'step': step_name,\n",
    "                'elapsed_seconds': time.time() - self.start_time,\n",
    "                **gpu_metrics\n",
    "            }\n",
    "            self.current_trial['timestamps'].append(timestamp)\n",
    "            logger.info(f\"Step {step_name} completed after {timestamp['elapsed_seconds']:.2f}s\")\n",
    "    \n",
    "    def _get_gpu_type(self):\n",
    "        try:\n",
    "            gpu = GPUtil.getGPUs()[0]\n",
    "            return gpu.name\n",
    "        except:\n",
    "            return \"Unknown\"\n",
    "            \n",
    "    def _get_gpu_metrics(self):\n",
    "        try:\n",
    "            gpu = GPUtil.getGPUs()[0]\n",
    "            return {\n",
    "                'gpu_utilization': gpu.load * 100,\n",
    "                'gpu_memory_used': gpu.memoryUsed,\n",
    "                'gpu_memory_total': gpu.memoryTotal\n",
    "            }\n",
    "        except:\n",
    "            return {\n",
    "                'gpu_utilization': None,\n",
    "                'gpu_memory_used': None,\n",
    "                'gpu_memory_total': None\n",
    "            }\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    # Same as before\n",
    "    processor: AutoProcessor\n",
    "    padding: Union[bool, str] = \"longest\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        max_length = min(\n",
    "            max(len(feature[\"input_values\"]) for feature in features),\n",
    "            MAX_AUDIO_LENGTH\n",
    "        )\n",
    "        padded_inputs = []\n",
    "        attention_mask = []\n",
    "        \n",
    "        for feature in features:\n",
    "            input_length = len(feature[\"input_values\"])\n",
    "            padding_length = max_length - input_length\n",
    "            \n",
    "            if isinstance(feature[\"input_values\"], list):\n",
    "                input_values = torch.tensor(feature[\"input_values\"])\n",
    "            else:\n",
    "                input_values = feature[\"input_values\"]\n",
    "            \n",
    "            input_values = input_values.squeeze()\n",
    "            \n",
    "            if padding_length > 0:\n",
    "                padded_input = torch.nn.functional.pad(input_values, (0, padding_length))\n",
    "                attention_mask.append(torch.cat([torch.ones(input_length), torch.zeros(padding_length)]))\n",
    "            else:\n",
    "                padded_input = input_values\n",
    "                attention_mask.append(torch.ones(input_length))\n",
    "            \n",
    "            padded_inputs.append(padded_input)\n",
    "\n",
    "        batch = {\n",
    "            \"input_values\": torch.stack(padded_inputs),\n",
    "            \"attention_mask\": torch.stack(attention_mask)\n",
    "        }\n",
    "\n",
    "        with self.processor.as_target_processor():\n",
    "            label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "def prepare_model_and_processor(pre_trained_id, timit_vocab, base_dir):\n",
    "    \"\"\"Prepare model and processor with vocabulary resizing\"\"\"\n",
    "    logger.info(\"Loading pretrained model and processor...\")\n",
    "    \n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    processor = AutoProcessor.from_pretrained(pre_trained_id)\n",
    "    model = AutoModelForCTC.from_pretrained(pre_trained_id)\n",
    "    \n",
    "    old_vocab = processor.tokenizer.get_vocab()\n",
    "    logger.info(f\"Initial vocabulary size: {len(old_vocab)}\")\n",
    "    \n",
    "    special_tokens = {\n",
    "        \"[PAD]\": 0,\n",
    "        \"<s>\": 1,\n",
    "        \"</s>\": 2,\n",
    "        \"[UNK]\": 3\n",
    "    }\n",
    "    \n",
    "    regular_tokens = sorted(list(timit_vocab - set(['<pad>', '<unk>'])))\n",
    "    vocab = {**special_tokens}\n",
    "    \n",
    "    for idx, token in enumerate(regular_tokens):\n",
    "        vocab[token] = idx + len(special_tokens)\n",
    "    \n",
    "    logger.info(f\"New vocabulary size: {len(vocab)}\")\n",
    "    \n",
    "    vocab_file = os.path.join(base_dir, \"vocab.json\")\n",
    "    with open(vocab_file, 'w') as f:\n",
    "        json.dump(vocab, f, indent=2)\n",
    "    \n",
    "    tokenizer_config = {\n",
    "        \"vocab\": vocab,\n",
    "        \"pad_token\": \"[PAD]\",\n",
    "        \"bos_token\": \"<s>\",\n",
    "        \"eos_token\": \"</s>\",\n",
    "        \"unk_token\": \"[UNK]\",\n",
    "    }\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:\n",
    "        json.dump(vocab, f)\n",
    "        vocab_path = f.name\n",
    "    \n",
    "    new_tokenizer = Wav2Vec2CTCTokenizer(\n",
    "        vocab_path,\n",
    "        **{k: v for k, v in tokenizer_config.items() if k != \"vocab\"}\n",
    "    )\n",
    "    \n",
    "    processor.tokenizer = new_tokenizer\n",
    "    \n",
    "    # Save processor configuration\n",
    "    processor_path = os.path.join(base_dir, \"processor\")\n",
    "    processor.save_pretrained(processor_path)\n",
    "    \n",
    "    # Resize the linear head\n",
    "    old_weights = model.lm_head.weight.data\n",
    "    old_bias = model.lm_head.bias.data\n",
    "    \n",
    "    new_layer = torch.nn.Linear(model.lm_head.in_features, len(processor.tokenizer))\n",
    "    torch.nn.init.xavier_uniform_(new_layer.weight.data, gain=0.1)\n",
    "    new_layer.bias.data.uniform_(-0.1, 0.1)\n",
    "    \n",
    "    transfer_count = 0\n",
    "    for token, new_idx in vocab.items():\n",
    "        if token in old_vocab:\n",
    "            old_idx = old_vocab[token]\n",
    "            new_layer.weight.data[new_idx, :] = old_weights[old_idx, :]\n",
    "            new_layer.bias.data[new_idx] = old_bias[old_idx]\n",
    "            transfer_count += 1\n",
    "            \n",
    "    logger.info(f\"Transferred weights for {transfer_count} tokens\")\n",
    "    \n",
    "    model.lm_head = new_layer\n",
    "    model.config.vocab_size = len(vocab)\n",
    "    model.config.pad_token_id = special_tokens[\"[PAD]\"]\n",
    "    model.config.bos_token_id = special_tokens[\"<s>\"]\n",
    "    model.config.eos_token_id = special_tokens[\"</s>\"]\n",
    "    model.config.unk_token_id = special_tokens[\"[UNK]\"]\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "def prepare_datasets(train_df, processor):\n",
    "    \"\"\"Dataset preparation with optimized batch processing\"\"\"\n",
    "    # Same as before\n",
    "    logger.info(\"Splitting dataset into train and evaluation...\")\n",
    "    \n",
    "    train_data, eval_data = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    def process_data(batch):\n",
    "        audio = preprocess_audio(batch[\"audio\"])\n",
    "        audio = audio.squeeze()\n",
    "        \n",
    "        inputs = processor(\n",
    "            audio, \n",
    "            sampling_rate=SAMPLING_RATE, \n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        input_values = np.squeeze(inputs[\"input_values\"])\n",
    "        \n",
    "        with processor.as_target_processor():\n",
    "            labels = processor(batch[\"ipa\"]).input_ids\n",
    "        \n",
    "        return {\n",
    "            \"input_values\": input_values,\n",
    "            \"labels\": labels,\n",
    "            \"audio\": audio\n",
    "        }\n",
    "    \n",
    "    train_dataset = Dataset.from_pandas(train_data)\n",
    "    eval_dataset = Dataset.from_pandas(eval_data)\n",
    "    \n",
    "    train_dataset = train_dataset.map(\n",
    "        process_data,\n",
    "        batch_size=32,\n",
    "        num_proc=4,\n",
    "        remove_columns=[col for col in train_dataset.column_names if col != \"audio\"]\n",
    "    )\n",
    "    eval_dataset = eval_dataset.map(\n",
    "        process_data,\n",
    "        batch_size=32,\n",
    "        num_proc=4,\n",
    "        remove_columns=[col for col in eval_dataset.column_names if col != \"audio\"]\n",
    "    )\n",
    "    \n",
    "    return train_dataset, eval_dataset\n",
    "\n",
    "def preprocess_audio(audio_input):\n",
    "    \"\"\"Audio preprocessing function\"\"\"\n",
    "    # Same as before\n",
    "    if isinstance(audio_input, str):\n",
    "        waveform, sample_rate = torchaudio.load(audio_input)\n",
    "        audio_numpy = waveform.squeeze().numpy()\n",
    "    elif isinstance(audio_input, (list, np.ndarray, torch.Tensor)):\n",
    "        audio_numpy = np.array(audio_input, dtype=np.float32)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported audio type: {type(audio_input)}\")\n",
    "    \n",
    "    audio_numpy = audio_numpy.astype(np.float32)\n",
    "    if len(audio_numpy.shape) > 1:\n",
    "        audio_numpy = np.mean(audio_numpy, axis=0)\n",
    "    \n",
    "    if np.abs(audio_numpy).max() > 1:\n",
    "        audio_numpy = audio_numpy / np.abs(audio_numpy).max()\n",
    "    \n",
    "    if len(audio_numpy) > MAX_AUDIO_LENGTH:\n",
    "        audio_numpy = audio_numpy[:MAX_AUDIO_LENGTH]\n",
    "    elif len(audio_numpy) < MAX_AUDIO_LENGTH:\n",
    "        padding = np.zeros(MAX_AUDIO_LENGTH - len(audio_numpy), dtype=np.float32)\n",
    "        audio_numpy = np.concatenate([audio_numpy, padding])\n",
    "    \n",
    "    return audio_numpy\n",
    "\n",
    "def run_finetuning(train_df, pre_trained_id, timit_vocab, output_dir):\n",
    "    \"\"\"Run finetuning with optimal hyperparameters\"\"\"\n",
    "    tracker = TrialTimeTracker(os.path.join(output_dir, \"finetuning_log.jsonl\"))\n",
    "    \n",
    "    config = {\n",
    "        \"learning_rate\": 9.999999999999999e-05,\n",
    "        \"per_device_train_batch_size\": 4,\n",
    "        \"per_device_eval_batch_size\": 4,\n",
    "        \"gradient_accumulation_steps\": 2,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "        \"adam_beta1\": 0.9,\n",
    "        \"adam_beta2\": 0.975,\n",
    "        \"adam_epsilon\": 1e-08\n",
    "    }\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"xlsr-phoneme-finetuning-final\",\n",
    "        name=\"finetuning_optimal_params\",\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    tracker.start_trial(config)\n",
    "    \n",
    "    tracker.log_timestamp(\"model_init_start\")\n",
    "    model, processor = prepare_model_and_processor(pre_trained_id, timit_vocab, output_dir)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    tracker.log_timestamp(\"model_init_complete\")\n",
    "    \n",
    "    tracker.log_timestamp(\"dataset_prep_start\")\n",
    "    train_dataset, eval_dataset = prepare_datasets(train_df, processor)\n",
    "    tracker.log_timestamp(\"dataset_prep_complete\")\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        **config,\n",
    "        num_train_epochs=20,\n",
    "        logging_steps=50,\n",
    "        save_steps=100,\n",
    "        eval_steps=100,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        save_strategy=\"steps\",\n",
    "        save_total_limit=2,\n",
    "        fp16=True,\n",
    "        dataloader_num_workers=16,\n",
    "        gradient_checkpointing=True,\n",
    "        dataloader_prefetch_factor=4,\n",
    "        ddp_find_unused_parameters=False\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=DataCollatorCTCWithPadding(processor=processor, padding=\"longest\"),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "    )\n",
    "    \n",
    "    tracker.log_timestamp(\"training_start\")\n",
    "    train_result = trainer.train()\n",
    "    tracker.log_timestamp(\"training_complete\")\n",
    "    \n",
    "    eval_result = trainer.evaluate()\n",
    "    tracker.log_timestamp(\"evaluation_complete\")\n",
    "    \n",
    "    logger.info(\"Saving final model and processor...\")\n",
    "    trainer.save_model(os.path.join(output_dir, \"final_model\"))\n",
    "    processor.save_pretrained(os.path.join(output_dir, \"final_processor\"))\n",
    "    \n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_finetuning(\n",
    "        train_df=train_df,  # Your training DataFrame\n",
    "        pre_trained_id=PRE_TRAINED_ID,  # Your pretrained model ID\n",
    "        timit_vocab=timit_vocab,  # Your TIMIT vocabulary set\n",
    "        output_dir=\"./results/finetune_resize_final\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model on Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCTC, Wav2Vec2Processor\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Log in to Hugging Face (if not already logged in)\n",
    "# login(\"your-huggingface-api-token\")\n",
    "\n",
    "# Load the model and processor from your local directory\n",
    "model = AutoModelForCTC.from_pretrained(\"/home/arunasrivastava/ML/notebooks/results-b0\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"/home/arunasrivastava/ML/notebooks/results-b0\")\n",
    "\n",
    "# Specify the directory where you want to save the model\n",
    "save_directory = \"./xlsr-timit-b0\"\n",
    "\n",
    "# Save and upload to Hugging Face Model Hub under an organization\n",
    "model.save_pretrained(save_directory, push_to_hub=True, repo_id=\"KoelLabs/xlsr-timit-b0\")\n",
    "processor.save_pretrained(save_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions to Upload Model to Hugging Face\n",
    "\n",
    "1. **Login to Hugging Face**  \n",
    "   Run the following command to log in to your Hugging Face account:\n",
    "   ```bash\n",
    "   huggingface-cli login\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Create a Repository**  \n",
    "   Run the following command to log in to your Hugging Face account:\n",
    "   ```bash\n",
    "   huggingface-cli repo create your-model-name --organization your-org-name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Git clone**  \n",
    "   Run the following command to clone your repo:\n",
    "   ```bash \n",
    "   git clone https://huggingface.co/your-org-name/your-model-name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Upload Model**  (if you did not make it on Hugging Face already)\n",
    "   Run the following command to clone your repo:\n",
    "   ```bash \n",
    "   huggingface-cli repo upload --path ./<path here>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Move Other Files**  \n",
    "   Run the following command to copy each file into the repo:\n",
    "   ```bash\n",
    "   cp -r ./<path here> <your-cloned-repo-folder> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Stage & Commit!**\n",
    "    Run the following git commands\n",
    "    ```bash\n",
    "    git add README.md\n",
    "    git commit -m \"Add model card\"\n",
    "    git push\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YAAAAY CONGRATULATIONS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Hyperparam search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install optuna joblib\n",
    "# you can use WADNB sweeps as well, whatever is easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
