{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import zipfile\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "# run as much as possible accelerated by apple silicon, fall back to cpu if not possible\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from scripts.audio import audio_file_to_array\n",
    "from scripts.ipa import timit2ipa\n",
    "from scripts.ipa import filter_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('..')\n",
    "from scripts.eval_tests.panphon_model_eval import panphon_model_eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set espeak library path for macOS\n",
    "if sys.platform == \"darwin\":\n",
    "    from phonemizer.backend.espeak.wrapper import EspeakWrapper\n",
    "\n",
    "    _ESPEAK_LIBRARY = \"/opt/homebrew/Cellar/espeak/1.48.04_1/lib/libespeak.1.1.48.dylib\"\n",
    "    EspeakWrapper.set_library(_ESPEAK_LIBRARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCTC, AutoTokenizer, AutoFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = os.path.join('..', 'models', 'timit-xlsr-finetune-B')\n",
    "PRE_TRAINED_ID = \"ginic/gender_split_70_female_4_wav2vec2-large-xlsr-53-buckeye-ipa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit = zipfile.ZipFile('../.data/TIMIT.zip', 'r')\n",
    "timit_files = timit.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = list(set(map(lambda x: x.split('.')[0], filter(lambda x: x.startswith('data/TRAIN'), timit_files))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_HEADER_SIZE = 44\n",
    "def zipped_wav_to_array(filename):\n",
    "    with timit.open(filename) as wav_file:\n",
    "        return np.frombuffer(wav_file.read(), dtype=np.int16)[WAV_HEADER_SIZE//2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up Dataset Vocab \n",
    "You will see that the initial vocab of most multilingual phoneme transcription models is quite big (300-400 phonemes) we will reduce this in the dataset to improve overall performance for English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>ipa</th>\n",
       "      <th>phoneme_starts</th>\n",
       "      <th>phoneme_ends</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>θɪkɡluʔuzaʊɾʌðətub</td>\n",
       "      <td>[0, 2120, 2960, 4181, 4835, 7124, 7340, 9040, ...</td>\n",
       "      <td>[2120, 2960, 4181, 4835, 7124, 7340, 9040, 119...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>hɪzɑɹtɪsikiʔɑmpliʃməntsɡɛɹintidhimʔintɹiindiʔɛ...</td>\n",
       "      <td>[0, 2360, 3020, 4560, 5560, 6349, 7770, 8540, ...</td>\n",
       "      <td>[2360, 3020, 4560, 5560, 6349, 7770, 8540, 975...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>ʃihædjɹdɑɹksutiŋɡɹisiwɔʃwɔɾɹʔɔljɪɹ</td>\n",
       "      <td>[0, 2040, 3800, 5000, 6767, 9640, 10234, 11000...</td>\n",
       "      <td>[2040, 3800, 5000, 6767, 9640, 10234, 11000, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>əkɔɹɾiŋtimaɪintɹpɹteɪʃnʌvðəpɹɑbləmulaɪnzmʌsbip...</td>\n",
       "      <td>[0, 2266, 2891, 3880, 5400, 6573, 7160, 7392, ...</td>\n",
       "      <td>[2266, 2891, 3880, 5400, 6573, 7160, 7392, 827...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>ðəpɹufðɛʔjuɹsikiŋiznɑɾəveɪləblənbʊks</td>\n",
       "      <td>[0, 2200, 2520, 3160, 4120, 5320, 6066, 7560, ...</td>\n",
       "      <td>[2200, 2520, 3160, 4120, 5320, 6066, 7560, 822...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               audio  \\\n",
       "0  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "1  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "2  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "3  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "4  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "\n",
       "                                                 ipa  \\\n",
       "0                                 θɪkɡluʔuzaʊɾʌðətub   \n",
       "1  hɪzɑɹtɪsikiʔɑmpliʃməntsɡɛɹintidhimʔintɹiindiʔɛ...   \n",
       "2                 ʃihædjɹdɑɹksutiŋɡɹisiwɔʃwɔɾɹʔɔljɪɹ   \n",
       "3  əkɔɹɾiŋtimaɪintɹpɹteɪʃnʌvðəpɹɑbləmulaɪnzmʌsbip...   \n",
       "4               ðəpɹufðɛʔjuɹsikiŋiznɑɾəveɪləblənbʊks   \n",
       "\n",
       "                                      phoneme_starts  \\\n",
       "0  [0, 2120, 2960, 4181, 4835, 7124, 7340, 9040, ...   \n",
       "1  [0, 2360, 3020, 4560, 5560, 6349, 7770, 8540, ...   \n",
       "2  [0, 2040, 3800, 5000, 6767, 9640, 10234, 11000...   \n",
       "3  [0, 2266, 2891, 3880, 5400, 6573, 7160, 7392, ...   \n",
       "4  [0, 2200, 2520, 3160, 4120, 5320, 6066, 7560, ...   \n",
       "\n",
       "                                        phoneme_ends  \n",
       "0  [2120, 2960, 4181, 4835, 7124, 7340, 9040, 119...  \n",
       "1  [2360, 3020, 4560, 5560, 6349, 7770, 8540, 975...  \n",
       "2  [2040, 3800, 5000, 6767, 9640, 10234, 11000, 1...  \n",
       "3  [2266, 2891, 3880, 5400, 6573, 7160, 7392, 827...  \n",
       "4  [2200, 2520, 3160, 4120, 5320, 6066, 7560, 822...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, ensure that the simplify_timit function is available and works\n",
    "def simplify_timit(phoneme):\n",
    "    # Substitute the phoneme based on the dictionary\n",
    "    substitution_dict = {\n",
    "        'ɾ̃': 'ɾ',  # Replace nasalized flap with plain flap\n",
    "        'ŋ̍': 'ŋ',  # Remove syllabic marker from 'ŋ̍'\n",
    "        'ə̥': 'ə',  # Remove voiceless marker from 'ə̥'\n",
    "        'ɝ': 'ɹ',   # Simplify rhotacized schwa to 'ɹ'\n",
    "        'ɚ': 'ɹ',   # Simplify rhotacized schwa to 'ɹ'\n",
    "        'l̩': 'l',   # Remove syllabic marker from 'l̩'\n",
    "        'm̩': 'm',   # Remove syllabic marker from 'm̩'\n",
    "        'n̩': 'n',   # Remove syllabic marker from 'n̩'\n",
    "        '̩': '',     # Remove syllabic marker\n",
    "        'ʉ': 'u',    # Replace high central rounded vowel with high back rounded vowel\n",
    "        'ɨ': 'i',    # Replace high central unrounded vowel with high front unrounded vowel\n",
    "        ' ': '',     # Remove nasalization marker\n",
    "        'ɦ': 'h',    # Replace voiceless glottal fricative with voiceless glottal fricative\n",
    "        # Add other necessary substitutions if needed\n",
    "    }\n",
    "    # Apply the substitution for the phoneme\n",
    "    return substitution_dict.get(phoneme, phoneme)  # Return simplified phoneme or the original if no replacement\n",
    "\n",
    "def remove_stress_mark(text):\n",
    "    \"\"\"\n",
    "    Removes the combining double inverted breve (͡) from text.\n",
    "    \n",
    "    Args:\n",
    "        text: String or iterable containing IPA symbols\n",
    "        \n",
    "    Returns:\n",
    "        Text with stress marks removed\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return text.replace('͡', '')\n",
    "    else:\n",
    "        raise TypeError(\"Input must be string, set, or list\")\n",
    "    \n",
    "# Updated timit_file_to_dict to use the simplified IPA phonemes\n",
    "def timit_file_to_dict(filename):\n",
    "    with timit.open(filename + '.PHN') as phn_file:\n",
    "        timestamped_phonemes = []\n",
    "        for line in phn_file.read().decode('utf-8').split('\\n'):\n",
    "            if line == '':\n",
    "                continue\n",
    "            start, end, phoneme = line.split()\n",
    "            \n",
    "            # Convert to IPA first and then simplify the phoneme\n",
    "            ipa_phonemes = timit2ipa(phoneme, \"eng\")\n",
    "            \n",
    "            # Now filter only the necessary characters (this simplifies the phoneme to the basic form)\n",
    "            cleaned_ipa = filter_chars(simplify_timit(ipa_phonemes), filter_type=\"letters\")\n",
    "            cleaned_ipa = remove_stress_mark(cleaned_ipa)\n",
    "            timestamped_phonemes.append((cleaned_ipa, int(start), int(end)))\n",
    "\n",
    "    return {'timestamped_phonemes': timestamped_phonemes, 'wav_filename': filename + '.WAV'}\n",
    "\n",
    "# Updated files_to_df to create DataFrame with cleaned-up phonemes\n",
    "def files_to_df(files):\n",
    "    records = []\n",
    "    for filename in files:\n",
    "        parsed = timit_file_to_dict(filename)\n",
    "        parsed['audio'] = zipped_wav_to_array(parsed['wav_filename'])\n",
    "        del parsed['wav_filename']\n",
    "        parsed['ipa'] = \"\".join(phoneme for phoneme, _, _ in parsed['timestamped_phonemes'])\n",
    "        parsed['phoneme_starts'] = [start for _, start, _ in parsed['timestamped_phonemes']]\n",
    "        parsed['phoneme_ends'] = [end for _, _, end in parsed['timestamped_phonemes']]\n",
    "        del parsed['timestamped_phonemes']\n",
    "        records.append(parsed)\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Call files_to_df for your test files\n",
    "train_df = files_to_df(training_files)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Up Audio\n",
    "We will just crop out the start signal :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_audio(row):\n",
    "    # Access the phoneme_starts column and get the last value of the list\n",
    "    end = row['phoneme_starts'][-1]\n",
    "    # Access the phoneme_ends column and get the first value of the list\n",
    "    start = row['phoneme_ends'][0]\n",
    "    # Crop the audio from start to end\n",
    "    # note that start and end are in samples, not seconds\n",
    "    cropped_audio = row['audio'][start:end]\n",
    "    \n",
    "    return cropped_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# graph one audio to see start signal that we will remove\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_df['audio'][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>ipa</th>\n",
       "      <th>phoneme_starts</th>\n",
       "      <th>phoneme_ends</th>\n",
       "      <th>cropped_audio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>θɪkɡluʔuzaʊɾʌðətub</td>\n",
       "      <td>[0, 2120, 2960, 4181, 4835, 7124, 7340, 9040, ...</td>\n",
       "      <td>[2120, 2960, 4181, 4835, 7124, 7340, 9040, 119...</td>\n",
       "      <td>[0, 3, 2, 2, 2, 1, -2, 0, -2, 2, -1, 2, 1, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>hɪzɑɹtɪsikiʔɑmpliʃməntsɡɛɹintidhimʔintɹiindiʔɛ...</td>\n",
       "      <td>[0, 2360, 3020, 4560, 5560, 6349, 7770, 8540, ...</td>\n",
       "      <td>[2360, 3020, 4560, 5560, 6349, 7770, 8540, 975...</td>\n",
       "      <td>[3, 6, 4, 5, 5, 5, 2, 5, 0, -1, -1, 3, 0, 7, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>ʃihædjɹdɑɹksutiŋɡɹisiwɔʃwɔɾɹʔɔljɪɹ</td>\n",
       "      <td>[0, 2040, 3800, 5000, 6767, 9640, 10234, 11000...</td>\n",
       "      <td>[2040, 3800, 5000, 6767, 9640, 10234, 11000, 1...</td>\n",
       "      <td>[0, 4, 0, 2, 3, 1, 2, 0, -1, 1, 1, 2, 1, 1, 2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>əkɔɹɾiŋtimaɪintɹpɹteɪʃnʌvðəpɹɑbləmulaɪnzmʌsbip...</td>\n",
       "      <td>[0, 2266, 2891, 3880, 5400, 6573, 7160, 7392, ...</td>\n",
       "      <td>[2266, 2891, 3880, 5400, 6573, 7160, 7392, 827...</td>\n",
       "      <td>[-4, 0, 0, 0, 8, 11, 5, 4, 0, -3, -5, 2, 2, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[25971, 30303, 29285, 26995, 28271, 11552, 131...</td>\n",
       "      <td>ðəpɹufðɛʔjuɹsikiŋiznɑɾəveɪləblənbʊks</td>\n",
       "      <td>[0, 2200, 2520, 3160, 4120, 5320, 6066, 7560, ...</td>\n",
       "      <td>[2200, 2520, 3160, 4120, 5320, 6066, 7560, 822...</td>\n",
       "      <td>[0, 0, 3, 0, 0, 3, 2, 2, -1, 1, 3, 4, 1, -1, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               audio  \\\n",
       "0  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "1  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "2  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "3  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "4  [25971, 30303, 29285, 26995, 28271, 11552, 131...   \n",
       "\n",
       "                                                 ipa  \\\n",
       "0                                 θɪkɡluʔuzaʊɾʌðətub   \n",
       "1  hɪzɑɹtɪsikiʔɑmpliʃməntsɡɛɹintidhimʔintɹiindiʔɛ...   \n",
       "2                 ʃihædjɹdɑɹksutiŋɡɹisiwɔʃwɔɾɹʔɔljɪɹ   \n",
       "3  əkɔɹɾiŋtimaɪintɹpɹteɪʃnʌvðəpɹɑbləmulaɪnzmʌsbip...   \n",
       "4               ðəpɹufðɛʔjuɹsikiŋiznɑɾəveɪləblənbʊks   \n",
       "\n",
       "                                      phoneme_starts  \\\n",
       "0  [0, 2120, 2960, 4181, 4835, 7124, 7340, 9040, ...   \n",
       "1  [0, 2360, 3020, 4560, 5560, 6349, 7770, 8540, ...   \n",
       "2  [0, 2040, 3800, 5000, 6767, 9640, 10234, 11000...   \n",
       "3  [0, 2266, 2891, 3880, 5400, 6573, 7160, 7392, ...   \n",
       "4  [0, 2200, 2520, 3160, 4120, 5320, 6066, 7560, ...   \n",
       "\n",
       "                                        phoneme_ends  \\\n",
       "0  [2120, 2960, 4181, 4835, 7124, 7340, 9040, 119...   \n",
       "1  [2360, 3020, 4560, 5560, 6349, 7770, 8540, 975...   \n",
       "2  [2040, 3800, 5000, 6767, 9640, 10234, 11000, 1...   \n",
       "3  [2266, 2891, 3880, 5400, 6573, 7160, 7392, 827...   \n",
       "4  [2200, 2520, 3160, 4120, 5320, 6066, 7560, 822...   \n",
       "\n",
       "                                       cropped_audio  \n",
       "0  [0, 3, 2, 2, 2, 1, -2, 0, -2, 2, -1, 2, 1, 4, ...  \n",
       "1  [3, 6, 4, 5, 5, 5, 2, 5, 0, -1, -1, 3, 0, 7, -...  \n",
       "2  [0, 4, 0, 2, 3, 1, 2, 0, -1, 1, 1, 2, 1, 1, 2,...  \n",
       "3  [-4, 0, 0, 0, 8, 11, 5, 4, 0, -3, -5, 2, 2, 3,...  \n",
       "4  [0, 0, 3, 0, 0, 3, 2, 2, -1, 1, 3, 4, 1, -1, 0...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the process_row function to each row in the DataFrame\n",
    "train_df['cropped_audio'] = train_df.apply(crop_audio, axis=1) \n",
    "train_df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb94dd44700>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize again, much better!\n",
    "plt.plot(train_df['cropped_audio'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ipa</th>\n",
       "      <th>audio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>θɪkɡluʔuzaʊɾʌðətub</td>\n",
       "      <td>[0, 3, 2, 2, 2, 1, -2, 0, -2, 2, -1, 2, 1, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hɪzɑɹtɪsikiʔɑmpliʃməntsɡɛɹintidhimʔintɹiindiʔɛ...</td>\n",
       "      <td>[3, 6, 4, 5, 5, 5, 2, 5, 0, -1, -1, 3, 0, 7, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ʃihædjɹdɑɹksutiŋɡɹisiwɔʃwɔɾɹʔɔljɪɹ</td>\n",
       "      <td>[0, 4, 0, 2, 3, 1, 2, 0, -1, 1, 1, 2, 1, 1, 2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>əkɔɹɾiŋtimaɪintɹpɹteɪʃnʌvðəpɹɑbləmulaɪnzmʌsbip...</td>\n",
       "      <td>[-4, 0, 0, 0, 8, 11, 5, 4, 0, -3, -5, 2, 2, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ðəpɹufðɛʔjuɹsikiŋiznɑɾəveɪləblənbʊks</td>\n",
       "      <td>[0, 0, 3, 0, 0, 3, 2, 2, -1, 1, 3, 4, 1, -1, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 ipa  \\\n",
       "0                                 θɪkɡluʔuzaʊɾʌðətub   \n",
       "1  hɪzɑɹtɪsikiʔɑmpliʃməntsɡɛɹintidhimʔintɹiindiʔɛ...   \n",
       "2                 ʃihædjɹdɑɹksutiŋɡɹisiwɔʃwɔɾɹʔɔljɪɹ   \n",
       "3  əkɔɹɾiŋtimaɪintɹpɹteɪʃnʌvðəpɹɑbləmulaɪnzmʌsbip...   \n",
       "4               ðəpɹufðɛʔjuɹsikiŋiznɑɾəveɪləblənbʊks   \n",
       "\n",
       "                                               audio  \n",
       "0  [0, 3, 2, 2, 2, 1, -2, 0, -2, 2, -1, 2, 1, 4, ...  \n",
       "1  [3, 6, 4, 5, 5, 5, 2, 5, 0, -1, -1, 3, 0, 7, -...  \n",
       "2  [0, 4, 0, 2, 3, 1, 2, 0, -1, 1, 1, 2, 1, 1, 2,...  \n",
       "3  [-4, 0, 0, 0, 8, 11, 5, 4, 0, -3, -5, 2, 2, 3,...  \n",
       "4  [0, 0, 3, 0, 0, 3, 2, 2, -1, 1, 3, 4, 1, -1, 0...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_df comprised of just cropped_audio and phonemes\n",
    "train_df = train_df.drop(columns=['phoneme_starts', 'phoneme_ends', 'audio'])\n",
    "# rename cropped audio to audio\n",
    "train_df = train_df.rename(columns={'cropped_audio': 'audio'})\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend Phoneme Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'u', 'ɡ', 'ɔ', 'm', 'ɪ', 'ɛ', 'h', 'ð', 'ʃ', 'w', 'l', 'ɾ', 'k', 'θ', 'b', 'o', 'ŋ', 'j', 'f', 'v', 't', 'd', 'n', 'ʌ', 'ə', 'ʔ', 'e', 'ʒ', 'ʊ', 'p', 'æ', 'ɑ', 'ɹ', 'i', 's', 'z', 'a'}\n"
     ]
    }
   ],
   "source": [
    "timit_vocab = set(\"\".join(train_df['ipa']))\n",
    "print(timit_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 267, 'E': 45, 'H': 72, 'I': 20, 'O': 305, 'U': 292, '[PAD]': 310, '[UNK]': 309, '_': 182, 'a': 53, 'ã': 125, 'b': 113, 'b̪': 177, 'b̪͡v': 289, 'b͡ꞵ': 287, 'c': 25, 'cʼ': 229, 'c͡ç': 66, 'd': 176, 'd̼': 118, 'd͡z': 263, 'd͡ð': 244, 'd͡ɮ': 18, 'd͡ʑ': 94, 'd͡ʒ': 280, 'e': 175, 'ẽ': 211, 'e̞': 114, 'ẽ̞': 265, 'f': 139, 'fʼ': 249, 'h': 179, 'i': 245, 'j': 215, 'k': 282, 'kxʼ': 281, 'kǀ': 41, 'kǁ': 61, 'kǂ': 21, 'kǃ': 70, 'kʘ': 79, 'kʼ': 39, 'k̚': 89, 'k͡p': 48, 'k͡x': 198, 'l': 303, 'm': 75, 'm̥': 172, 'n': 102, 'n̥': 28, 'n̼': 169, 'o': 117, 'õ': 119, 'o̞': 62, 'õ̞': 234, 'p': 259, 'pʼ': 286, 'p̚': 149, 'p̪': 273, 'p̪͡f': 105, 'p͡f': 225, 'p͡ɸ': 103, 'q': 130, 'qǀ': 302, 'qǁ': 126, 'qǂ': 299, 'qǃ': 37, 'qʘ': 261, 'qʼ': 58, 'q͡ʡ': 127, 'q͡χʼ': 291, 'q͡ꭓ': 4, 'r': 82, 'r̥': 34, 's': 247, 'sʼ': 200, 't': 307, 'tʼ': 183, 't̚': 241, 't̪͡θʼ': 216, 't̼': 87, 't͡s': 156, 't͡sʼ': 43, 't͡ɕ': 264, 't͡ɬ': 170, 't͡ɬʼ': 164, 't͡ʃ': 91, 't͡ʃʼ': 108, 't͡θ': 140, 'u': 154, 'ũ': 81, 'v': 243, 'w': 143, 'x': 133, 'xʼ': 129, 'y': 214, 'ỹ': 277, 'z': 271, '{': 251, '}': 74, 'ã': 99, 'ä': 165, 'ä̃': 115, 'æ': 191, 'æ̃': 217, 'ç': 71, 'ð': 69, 'ð̠': 192, 'ð̼': 9, 'õ': 157, 'ø': 47, 'ø̃': 46, 'ø̞': 112, 'ø̞̃': 12, 'ħ': 224, 'ĩ': 180, 'ŋ': 213, 'ŋǀ': 231, 'ŋǁ': 304, 'ŋǂ': 301, 'ŋǃ': 284, 'ŋʘ': 168, 'ŋ̊': 27, 'ŋ͡m': 296, 'œ': 29, 'œ̃': 153, 'ũ': 31, 'ɐ': 142, 'ɐ̃': 7, 'ɑ': 49, 'ɑ̃': 50, 'ɒ': 116, 'ɒ̃': 85, 'ɓ': 178, 'ɓ̥': 13, 'ɔ': 228, 'ɔ̃': 186, 'ɕ': 220, 'ɕʼ': 36, 'ɖ': 256, 'ɖ͡ʐ': 77, 'ɗ': 93, 'ɗ̥': 270, 'ɘ': 104, 'ɘ̃': 189, 'ə': 6, 'ə̃': 294, 'ɚ': 184, 'ɛ': 293, 'ɛ̃': 98, 'ɜ': 92, 'ɜ̃': 73, 'ɝ': 196, 'ɞ': 210, 'ɞ̃': 54, 'ɟ': 68, 'ɟ͡ʝ': 86, 'ɠ': 160, 'ɠ̊': 1, 'ɡ': 252, 'ɡǀ': 19, 'ɡǁ': 226, 'ɡǂ': 131, 'ɡǃ': 152, 'ɡʘ': 162, 'ɡ̆': 201, 'ɡ͡b': 185, 'ɡ͡ɣ': 78, 'ɢ': 195, 'ɢǀ': 88, 'ɢǁ': 258, 'ɢǂ': 64, 'ɢǃ': 308, 'ɢʘ': 107, 'ɢ̆': 123, 'ɢ͡ʁ': 111, 'ɣ': 219, 'ɤ': 194, 'ɤ̃': 17, 'ɤ̞': 187, 'ɤ̞̃': 60, 'ɥ': 24, 'ɥ̊': 135, 'ɧ': 246, 'ɨ': 40, 'ɨ̃': 109, 'ɪ': 8, 'ɪ̃': 97, 'ɬ': 121, 'ɬʼ': 3, 'ɭ': 290, 'ɭ˔': 279, 'ɭ̆': 236, 'ɭ̥̆': 240, 'ɮ': 266, 'ɯ': 275, 'ɯ̃': 300, 'ɰ': 208, 'ɱ': 295, 'ɲ': 128, 'ɲ̊': 197, 'ɳ': 63, 'ɳ̊': 144, 'ɴ': 136, 'ɴǀ': 101, 'ɴǁ': 14, 'ɴǂ': 238, 'ɴǃ': 147, 'ɴʘ': 221, 'ɵ': 95, 'ɵ̃': 51, 'ɶ': 145, 'ɶ̃': 146, 'ɸ': 166, 'ɸʼ': 306, 'ɹ': 223, 'ɹ̠˔': 202, 'ɹ̠̊˔': 173, 'ɺ': 76, 'ɺ̥': 181, 'ɻ': 163, 'ɻ˔': 30, 'ɻ̊˔': 42, 'ɽ': 254, 'ɽ̊': 218, 'ɾ': 232, 'ɾ̥': 120, 'ɾ̼': 32, 'ʀ': 52, 'ʀ̥': 148, 'ʁ': 239, 'ʂ': 242, 'ʂʼ': 110, 'ʃ': 253, 'ʃʼ': 230, 'ʄ': 132, 'ʄ̊': 155, 'ʈ': 199, 'ʈʼ': 44, 'ʈ͡ʂ': 190, 'ʈ͡ʂʼ': 67, 'ʉ': 80, 'ʉ̃': 250, 'ʊ': 83, 'ʊ̃': 205, 'ʋ': 106, 'ʌ': 272, 'ʌ̃': 158, 'ʍ': 141, 'ʎ': 276, 'ʎ̆': 84, 'ʎ̝': 262, 'ʎ̝̊': 59, 'ʏ': 23, 'ʏ̃': 237, 'ʐ': 209, 'ʑ': 137, 'ʒ': 171, 'ʔ': 10, 'ʔ̞': 257, 'ʔ͡h': 203, 'ʕ': 38, 'ʙ': 274, 'ʙ̥': 167, 'ʛ': 233, 'ʛ̥': 159, 'ʜ': 298, 'ʝ': 35, 'ʟ': 150, 'ʟ̆': 248, 'ʟ̝': 33, 'ʟ̝̊': 161, 'ʟ̠': 285, 'ʡ': 174, 'ʡʼ': 5, 'ʡ̆': 22, 'ʡ͡ʜ': 56, 'ʡ͡ʢ': 255, 'ʢ': 188, 'ʰ': 206, 'ʲ': 0, 'ʷ': 288, 'ː': 235, '˞': 90, 'ˠ': 122, 'ˡ': 16, 'ˣ': 96, 'ˤ': 15, '̃': 57, '̩': 204, 'β': 269, 'θ': 283, 'θʼ': 26, 'θ̠': 100, 'θ̼': 227, 'χ': 65, 'χʼ': 260, 'ᵐ': 124, 'ᵑ': 297, 'ᶑ': 212, 'ᶑ̊': 268, 'ᶬ': 151, 'ᶮ': 55, 'ᶯ': 207, 'ᶰ': 11, 'ᶿ': 138, 'ẽ': 2, 'ⁿ': 222, 'ⱱ': 134, 'ⱱ̟': 193, 'ꞎ': 278, '<s>': 311, '</s>': 312}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_ID)\n",
    "vocab = tokenizer.get_vocab()\n",
    "# you will see how large the vocab is, we will resize our linear layer later to make it work for our smaller vocab\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens that are in timit but not in the pretrained model set()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_vocab = timit_vocab.difference(set(vocab.keys()) | {' '})\n",
    "print(\"tokens that are in timit but not in the pretrained model\", additional_vocab)\n",
    "tokenizer.add_tokens(list(additional_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(PRE_TRAINED_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to update the feature extractor since it has been pretrained on 16kHz audio which matches the TIMIT dataset.\n",
    "\n",
    "For datasets with different sampling rates, the feature extractor should be updated or the audio resampled (easier).\n",
    "\n",
    "This is also where code to add extra features (such as conditioning on speaker's native language etc.) would be added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINE-TUNE!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchaudio in /home/arunasri/ML/venv/lib/python3.8/site-packages (2.4.1)\n",
      "Requirement already satisfied: torch==2.4.1 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torchaudio) (2.4.1)\n",
      "Requirement already satisfied: filelock in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from torch==2.4.1->torchaudio) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1->torchaudio) (12.6.85)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from jinja2->torch==2.4.1->torchaudio) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from sympy->torch==2.4.1->torchaudio) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Running experiment with linear layer resizing...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/arunasri/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/arunasri/ML/notebooks/wandb/run-20250116_211809-9rasp04y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aruna-team/xlsr-buckeye-phoneme-prediction-linear-size/runs/9rasp04y' target=\"_blank\">linear_resize</a></strong> to <a href='https://wandb.ai/aruna-team/xlsr-buckeye-phoneme-prediction-linear-size' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aruna-team/xlsr-buckeye-phoneme-prediction-linear-size' target=\"_blank\">https://wandb.ai/aruna-team/xlsr-buckeye-phoneme-prediction-linear-size</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aruna-team/xlsr-buckeye-phoneme-prediction-linear-size/runs/9rasp04y' target=\"_blank\">https://wandb.ai/aruna-team/xlsr-buckeye-phoneme-prediction-linear-size/runs/9rasp04y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading pretrained model and processor...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 462\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m metrics_resize, metrics_no_resize\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 462\u001b[0m     \u001b[43mrun_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimit_vocab\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 448\u001b[0m, in \u001b[0;36mrun_experiments\u001b[0;34m(train_df, timit_vocab)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# Experiment 1: With linear layer resizing\u001b[39;00m\n\u001b[1;32m    447\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning experiment with linear layer resizing...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 448\u001b[0m metrics_resize \u001b[38;5;241m=\u001b[39m \u001b[43mmain_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimit_vocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresize_linear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# Experiment 2: Without linear layer resizing\u001b[39;00m\n\u001b[1;32m    451\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning experiment without linear layer resizing...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[30], line 360\u001b[0m, in \u001b[0;36mmain_experiment\u001b[0;34m(train_df, timit_vocab, resize_linear)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# Choose model preparation function based on experiment\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resize_linear:\n\u001b[0;32m--> 360\u001b[0m     model, processor \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_model_and_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimit_vocab\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Original function\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m     model, processor \u001b[38;5;241m=\u001b[39m prepare_model_and_processor_no_resize(timit_vocab)  \u001b[38;5;66;03m# New function\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[30], line 100\u001b[0m, in \u001b[0;36mprepare_model_and_processor\u001b[0;34m(timit_vocab)\u001b[0m\n\u001b[1;32m     97\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading pretrained model and processor...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(PRE_TRAINED_ID)\n\u001b[0;32m--> 100\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCTC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPRE_TRAINED_ID\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Store the old vocabulary before making any changes\u001b[39;00m\n\u001b[1;32m    103\u001b[0m old_vocab \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mget_vocab()\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/transformers/modeling_utils.py:3809\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3793\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3794\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   3795\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3796\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[1;32m   3797\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3807\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[1;32m   3808\u001b[0m     }\n\u001b[0;32m-> 3809\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[1;32m   3814\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/huggingface_hub/file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m   1203\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1218\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1219\u001b[0m     )\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/huggingface_hub/file_download.py:1367\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1365\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1367\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1377\u001b[0m     _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/huggingface_hub/file_download.py:1884\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1881\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m   1882\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m-> 1884\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1891\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1893\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1894\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/huggingface_hub/file_download.py:539\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    537\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    541\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/urllib3/response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[0;32m--> 628\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    631\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/urllib3/response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    564\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/urllib3/response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/http/client.py:455\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 455\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/http/client.py:499\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    494\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 499\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tempfile\n",
    "from transformers import AutoProcessor, AutoModelForCTC, Trainer, TrainingArguments, Wav2Vec2CTCTokenizer\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union\n",
    "import logging\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchaudio\n",
    "import json\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "\n",
    "#B1- LESS TRAINING EPOCHS\n",
    "#B2- LENGTH PENALTY \n",
    "#B3- MORE TRAINING EPOCHS\n",
    "\n",
    "NEW_MODEL_ID = \"./notebooks/results-b0\"\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Constants\n",
    "SAMPLING_RATE = 16000\n",
    "MAX_AUDIO_LENGTH = 160000  # 10 seconds at 16kHz\n",
    "MAX_LABEL_LENGTH = 100\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    \"\"\"\n",
    "    processor: AutoProcessor\n",
    "    padding: Union[bool, str] = \"longest\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Add max length check\n",
    "        max_length = min(\n",
    "            max(len(feature[\"input_values\"]) for feature in features),\n",
    "            MAX_AUDIO_LENGTH\n",
    "        )\n",
    "        # Pad input_values\n",
    "        padded_inputs = []\n",
    "        attention_mask = []\n",
    "        for feature in features:\n",
    "            input_length = len(feature[\"input_values\"])\n",
    "            padding_length = max_length - input_length\n",
    "            \n",
    "            # Convert to tensor and ensure correct shape\n",
    "            if isinstance(feature[\"input_values\"], list):\n",
    "                input_values = torch.tensor(feature[\"input_values\"])\n",
    "            else:\n",
    "                input_values = feature[\"input_values\"]\n",
    "            \n",
    "            # Remove any extra dimensions and ensure it's 1D\n",
    "            input_values = input_values.squeeze()\n",
    "            \n",
    "            # Pad with zeros\n",
    "            if padding_length > 0:\n",
    "                padded_input = torch.nn.functional.pad(input_values, (0, padding_length))\n",
    "                attention_mask.append(torch.cat([torch.ones(input_length), torch.zeros(padding_length)]))\n",
    "            else:\n",
    "                padded_input = input_values\n",
    "                attention_mask.append(torch.ones(input_length))\n",
    "            \n",
    "            padded_inputs.append(padded_input)\n",
    "\n",
    "        # Stack all padded inputs\n",
    "        batch = {\n",
    "            \"input_values\": torch.stack(padded_inputs),\n",
    "            \"attention_mask\": torch.stack(attention_mask)\n",
    "        }\n",
    "\n",
    "        # Process labels\n",
    "        with self.processor.as_target_processor():\n",
    "            label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # Replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "def prepare_model_and_processor(timit_vocab):\n",
    "    \"\"\"\n",
    "    Original function that resizes the linear layer to match TIMIT vocab.\n",
    "    \"\"\"\n",
    "    logger.info(\"Loading pretrained model and processor...\")\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(PRE_TRAINED_ID)\n",
    "    model = AutoModelForCTC.from_pretrained(PRE_TRAINED_ID)\n",
    "    \n",
    "    # Store the old vocabulary before making any changes\n",
    "    old_vocab = processor.tokenizer.get_vocab()\n",
    "    logger.info(f\"Initial vocabulary size: {len(old_vocab)}\")\n",
    "    \n",
    "    # Define fixed special token IDs - pad token first for CTC\n",
    "    special_tokens = {\n",
    "        \"<pad>\": 0,  # Changed to 0 for CTC blank token\n",
    "        \"<s>\": 1,    # Shifted up\n",
    "        \"</s>\": 2,   # Shifted up\n",
    "        \"<unk>\": 3   # Shifted up\n",
    "    }\n",
    "    \n",
    "    # Get regular tokens (excluding special tokens)\n",
    "    regular_tokens = sorted(list(timit_vocab - set(special_tokens.keys())))\n",
    "    \n",
    "    # Create combined vocabulary with fixed IDs\n",
    "    vocab = {**special_tokens}  # Start with special tokens\n",
    "    \n",
    "    # Add regular tokens after special tokens\n",
    "    for idx, token in enumerate(regular_tokens):\n",
    "        vocab[token] = idx + len(special_tokens)\n",
    "    \n",
    "    # Create tokenizer config\n",
    "    tokenizer_config = {\n",
    "        \"vocab\": vocab,\n",
    "        \"pad_token\": \"<pad>\",\n",
    "        \"bos_token\": \"<s>\",\n",
    "        \"eos_token\": \"</s>\",\n",
    "        \"unk_token\": \"<unk>\",\n",
    "    }\n",
    "    \n",
    "    # Save vocab to temporary file\n",
    "    with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:\n",
    "        json.dump(vocab, f)\n",
    "        vocab_path = f.name\n",
    "    \n",
    "    # Create new tokenizer\n",
    "    new_tokenizer = Wav2Vec2CTCTokenizer(\n",
    "        vocab_path,\n",
    "        **{k: v for k, v in tokenizer_config.items() if k != \"vocab\"}\n",
    "    )\n",
    "    \n",
    "    # Update processor with new tokenizer\n",
    "    processor.tokenizer = new_tokenizer\n",
    "    os.remove(vocab_path)\n",
    "    \n",
    "    # Handle model weights\n",
    "    old_weights = model.lm_head.weight.data\n",
    "    old_bias = model.lm_head.bias.data\n",
    "    \n",
    "    # Create new layer with smaller size\n",
    "    new_layer = torch.nn.Linear(model.lm_head.in_features, len(processor.tokenizer))\n",
    "    new_layer.weight.data.zero_()\n",
    "    new_layer.bias.data.zero_()\n",
    "    \n",
    "    # Transfer weights and bias using the stored old_vocab\n",
    "    for token, new_idx in new_tokenizer.get_vocab().items():\n",
    "        if token in old_vocab:\n",
    "            old_idx = old_vocab[token]\n",
    "            new_layer.weight.data[new_idx, :] = old_weights[old_idx, :]\n",
    "            new_layer.bias.data[new_idx] = old_bias[old_idx]\n",
    "    \n",
    "    model.lm_head = new_layer\n",
    "    model.config.vocab_size = len(processor.tokenizer)\n",
    "    model.config.pad_token_id = 0\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "def prepare_model_and_processor_no_resize(timit_vocab):\n",
    "    \"\"\"\n",
    "    New function that keeps the original linear layer size.\n",
    "    \"\"\"\n",
    "    logger.info(\"Loading pretrained model and processor (no resize version)...\")\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(PRE_TRAINED_ID)\n",
    "    model = AutoModelForCTC.from_pretrained(PRE_TRAINED_ID)\n",
    "    \n",
    "    # Store the old vocabulary\n",
    "    old_vocab = processor.tokenizer.get_vocab()\n",
    "    logger.info(f\"Initial vocabulary size: {len(old_vocab)}\")\n",
    "    original_vocab_size = len(old_vocab)\n",
    "    \n",
    "    # Define fixed special token IDs\n",
    "    special_tokens = {\n",
    "        \"<pad>\": 0,\n",
    "        \"<s>\": 1,\n",
    "        \"</s>\": 2,\n",
    "        \"<unk>\": 3\n",
    "    }\n",
    "    \n",
    "    # Create new vocabulary mapping while keeping original size\n",
    "    vocab = {**special_tokens}\n",
    "    \n",
    "    # Add TIMIT tokens after special tokens\n",
    "    regular_tokens = sorted(list(timit_vocab - set(special_tokens.keys())))\n",
    "    for idx, token in enumerate(regular_tokens):\n",
    "        vocab[token] = idx + len(special_tokens)\n",
    "        \n",
    "    # Fill remaining indices with unused tokens to maintain original size\n",
    "    current_size = len(vocab)\n",
    "    for i in range(current_size, original_vocab_size):\n",
    "        vocab[f\"<unused{i}>\"] = i\n",
    "        \n",
    "    # Save vocab to temporary file\n",
    "    with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:\n",
    "        json.dump(vocab, f)\n",
    "        vocab_path = f.name\n",
    "    \n",
    "    # Create new tokenizer\n",
    "    tokenizer_config = {\n",
    "        \"pad_token\": \"<pad>\",\n",
    "        \"bos_token\": \"<s>\",\n",
    "        \"eos_token\": \"</s>\",\n",
    "        \"unk_token\": \"<unk>\",\n",
    "    }\n",
    "    \n",
    "    new_tokenizer = Wav2Vec2CTCTokenizer(\n",
    "        vocab_path,\n",
    "        **tokenizer_config\n",
    "    )\n",
    "    \n",
    "    # Update processor with new tokenizer\n",
    "    processor.tokenizer = new_tokenizer\n",
    "    os.remove(vocab_path)\n",
    "    \n",
    "    # Keep original linear layer - no resizing\n",
    "    logger.info(f\"Linear layer size maintained at: {model.lm_head.out_features}\")\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "def preprocess_audio(audio_input):\n",
    "    \"\"\"\n",
    "    Load and preprocess audio to match expected format.\n",
    "    \"\"\"\n",
    "    # Handle different input types\n",
    "    if isinstance(audio_input, str):  # File path\n",
    "        waveform, sample_rate = torchaudio.load(audio_input)\n",
    "        audio_numpy = waveform.squeeze().numpy()\n",
    "        \n",
    "    elif isinstance(audio_input, list):  # List input\n",
    "        audio_numpy = np.array(audio_input, dtype=np.float32)\n",
    "        \n",
    "    elif isinstance(audio_input, np.ndarray):  # Already numpy array\n",
    "        audio_numpy = audio_input\n",
    "        \n",
    "    elif isinstance(audio_input, torch.Tensor):  # Torch tensor\n",
    "        audio_numpy = audio_input.numpy()\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported audio type: {type(audio_input)}\")\n",
    "    \n",
    "    # Ensure float32 dtype\n",
    "    audio_numpy = audio_numpy.astype(np.float32)\n",
    "    \n",
    "    # Ensure 1D array\n",
    "    if len(audio_numpy.shape) > 1:\n",
    "        audio_numpy = np.mean(audio_numpy, axis=0)\n",
    "    \n",
    "    # Normalize audio\n",
    "    if np.abs(audio_numpy).max() > 1:\n",
    "        audio_numpy = audio_numpy / np.abs(audio_numpy).max()\n",
    "    \n",
    "    # Trim or pad to max length\n",
    "    if len(audio_numpy) > MAX_AUDIO_LENGTH:\n",
    "        audio_numpy = audio_numpy[:MAX_AUDIO_LENGTH]\n",
    "    elif len(audio_numpy) < MAX_AUDIO_LENGTH:\n",
    "        padding = np.zeros(MAX_AUDIO_LENGTH - len(audio_numpy), dtype=np.float32)\n",
    "        audio_numpy = np.concatenate([audio_numpy, padding])\n",
    "    \n",
    "    return audio_numpy\n",
    "\n",
    "def prepare_datasets(train_df, processor):\n",
    "    \"\"\"\n",
    "    Dataset preparation with multiprocessing disabled for stability\n",
    "    \"\"\"\n",
    "    logger.info(\"Splitting dataset into train and evaluation...\")\n",
    "    \n",
    "    train_data, eval_data = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    def process_data(batch):\n",
    "        # Preprocess audio\n",
    "        audio = preprocess_audio(batch[\"audio\"])\n",
    "        audio = audio.squeeze()\n",
    "        \n",
    "        # Process audio without any padding or tensor conversion\n",
    "        inputs = processor(\n",
    "            audio, \n",
    "            sampling_rate=SAMPLING_RATE, \n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        input_values = np.squeeze(inputs[\"input_values\"])\n",
    "        \n",
    "        # Process labels without any padding or tensor conversion\n",
    "        with processor.as_target_processor():\n",
    "            labels = processor(batch[\"ipa\"]).input_ids\n",
    "        \n",
    "        return {\n",
    "            \"input_values\": input_values,\n",
    "            \"labels\": labels,\n",
    "            \"audio\": audio\n",
    "        }\n",
    "    \n",
    "    # Convert DataFrames to Datasets\n",
    "    train_dataset = Dataset.from_pandas(train_data)\n",
    "    eval_dataset = Dataset.from_pandas(eval_data)\n",
    "    \n",
    "    # Apply processing without multiprocessing\n",
    "    train_dataset = train_dataset.map(\n",
    "        process_data,\n",
    "        remove_columns=[col for col in train_dataset.column_names if col != \"audio\"],\n",
    "        batch_size=32,  # Keep batch processing for efficiency\n",
    "        num_proc=None  # Disable multiprocessing\n",
    "    )\n",
    "    eval_dataset = eval_dataset.map(\n",
    "        process_data,\n",
    "        remove_columns=[col for col in eval_dataset.column_names if col != \"audio\"],\n",
    "        batch_size=32,\n",
    "        num_proc=None  # Disable multiprocessing\n",
    "    )\n",
    "    \n",
    "    return train_dataset, eval_dataset\n",
    "def verify_dimensions(batch, logger):\n",
    "    \"\"\"Debug helper to verify tensor dimensions\"\"\"\n",
    "    logger.info(\"Batch dimensions:\")\n",
    "    for key, value in batch.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            logger.info(f\"{key}: {value.shape}\")\n",
    "\n",
    "def verify_token_ids(processor, stage=\"\"):\n",
    "    \"\"\"Simple token verification function\"\"\"\n",
    "    logger.info(f\"\\n=== Token ID Verification [{stage}] ===\")\n",
    "    vocab = processor.tokenizer.get_vocab()\n",
    "    logger.info(f\"Vocabulary size: {len(vocab)}\")\n",
    "    \n",
    "    # Check special tokens\n",
    "    special_tokens = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"]\n",
    "    for token in special_tokens:\n",
    "        token_id = vocab.get(token)\n",
    "        logger.info(f\"Token {token}: ID {token_id}\")\n",
    "    \n",
    "    # Sample a few regular tokens\n",
    "    regular_tokens = [t for t in list(vocab.keys())[:5] if t not in special_tokens]\n",
    "    logger.info(\"Sample regular token IDs:\")\n",
    "    for token in regular_tokens:\n",
    "        logger.info(f\"Token {token}: ID {vocab[token]}\")\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def main_experiment(train_df, timit_vocab, resize_linear=False):\n",
    "    \"\"\"\n",
    "    Modified main function to run experiments with and without linear layer resizing\n",
    "    \"\"\"\n",
    "    experiment_name = \"linear_resize\" if resize_linear else \"linear_no_resize\"\n",
    "    wandb.init(project=\"xlsr-buckeye-phoneme-prediction-linear-size\", name=experiment_name)\n",
    "    \n",
    "    # Choose model preparation function based on experiment\n",
    "    if resize_linear:\n",
    "        model, processor = prepare_model_and_processor(timit_vocab)  # Original function\n",
    "    else:\n",
    "        model, processor = prepare_model_and_processor_no_resize(timit_vocab)  # New function\n",
    "    \n",
    "    # Log linear layer size\n",
    "    logger.info(f\"Linear layer output size: {model.lm_head.out_features}\")\n",
    "    wandb.log({\"linear_layer_size\": model.lm_head.out_features})\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    \n",
    "    train_dataset, eval_dataset = prepare_datasets(train_df, processor)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results-{experiment_name}\",\n",
    "        learning_rate=8e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        gradient_accumulation_steps=4,\n",
    "        max_grad_norm=0.907,\n",
    "        adam_beta1=0.93287,\n",
    "        adam_beta2=0.96748,\n",
    "        warmup_ratio=0.08253,\n",
    "        lr_scheduler_type=\"cosine_with_restarts\",\n",
    "        num_train_epochs=40,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        save_steps=300,\n",
    "        eval_steps=50,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        save_safetensors=True,\n",
    "        save_strategy=\"steps\",\n",
    "        save_total_limit=3,\n",
    "        fp16=True,\n",
    "        dataloader_num_workers=4,\n",
    "        dataloader_pin_memory=True,\n",
    "        gradient_checkpointing=True,\n",
    "    )\n",
    "    \n",
    "    data_collator = DataCollatorCTCWithPadding(\n",
    "        processor=processor,\n",
    "        padding=\"longest\"\n",
    "    )\n",
    "    \n",
    "    # Add early stopping callback\n",
    "    early_stopping = EarlyStoppingCallback(\n",
    "        early_stopping_patience=5,\n",
    "        early_stopping_threshold=0.01\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Train and save metrics\n",
    "    train_result = trainer.train()\n",
    "    metrics = train_result.metrics\n",
    "    \n",
    "    # Log final metrics\n",
    "    wandb.log({\n",
    "        \"final_train_loss\": metrics[\"train_loss\"],\n",
    "        \"vocab_size\": len(processor.tokenizer.get_vocab()),\n",
    "        \"linear_size\": model.lm_head.out_features\n",
    "    })\n",
    "    \n",
    "    # Save the final model and processor\n",
    "    model.save_pretrained(f\"./results-{experiment_name}\")\n",
    "    processor.save_pretrained(f\"./results-{experiment_name}\")\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def run_experiments(train_df, timit_vocab):\n",
    "    \"\"\"\n",
    "    Run both experiments and compare results\n",
    "    \"\"\"\n",
    "    # Experiment 1: With linear layer resizing\n",
    "    logger.info(\"Running experiment with linear layer resizing...\")\n",
    "    metrics_resize = main_experiment(train_df, timit_vocab, resize_linear=True)\n",
    "    \n",
    "    # Experiment 2: Without linear layer resizing\n",
    "    logger.info(\"Running experiment without linear layer resizing...\")\n",
    "    metrics_no_resize = main_experiment(train_df, timit_vocab, resize_linear=False)\n",
    "    \n",
    "    # Compare results\n",
    "    logger.info(\"\\nExperiment Results Comparison:\")\n",
    "    logger.info(f\"With resizing - Final loss: {metrics_resize['train_loss']:.4f}\")\n",
    "    logger.info(f\"Without resizing - Final loss: {metrics_no_resize['train_loss']:.4f}\")\n",
    "    \n",
    "    return metrics_resize, metrics_no_resize\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiments(train_df, timit_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model on Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: '/home/arunasrivastava/ML/notebooks/results-b0'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/arunasrivastava/ML/notebooks/results-b0'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m login\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Log in to Hugging Face (if not already logged in)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# login(\"your-huggingface-api-token\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load the model and processor from your local directory\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCTC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/arunasrivastava/ML/notebooks/results-b0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m processor \u001b[38;5;241m=\u001b[39m Wav2Vec2Processor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/arunasrivastava/ML/notebooks/results-b0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Specify the directory where you want to save the model\u001b[39;00m\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:487\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m    486\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m--> 487\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/transformers/utils/hub.py:469\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    471\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: '/home/arunasrivastava/ML/notebooks/results-b0'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCTC, Wav2Vec2Processor\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Log in to Hugging Face (if not already logged in)\n",
    "# login(\"your-huggingface-api-token\")\n",
    "\n",
    "# Load the model and processor from your local directory\n",
    "model = AutoModelForCTC.from_pretrained(\"/home/arunasrivastava/ML/notebooks/results-b0\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"/home/arunasrivastava/ML/notebooks/results-b0\")\n",
    "\n",
    "# Specify the directory where you want to save the model\n",
    "save_directory = \"./xlsr-timit-b0\"\n",
    "\n",
    "# Save and upload to Hugging Face Model Hub under an organization\n",
    "model.save_pretrained(save_directory, push_to_hub=True, repo_id=\"KoelLabs/xlsr-timit-b0\")\n",
    "processor.save_pretrained(save_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions to Upload Model to Hugging Face\n",
    "\n",
    "1. **Login to Hugging Face**  \n",
    "   Run the following command to log in to your Hugging Face account:\n",
    "   ```bash\n",
    "   huggingface-cli login\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Create a Repository**  \n",
    "   Run the following command to log in to your Hugging Face account:\n",
    "   ```bash\n",
    "   huggingface-cli repo create your-model-name --organization your-org-name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Git clone**  \n",
    "   Run the following command to clone your repo:\n",
    "   ```bash \n",
    "   git clone https://huggingface.co/your-org-name/your-model-name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Upload Model**  (if you did not make it on Hugging Face already)\n",
    "   Run the following command to clone your repo:\n",
    "   ```bash \n",
    "   huggingface-cli repo upload --path ./<path here>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Move Other Files**  \n",
    "   Run the following command to copy each file into the repo:\n",
    "   ```bash\n",
    "   cp -r ./<path here> <your-cloned-repo-folder> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Stage & Commit!**\n",
    "    Run the following git commands\n",
    "    ```bash\n",
    "    git add README.md\n",
    "    git commit -m \"Add model card\"\n",
    "    git push\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YAAAAY CONGRATULATIONS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Hyperparam search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: joblib in /home/arunasri/ML/venv/lib/python3.8/site-packages (1.4.2)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: colorlog in /home/arunasri/ML/venv/lib/python3.8/site-packages (from optuna) (6.8.2)\n",
      "Requirement already satisfied: numpy in /home/arunasri/ML/venv/lib/python3.8/site-packages (from optuna) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from optuna) (24.1)\n",
      "Collecting sqlalchemy>=1.4.2 (from optuna)\n",
      "  Downloading SQLAlchemy-2.0.37-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: tqdm in /home/arunasri/ML/venv/lib/python3.8/site-packages (from optuna) (4.66.5)\n",
      "Requirement already satisfied: PyYAML in /home/arunasri/ML/venv/lib/python3.8/site-packages (from optuna) (5.3)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: importlib-metadata in /home/arunasri/ML/venv/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (8.5.0)\n",
      "Requirement already satisfied: importlib-resources in /home/arunasri/ML/venv/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (6.4.5)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy>=1.4.2->optuna)\n",
      "  Downloading greenlet-3.1.1-cp38-cp38-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from importlib-metadata->alembic>=1.5.0->optuna) (3.20.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/arunasri/ML/venv/lib/python3.8/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
      "Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
      "Downloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
      "Downloading SQLAlchemy-2.0.37-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.1.1-cp38-cp38-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (605 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m606.0/606.0 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: Mako, greenlet, sqlalchemy, alembic, optuna\n",
      "Successfully installed Mako-1.3.8 alembic-1.14.0 greenlet-3.1.1 optuna-4.1.0 sqlalchemy-2.0.37\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install optuna joblib\n",
    "# you can use WADNB sweeps as well, whatever is easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import tempfile\n",
    "from transformers import AutoProcessor, AutoModelForCTC, Trainer, TrainingArguments, TrainerCallback\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union\n",
    "import torchaudio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Constants\n",
    "SAMPLING_RATE = 16000\n",
    "MAX_AUDIO_LENGTH = 160000  # 10 seconds at 16kHz\n",
    "MAX_LABEL_LENGTH = 100\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    \"\"\"\n",
    "    processor: AutoProcessor\n",
    "    padding: Union[bool, str] = \"longest\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Add max length check\n",
    "        max_length = min(\n",
    "            max(len(feature[\"input_values\"]) for feature in features),\n",
    "            MAX_AUDIO_LENGTH\n",
    "        )\n",
    "        # Pad input_values\n",
    "        padded_inputs = []\n",
    "        attention_mask = []\n",
    "        for feature in features:\n",
    "            input_length = len(feature[\"input_values\"])\n",
    "            padding_length = max_length - input_length\n",
    "            \n",
    "            # Convert to tensor and ensure correct shape\n",
    "            if isinstance(feature[\"input_values\"], list):\n",
    "                input_values = torch.tensor(feature[\"input_values\"])\n",
    "            else:\n",
    "                input_values = feature[\"input_values\"]\n",
    "            \n",
    "            # Remove any extra dimensions and ensure it's 1D\n",
    "            input_values = input_values.squeeze()\n",
    "            \n",
    "            # Pad with zeros\n",
    "            if padding_length > 0:\n",
    "                padded_input = torch.nn.functional.pad(input_values, (0, padding_length))\n",
    "                attention_mask.append(torch.cat([torch.ones(input_length), torch.zeros(padding_length)]))\n",
    "            else:\n",
    "                padded_input = input_values\n",
    "                attention_mask.append(torch.ones(input_length))\n",
    "            \n",
    "            padded_inputs.append(padded_input)\n",
    "\n",
    "        # Stack all padded inputs ensuring correct dimensions [batch_size, sequence_length]\n",
    "        batch = {\n",
    "            \"input_values\": torch.stack(padded_inputs),\n",
    "            \"attention_mask\": torch.stack(attention_mask)\n",
    "        }\n",
    "\n",
    "        # Process labels\n",
    "        with self.processor.as_target_processor():\n",
    "            label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # Replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "def preprocess_audio(audio_input):\n",
    "    \"\"\"\n",
    "    Load and preprocess audio to match expected format.\n",
    "    \"\"\"\n",
    "    # Handle different input types\n",
    "    if isinstance(audio_input, str):  # File path\n",
    "        waveform, sample_rate = torchaudio.load(audio_input)\n",
    "        audio_numpy = waveform.squeeze().numpy()\n",
    "        \n",
    "    elif isinstance(audio_input, list):  # List input\n",
    "        audio_numpy = np.array(audio_input, dtype=np.float32)\n",
    "        \n",
    "    elif isinstance(audio_input, np.ndarray):  # Already numpy array\n",
    "        audio_numpy = audio_input\n",
    "        \n",
    "    elif isinstance(audio_input, torch.Tensor):  # Torch tensor\n",
    "        audio_numpy = audio_input.numpy()\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported audio type: {type(audio_input)}\")\n",
    "    \n",
    "    # Ensure float32 dtype\n",
    "    audio_numpy = audio_numpy.astype(np.float32)\n",
    "    \n",
    "    # Ensure 1D array\n",
    "    if len(audio_numpy.shape) > 1:\n",
    "        audio_numpy = np.mean(audio_numpy, axis=0)\n",
    "    \n",
    "    # Normalize audio\n",
    "    if np.abs(audio_numpy).max() > 1:\n",
    "        audio_numpy = audio_numpy / np.abs(audio_numpy).max()\n",
    "    \n",
    "    # Trim or pad to max length\n",
    "    if len(audio_numpy) > MAX_AUDIO_LENGTH:\n",
    "        audio_numpy = audio_numpy[:MAX_AUDIO_LENGTH]\n",
    "    elif len(audio_numpy) < MAX_AUDIO_LENGTH:\n",
    "        padding = np.zeros(MAX_AUDIO_LENGTH - len(audio_numpy), dtype=np.float32)\n",
    "        audio_numpy = np.concatenate([audio_numpy, padding])\n",
    "    \n",
    "    return audio_numpy\n",
    "\n",
    "def prepare_datasets(train_df, processor):\n",
    "    \"\"\"\n",
    "    Dataset preparation compatible with original data collator\n",
    "    \"\"\"\n",
    "    logger.info(\"Splitting dataset into train and evaluation...\")\n",
    "    \n",
    "    train_data, eval_data = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    def process_data(batch):\n",
    "        # Preprocess audio\n",
    "        audio = preprocess_audio(batch[\"audio\"])\n",
    "        audio = audio.squeeze()\n",
    "        \n",
    "        # Process audio without any padding or tensor conversion\n",
    "        inputs = processor(\n",
    "            audio, \n",
    "            sampling_rate=SAMPLING_RATE, \n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        input_values = np.squeeze(inputs[\"input_values\"])\n",
    "        \n",
    "        # Process labels without any padding or tensor conversion\n",
    "        with processor.as_target_processor():\n",
    "            labels = processor(batch[\"ipa\"]).input_ids\n",
    "        \n",
    "        return {\n",
    "            \"input_values\": input_values,\n",
    "            \"labels\": labels,\n",
    "            \"audio\": audio\n",
    "        }\n",
    "    \n",
    "    # Convert DataFrames to Datasets with optimized batch processing\n",
    "    train_dataset = Dataset.from_pandas(train_data)\n",
    "    eval_dataset = Dataset.from_pandas(eval_data)\n",
    "    \n",
    "    # Apply processing with larger batches\n",
    "    train_dataset = train_dataset.map(\n",
    "        process_data,\n",
    "        batch_size=32,\n",
    "        num_proc=4,\n",
    "        remove_columns=[col for col in train_dataset.column_names if col != \"audio\"]\n",
    "    )\n",
    "    eval_dataset = eval_dataset.map(\n",
    "        process_data,\n",
    "        batch_size=32,\n",
    "        num_proc=4,\n",
    "        remove_columns=[col for col in eval_dataset.column_names if col != \"audio\"]\n",
    "    )\n",
    "    \n",
    "    return train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'GPUtil'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpsutil\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mGPUtil\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'GPUtil'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCTC, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "import wandb\n",
    "import torch\n",
    "import time\n",
    "import psutil\n",
    "import GPUtil\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ResourceMonitor:\n",
    "    \"\"\"Monitor GPU and system resources during training\"\"\"\n",
    "    def __init__(self, log_file):\n",
    "        self.start_time = None\n",
    "        self.log_file = log_file\n",
    "        self.metrics = []\n",
    "        \n",
    "    def start(self):\n",
    "        \"\"\"Start monitoring resources\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        self.metrics = []\n",
    "        \n",
    "    def log_metrics(self):\n",
    "        \"\"\"Record current resource usage\"\"\"\n",
    "        if not self.start_time:\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            # Get GPU metrics\n",
    "            gpu = GPUtil.getGPUs()[0]  # Assuming single GPU\n",
    "            gpu_metrics = {\n",
    "                'gpu_utilization': gpu.load * 100,\n",
    "                'gpu_memory_used': gpu.memoryUsed,\n",
    "                'gpu_memory_total': gpu.memoryTotal\n",
    "            }\n",
    "            \n",
    "            # Get CPU and memory metrics\n",
    "            cpu_percent = psutil.cpu_percent()\n",
    "            memory = psutil.virtual_memory()\n",
    "            \n",
    "            # Calculate elapsed time\n",
    "            elapsed_time = time.time() - self.start_time\n",
    "            \n",
    "            metrics = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'elapsed_seconds': elapsed_time,\n",
    "                'cpu_percent': cpu_percent,\n",
    "                'memory_percent': memory.percent,\n",
    "                **gpu_metrics\n",
    "            }\n",
    "            \n",
    "            self.metrics.append(metrics)\n",
    "            \n",
    "            # Log to file\n",
    "            with open(self.log_file, 'a') as f:\n",
    "                json.dump(metrics, f)\n",
    "                f.write('\\n')\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error logging metrics: {str(e)}\")\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Calculate summary statistics of resource usage\"\"\"\n",
    "        if not self.metrics:\n",
    "            return {}\n",
    "            \n",
    "        metrics_dict = {\n",
    "            'total_time': self.metrics[-1]['elapsed_seconds'],\n",
    "            'avg_gpu_utilization': sum(m['gpu_utilization'] for m in self.metrics) / len(self.metrics),\n",
    "            'max_gpu_memory': max(m['gpu_memory_used'] for m in self.metrics),\n",
    "            'avg_cpu_percent': sum(m['cpu_percent'] for m in self.metrics) / len(self.metrics)\n",
    "        }\n",
    "        return metrics_dict\n",
    "\n",
    "def objective(trial, train_df, pre_trained_id, resource_monitor, base_output_dir=\"./results-no-resize\"):\n",
    "    \"\"\"\n",
    "    Optuna objective function for hyperparameter optimization without vocab resizing\n",
    "    \"\"\"\n",
    "    # Generate unique run name\n",
    "    run_name = f\"trial_{trial.number}\"\n",
    "    output_dir = f\"{base_output_dir}/{run_name}\"\n",
    "    \n",
    "    # Initialize wandb\n",
    "    wandb.init(\n",
    "        project=\"xlsr-buckeye-phoneme-prediction-no-resize\",\n",
    "        name=run_name,\n",
    "        config={\"trial_number\": trial.number},\n",
    "        reinit=True\n",
    "    )\n",
    "    \n",
    "    # Sample hyperparameters (keeping same ranges as original)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n",
    "    grad_accum = trial.suggest_categorical(\"gradient_accumulation_steps\", [2, 4, 8])\n",
    "    warmup_ratio = trial.suggest_float(\"warmup_ratio\", 0.05, 0.15)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.001, 0.1, log=True)\n",
    "    max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.5, 2.0)\n",
    "    adam_beta1 = trial.suggest_float(\"adam_beta1\", 0.85, 0.95)\n",
    "    adam_beta2 = trial.suggest_float(\"adam_beta2\", 0.95, 0.999)\n",
    "    adam_epsilon = trial.suggest_float(\"adam_epsilon\", 1e-9, 1e-7, log=True)\n",
    "    \n",
    "    # Load pretrained model and processor without vocab resizing\n",
    "    processor = AutoProcessor.from_pretrained(pre_trained_id)\n",
    "    model = AutoModelForCTC.from_pretrained(pre_trained_id)\n",
    "    \n",
    "    # Enable gradient checkpointing\n",
    "    model.gradient_checkpointing_enable()\n",
    "    \n",
    "    # Prepare datasets (reusing original prepare_datasets function)\n",
    "    train_dataset, eval_dataset = prepare_datasets(train_df, processor)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=grad_accum,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        weight_decay=weight_decay,\n",
    "        num_train_epochs=5,  # Same as original search\n",
    "        logging_dir=f'{output_dir}/logs',\n",
    "        logging_steps=50,\n",
    "        save_steps=500,\n",
    "        eval_steps=100,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        save_safetensors=True,\n",
    "        save_strategy=\"steps\",\n",
    "        save_total_limit=1,\n",
    "        fp16=True,\n",
    "        fp16_full_eval=True,\n",
    "        dataloader_num_workers=4,\n",
    "        dataloader_pin_memory=True,\n",
    "        gradient_checkpointing=True,\n",
    "        adam_beta1=adam_beta1,\n",
    "        adam_beta2=adam_beta2,\n",
    "        adam_epsilon=adam_epsilon,\n",
    "    )\n",
    "    \n",
    "    # Create trainer with resource monitoring callback\n",
    "    class ResourceMonitorCallback(TrainerCallback):\n",
    "        def __init__(self, monitor):\n",
    "            self.monitor = monitor\n",
    "        \n",
    "        def on_step_end(self, args, state, control, **kwargs):\n",
    "            self.monitor.log_metrics()\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=DataCollatorCTCWithPadding(processor=processor, padding=\"longest\"),\n",
    "        callbacks=[\n",
    "            EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.01),\n",
    "            ResourceMonitorCallback(resource_monitor)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Train and evaluate\n",
    "    try:\n",
    "        resource_monitor.start()\n",
    "        train_result = trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        # Get resource usage summary\n",
    "        resource_summary = resource_monitor.get_summary()\n",
    "        \n",
    "        # Log results to wandb\n",
    "        wandb.log({\n",
    "            \"best_eval_loss\": eval_result[\"eval_loss\"],\n",
    "            \"final_train_loss\": train_result.training_loss,\n",
    "            **{f\"hp_{k}\": v for k, v in trial.params.items()},\n",
    "            **{f\"resource_{k}\": v for k, v in resource_summary.items()}\n",
    "        })\n",
    "        \n",
    "        wandb.finish()\n",
    "        \n",
    "        return eval_result[\"eval_loss\"]\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Trial {trial.number} failed with error: {str(e)}\")\n",
    "        wandb.finish()\n",
    "        return float('inf')\n",
    "\n",
    "def run_hyperparameter_search(train_df, pre_trained_id, n_trials=20):\n",
    "    \"\"\"\n",
    "    Run hyperparameter search with resource monitoring\n",
    "    \"\"\"\n",
    "    # Initialize resource monitor\n",
    "    monitor = ResourceMonitor(log_file=\"resource_usage.jsonl\")\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        sampler=optuna.samplers.TPESampler(seed=42),\n",
    "        pruner=optuna.pruners.MedianPruner(\n",
    "            n_startup_trials=5,\n",
    "            n_warmup_steps=5,\n",
    "            interval_steps=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    study.optimize(\n",
    "        partial(objective, train_df=train_df, pre_trained_id=pre_trained_id, resource_monitor=monitor),\n",
    "        n_trials=n_trials,\n",
    "        timeout=3600 * 12  # 12 hour timeout\n",
    "    )\n",
    "    \n",
    "    # Calculate total search time\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Get resource usage summary\n",
    "    resource_summary = monitor.get_summary()\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nHyperparameter Search Results:\")\n",
    "    print(f\"Total search time: {total_time:.2f} seconds\")\n",
    "    print(f\"Average GPU utilization: {resource_summary['avg_gpu_utilization']:.2f}%\")\n",
    "    print(f\"Maximum GPU memory used: {resource_summary['max_gpu_memory']:.2f} MB\")\n",
    "    print(f\"Average CPU utilization: {resource_summary['avg_cpu_percent']:.2f}%\")\n",
    "    \n",
    "    print(\"\\nBest trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"  Value: {trial.value}\")\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    \n",
    "    # Save study results\n",
    "    import joblib\n",
    "    joblib.dump(study, \"hyperparam_study_no_resize.pkl\")\n",
    "    \n",
    "    # Save resource metrics summary\n",
    "    with open('resource_summary.json', 'w') as f:\n",
    "        json.dump({\n",
    "            'total_search_time': total_time,\n",
    "            **resource_summary\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    return study\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = run_hyperparameter_search(train_df, PRE_TRAINED_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-16 21:24:17,694] A new study created in memory with name: linear_size_study_no_resize_20250116_212417\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:9rasp04y) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">linear_resize</strong> at: <a href='https://wandb.ai/aruna-team/xlsr-buckeye-phoneme-prediction-linear-size/runs/9rasp04y' target=\"_blank\">https://wandb.ai/aruna-team/xlsr-buckeye-phoneme-prediction-linear-size/runs/9rasp04y</a><br/> View project at: <a href='https://wandb.ai/aruna-team/xlsr-buckeye-phoneme-prediction-linear-size' target=\"_blank\">https://wandb.ai/aruna-team/xlsr-buckeye-phoneme-prediction-linear-size</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250116_211809-9rasp04y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:9rasp04y). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/arunasri/ML/notebooks/wandb/run-20250116_212418-kmnh0zeq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aruna-team/xlsr-buckeye-phoneme-prediction-linear-size/runs/kmnh0zeq' target=\"_blank\">trial_0_20250116_212417</a></strong> to <a href='https://wandb.ai/aruna-team/xlsr-buckeye-phoneme-prediction-linear-size' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aruna-team/xlsr-buckeye-phoneme-prediction-linear-size' target=\"_blank\">https://wandb.ai/aruna-team/xlsr-buckeye-phoneme-prediction-linear-size</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aruna-team/xlsr-buckeye-phoneme-prediction-linear-size/runs/kmnh0zeq' target=\"_blank\">https://wandb.ai/aruna-team/xlsr-buckeye-phoneme-prediction-linear-size/runs/kmnh0zeq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading pretrained model and processor (no resize version)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_0_20250116_212417</strong> at: <a href='https://wandb.ai/aruna-team/xlsr-buckeye-phoneme-prediction-linear-size/runs/kmnh0zeq' target=\"_blank\">https://wandb.ai/aruna-team/xlsr-buckeye-phoneme-prediction-linear-size/runs/kmnh0zeq</a><br/> View project at: <a href='https://wandb.ai/aruna-team/xlsr-buckeye-phoneme-prediction-linear-size' target=\"_blank\">https://wandb.ai/aruna-team/xlsr-buckeye-phoneme-prediction-linear-size</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250116_212418-kmnh0zeq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-01-16 21:24:32,971] Trial 0 failed with parameters: {'learning_rate': 5.611516415334504e-06, 'batch_size': 4, 'gradient_accumulation_steps': 2, 'warmup_ratio': 0.05580836121681995, 'weight_decay': 0.05399484409787434, 'max_grad_norm': 1.4016725176148133, 'adam_beta1': 0.9208072577796045, 'adam_beta2': 0.9510086402204943, 'adam_epsilon': 8.706020878304856e-08} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arunasri/ML/venv/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/tmp/ipykernel_373692/574223306.py\", line 53, in objective\n",
      "    model, processor = prepare_model_and_processor_no_resize(timit_vocab)\n",
      "  File \"/var/tmp/ipykernel_373692/1408254954.py\", line 177, in prepare_model_and_processor_no_resize\n",
      "    model = AutoModelForCTC.from_pretrained(PRE_TRAINED_ID)\n",
      "  File \"/home/arunasri/ML/venv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "  File \"/home/arunasri/ML/venv/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 3809, in from_pretrained\n",
      "    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n",
      "  File \"/home/arunasri/ML/venv/lib/python3.8/site-packages/transformers/utils/hub.py\", line 403, in cached_file\n",
      "    resolved_file = hf_hub_download(\n",
      "  File \"/home/arunasri/ML/venv/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/arunasri/ML/venv/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\n",
      "    return _hf_hub_download_to_cache_dir(\n",
      "  File \"/home/arunasri/ML/venv/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 1367, in _hf_hub_download_to_cache_dir\n",
      "    _download_to_tmp_and_move(\n",
      "  File \"/home/arunasri/ML/venv/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 1884, in _download_to_tmp_and_move\n",
      "    http_get(\n",
      "  File \"/home/arunasri/ML/venv/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 539, in http_get\n",
      "    for chunk in r.iter_content(chunk_size=DOWNLOAD_CHUNK_SIZE):\n",
      "  File \"/home/arunasri/ML/venv/lib/python3.8/site-packages/requests/models.py\", line 820, in generate\n",
      "    yield from self.raw.stream(chunk_size, decode_content=True)\n",
      "  File \"/home/arunasri/ML/venv/lib/python3.8/site-packages/urllib3/response.py\", line 628, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/home/arunasri/ML/venv/lib/python3.8/site-packages/urllib3/response.py\", line 567, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "  File \"/home/arunasri/ML/venv/lib/python3.8/site-packages/urllib3/response.py\", line 533, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "  File \"/home/arunasri/ML/venv/lib/python3.8/http/client.py\", line 455, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"/home/arunasri/ML/venv/lib/python3.8/http/client.py\", line 499, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"/home/arunasri/ML/venv/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/arunasri/ML/venv/lib/python3.8/ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/home/arunasri/ML/venv/lib/python3.8/ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "KeyboardInterrupt\n",
      "[W 2025-01-16 21:24:32,973] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 235\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 235\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_hyperparameter_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimit_vocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m results:\n\u001b[1;32m    237\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal best loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_trial\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[33], line 187\u001b[0m, in \u001b[0;36mrun_hyperparameter_search\u001b[0;34m(train_df, timit_vocab, n_trials)\u001b[0m\n\u001b[1;32m    175\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[1;32m    176\u001b[0m     direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    177\u001b[0m     study_name\u001b[38;5;241m=\u001b[39mstudy_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    184\u001b[0m )\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimit_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimit_vocab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduced from 20\u001b[39;49;00m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3600\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduced to 4 hours\u001b[39;49;00m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;167;43;01mException\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprint_intermediate_result\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Save study results with details\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_trial\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    198\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m\"\u001b[39m: study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mnumber,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m         }\n\u001b[1;32m    207\u001b[0m     }\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[33], line 53\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial, train_df, timit_vocab, base_output_dir)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Model preparation with error handling\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     model, processor \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_model_and_processor_no_resize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimit_vocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(processor\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mget_vocab()),\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear_layer_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mout_features\n\u001b[1;32m     57\u001b[0m     })\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[30], line 177\u001b[0m, in \u001b[0;36mprepare_model_and_processor_no_resize\u001b[0;34m(timit_vocab)\u001b[0m\n\u001b[1;32m    174\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading pretrained model and processor (no resize version)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    176\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(PRE_TRAINED_ID)\n\u001b[0;32m--> 177\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCTC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPRE_TRAINED_ID\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Store the old vocabulary\u001b[39;00m\n\u001b[1;32m    180\u001b[0m old_vocab \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mget_vocab()\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/transformers/modeling_utils.py:3809\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3793\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3794\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   3795\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3796\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[1;32m   3797\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3807\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[1;32m   3808\u001b[0m     }\n\u001b[0;32m-> 3809\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[1;32m   3814\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/huggingface_hub/file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m   1203\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1218\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1219\u001b[0m     )\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/huggingface_hub/file_download.py:1367\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1365\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1367\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1377\u001b[0m     _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/huggingface_hub/file_download.py:1884\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1881\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m   1882\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m-> 1884\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1891\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1893\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1894\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/huggingface_hub/file_download.py:539\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    537\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    541\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/urllib3/response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[0;32m--> 628\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    631\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/urllib3/response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    564\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/site-packages/urllib3/response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/http/client.py:455\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 455\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/http/client.py:499\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    494\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 499\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/ML/venv/lib/python3.8/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "from functools import partial\n",
    "import wandb\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "def objective(trial, train_df, timit_vocab, base_output_dir=\"./results\"):\n",
    "    \"\"\"\n",
    "    Optuna objective function for hyperparameter optimization with original ranges\n",
    "    \"\"\"\n",
    "    # Generate unique trial ID\n",
    "    trial_id = f\"trial_{trial.number}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Initialize wandb for this trial\n",
    "    run = wandb.init(\n",
    "        project=\"xlsr-buckeye-phoneme-prediction-linear-size\",\n",
    "        name=trial_id,\n",
    "        config={\n",
    "            \"trial_number\": trial.number,\n",
    "            \"model_type\": \"no_resize\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        reinit=True\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Sample hyperparameters with original ranges\n",
    "        config = {\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "            \"batch_size\": trial.suggest_categorical(\"batch_size\", [4, 8]),\n",
    "            \"grad_accum\": trial.suggest_categorical(\"gradient_accumulation_steps\", [2, 4, 8]),\n",
    "            \"warmup_ratio\": trial.suggest_float(\"warmup_ratio\", 0.05, 0.15),\n",
    "            \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.001, 0.1, log=True),\n",
    "            \"max_grad_norm\": trial.suggest_float(\"max_grad_norm\", 0.5, 2.0),\n",
    "            \"adam_beta1\": trial.suggest_float(\"adam_beta1\", 0.85, 0.95),\n",
    "            \"adam_beta2\": trial.suggest_float(\"adam_beta2\", 0.95, 0.999),\n",
    "            \"adam_epsilon\": trial.suggest_float(\"adam_epsilon\", 1e-9, 1e-7, log=True)\n",
    "        }\n",
    "        \n",
    "        # Log configuration\n",
    "        wandb.config.update(config)\n",
    "        \n",
    "        # Model preparation with error handling\n",
    "        try:\n",
    "            model, processor = prepare_model_and_processor_no_resize(timit_vocab)\n",
    "            wandb.log({\n",
    "                \"vocab_size\": len(processor.tokenizer.get_vocab()),\n",
    "                \"linear_layer_size\": model.lm_head.out_features\n",
    "            })\n",
    "        except Exception as e:\n",
    "            wandb.log({\"error\": f\"Model preparation failed: {str(e)}\"})\n",
    "            raise e\n",
    "        \n",
    "        # Dataset preparation with memory tracking and debug logging\n",
    "        try:\n",
    "            print(f\"\\nPreparing datasets for trial {trial.number}...\")\n",
    "            train_dataset, eval_dataset = prepare_datasets(train_df, processor)\n",
    "            print(f\"Successfully created datasets - Train size: {len(train_dataset)}, Eval size: {len(eval_dataset)}\")\n",
    "            wandb.log({\n",
    "                \"train_size\": len(train_dataset),\n",
    "                \"eval_size\": len(eval_dataset)\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"\\nDataset preparation failed for trial {trial.number}: {str(e)}\")\n",
    "            wandb.log({\"error\": f\"Dataset preparation failed: {str(e)}\"})\n",
    "            raise e\n",
    "        \n",
    "        # Configure training arguments with adjusted settings for stability\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{base_output_dir}/{trial_id}\",\n",
    "            learning_rate=config[\"learning_rate\"],\n",
    "            per_device_train_batch_size=config[\"batch_size\"],\n",
    "            per_device_eval_batch_size=config[\"batch_size\"],\n",
    "            gradient_accumulation_steps=config[\"grad_accum\"],\n",
    "            max_grad_norm=config[\"max_grad_norm\"],\n",
    "            warmup_ratio=config[\"warmup_ratio\"],\n",
    "            weight_decay=config[\"weight_decay\"],\n",
    "            num_train_epochs=5,\n",
    "            logging_dir=f'{base_output_dir}/{trial_id}/logs',\n",
    "            logging_steps=50,\n",
    "            save_steps=500,\n",
    "            eval_steps=100,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            save_total_limit=1,\n",
    "            fp16=True,\n",
    "            dataloader_num_workers=0,\n",
    "            dataloader_pin_memory=True,\n",
    "            gradient_checkpointing=True,\n",
    "            adam_beta1=config[\"adam_beta1\"],\n",
    "            adam_beta2=config[\"adam_beta2\"],\n",
    "            adam_epsilon=config[\"adam_epsilon\"],\n",
    "            save_safetensors=True,\n",
    "            save_strategy=\"steps\",\n",
    "            fp16_full_eval=True,\n",
    "            # Remove legacy prediction loop and ensure proper batch handling\n",
    "            dataloader_drop_last=True,  # Changed to True to ensure consistent batch sizes\n",
    "            prediction_loss_only=True,  # Added to focus on loss computation\n",
    "            remove_unused_columns=False,  # Added to prevent column removal issues\n",
    "            label_smoothing_factor=0.0,  # Explicitly set label smoothing\n",
    "            report_to=[\"wandb\"]\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer with early stopping\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            data_collator=DataCollatorCTCWithPadding(\n",
    "                processor=processor,\n",
    "                padding=\"longest\"\n",
    "            ),\n",
    "            callbacks=[\n",
    "                EarlyStoppingCallback(\n",
    "                    early_stopping_patience=3,\n",
    "                    early_stopping_threshold=0.01\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Training with error handling\n",
    "        try:\n",
    "            train_result = trainer.train()\n",
    "            eval_result = trainer.evaluate()\n",
    "            \n",
    "            # Log metrics\n",
    "            metrics = {\n",
    "                \"final_eval_loss\": eval_result[\"eval_loss\"],\n",
    "                \"final_train_loss\": train_result.metrics[\"train_loss\"],\n",
    "                \"total_steps\": train_result.metrics[\"step\"],\n",
    "                \"training_time\": train_result.metrics[\"train_runtime\"]\n",
    "            }\n",
    "            wandb.log(metrics)\n",
    "            \n",
    "            # Pruning hook for Optuna\n",
    "            trial.report(eval_result[\"eval_loss\"], step=train_result.metrics[\"step\"])\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "            return eval_result[\"eval_loss\"]\n",
    "            \n",
    "        except Exception as e:\n",
    "            wandb.log({\"error\": f\"Training failed: {str(e)}\"})\n",
    "            raise e\n",
    "            \n",
    "    except optuna.exceptions.TrialPruned:\n",
    "        wandb.log({\"status\": \"pruned\"})\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        wandb.log({\"status\": \"failed\", \"error\": str(e)})\n",
    "        raise\n",
    "    finally:\n",
    "        wandb.finish()\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "def run_hyperparameter_search(train_df, timit_vocab, n_trials=20):\n",
    "    \"\"\"\n",
    "    Run hyperparameter search with improved monitoring\n",
    "    \"\"\"\n",
    "    study_name = f\"linear_size_study_no_resize_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        study_name=study_name,\n",
    "        sampler=optuna.samplers.TPESampler(seed=42),\n",
    "        pruner=optuna.pruners.MedianPruner(\n",
    "            n_startup_trials=5,\n",
    "            n_warmup_steps=5,\n",
    "            interval_steps=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        study.optimize(\n",
    "            partial(objective, train_df=train_df, timit_vocab=timit_vocab),\n",
    "            n_trials=10,  # Reduced from 20\n",
    "            timeout=3600 * 4,  # Reduced to 4 hours\n",
    "            catch=(Exception,),\n",
    "            callbacks=[print_intermediate_result]\n",
    "        )\n",
    "        \n",
    "        # Save study results with details\n",
    "        results = {\n",
    "            \"best_trial\": {\n",
    "                \"number\": study.best_trial.number,\n",
    "                \"value\": study.best_trial.value,\n",
    "                \"params\": study.best_trial.params\n",
    "            },\n",
    "            \"study_statistics\": {\n",
    "                \"completed_trials\": len(study.trials),\n",
    "                \"pruned_trials\": len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]),\n",
    "                \"failed_trials\": len([t for t in study.trials if t.state == optuna.trial.TrialState.FAIL])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save results\n",
    "        import json\n",
    "        with open(f\"hyperparam_results_{study_name}.json\", \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        print(\"\\nBest trial results:\")\n",
    "        print(f\"  Value: {study.best_trial.value}\")\n",
    "        print(\"  Params: \")\n",
    "        for key, value in study.best_trial.params.items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Study failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def print_intermediate_result(study, trial):\n",
    "    \"\"\"Callback to print intermediate results\"\"\"\n",
    "    print(f\"\\nTrial {trial.number}:\")\n",
    "    print(f\"  Current value: {trial.value}\")\n",
    "    print(f\"  Best value so far: {study.best_value}\")\n",
    "    print(f\"  Parameters: {trial.params}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        results = run_hyperparameter_search(train_df, timit_vocab)\n",
    "        if results:\n",
    "            print(f\"\\nFinal best loss: {results['best_trial']['value']}\")\n",
    "        else:\n",
    "            print(\"\\nHyperparameter search failed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Main execution error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
