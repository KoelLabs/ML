# 100 hours from 53 languages with time aligned phoneme transcriptions and translations: https://doreco.huma-num.fr/UsingDoReCo
# Paper: https://aclanthology.org/2025.fieldmatters-1.5.pdf

import os
import sys

import zipfile
import pandas as pd

sys.path.append(os.path.join(os.path.dirname(__file__), ".."))
from data_loaders.common import BaseDataset, interactive_flag_samples
from core.audio import audio_bytes_to_array, TARGET_SAMPLE_RATE
from core.codes import xsampa2ipa

TOP_DIR = os.path.join(os.path.dirname(__file__), "..", "..", ".data", "DoReCo")

# We use specifically the Southern British English data for now: https://doreco.huma-num.fr/languages/sout3282, https://multicast.aspra.uni-bamberg.de/#english
AUDIO_ZIP = os.path.join(TOP_DIR, "sout3282_audio_v2.zip")
AUDIO_CORE_ZIP = os.path.join(TOP_DIR, "sout3282_audio_core_v2.zip")
SOURCE_SAMPLE_RATE = None  # auto detect

ANNOTATION_ZIP = os.path.join(TOP_DIR, "sout3282_annotations_v2.zip")
METADATA_PATH = "doreco_sout3282_extended_v1.3/doreco_sout3282_metadata.csv"
PHONEMES_PATH = "doreco_sout3282_extended_v1.3/doreco_sout3282_ph.csv"
# PHONEMES_PATH = "doreco_sout3282_extended_v1.3/doreco_sout3282_wd.csv"
INSTRUCTIONS_PATH = "doreco_sout3282_extended_v1.3/doreco_CONVENTIONS.txt"

SPECIAL_SYMBOLS = {
    "<<fp>>": "Filled pause",
    "<<fs>>": "False start",
    "<<pr>>": "Prolongation",
    "<<fm>>": "Foreign material",
    "<<sg>>": "Singing",
    "<<bc>>": "Backchannel",
    "<<id>>": "Ideophone",
    "<<on>>": "Onomatopoeic",
    "<<wip>": "Word-internal pause",
    "<<ui>>": "Unidentifiable",
    "<p:>": "Silent pause",
}


def is_special_symbol(ph: str):
    return type(ph) == float or ph.startswith("<<") or ph == "<p:>" or ph == "[INT]"


# Core tiers:
#   ref: Reference ID for tx units; generated by DoReCo
#   tx: Transcription of chunks of speech, separated by corpus creators based on a variety of criteria (syntactic, prosodic, semantic, or pragmatic)
#   ft: Free translation of the tx tier into a widely spoken language
#   wd: Word unit
#   mb: Morph unit
#   gl: Morphematic gloss of morph unit
#   ps: Part of speech category; can be of the word unit or the morph unit, depending on the dataset
#   ph: phone unit (in X-SAMPA format)
# Supplementary tiers
#   doreco-mb-algn: Units indicate a morpheme boundary which the alignment algorithm was not able to confidently assign to a phoneme boundary; helpful for filtering out such cases
#   mc-zero: Only present in MultiCAST corpora; units contain two kinds of information which could not be assigned to a phone: 1) clause boundaries and 2) zero words; the contents of the mb/gl/ps tiers are separated by spaces ' ', and multiple adjacent units are further separated by pipes '|'
#   refind and isnref: Only present in MultiCAST corpora; identification of referents following Schiborr, Nils N. & Schnell, Stefan & Thiele, Hanna. 2018. RefIND â€” Referent Indexing in Natural-language Discourse: Annotation guidelines. Version 1.1. (https://multicast.aspra.uni-bamberg.de/data/pubs/refind/Schiborr+etal2018_RefIND-guidelines_v1.1.pdf)


class DoReCoDataset(BaseDataset):
    def __init__(
        self,
        split="all",
        include_timestamps=False,
        include_speaker_info=False,
        include_text=False,
    ):
        """
        Valid splits are "all" to include all the utterances, T001-T010 for each of the recording sessions, or EN01-EN04 for each of the speakers.
        """
        super().__init__(split, include_timestamps, include_speaker_info, include_text)

        self.audio_zip = zipfile.ZipFile(AUDIO_ZIP, "r")
        self.audio_core_zip = zipfile.ZipFile(AUDIO_CORE_ZIP, "r")

        with zipfile.ZipFile(ANNOTATION_ZIP, "r") as annotation_zip:
            with annotation_zip.open(METADATA_PATH) as f:
                self.metadata_df = pd.read_csv(f)
                # columns = ['id', 'name', 'spk_code', 'spk_age', 'spk_age_c', 'spk_sex', 'rec_date', 'rec_date_c', 'genre', 'genre_stim', 'gloss', 'transl', 'sound_quality', 'background_noise', 'word_tokens', 'word_tokens_core', 'extended']
                # prepend "doreco_sout3282_" to all names in metadata_df to match the phonemes
                self.metadata_df["name"] = "doreco_sout3282_" + self.metadata_df["name"]

            with annotation_zip.open(PHONEMES_PATH) as f:
                self.phoneme_df = pd.read_csv(f)
                # columns = ['lang', 'file', 'core_extended', 'speaker', 'ph_ID', 'ph', 'start', 'end', 'ref', 'tx', 'ft', 'wd_ID', 'wd', 'mc-zero', 'mb_ID', 'mb', 'doreco-mb-algn', 'ps', 'gl', 'refind', 'isnref']

        filter_by_session = split.startswith("T")
        filter_by_speaker = split.startswith("EN")
        if filter_by_session:
            self.metadata_df = self.metadata_df[self.metadata_df["id"] == split]
            self.phoneme_df = self.phoneme_df[
                self.phoneme_df["file"].isin(self.metadata_df["name"].unique())
            ]
        elif filter_by_speaker:
            self.metadata_df = self.metadata_df[self.metadata_df["spk_code"] == split]
            self.phoneme_df = self.phoneme_df[self.phoneme_df["speaker"] == split]

        # only speakers EN01 and EN03 actually have phonemic transcriptions
        self.phoneme_df = self.phoneme_df[
            self.phoneme_df["speaker"].isin(["EN01", "EN03"])
        ]
        assert split in [
            "all",
            "EN01",
            "EN03",
        ], f"There are no phonemic transcriptions for {split}, only words"

        self.utterances = [
            x
            for x in self.phoneme_df["ref"].unique()
            if not is_special_symbol(x)
            if len(self.phoneme_df[self.phoneme_df["ref"] == x]) > 10
        ]

    def __del__(self):
        self.audio_zip.close()
        self.audio_core_zip.close()

    def __len__(self):
        return len(self.utterances)

    def _get_ix(self, ix):
        utterance: str = self.utterances[ix]

        # parse annotations
        phoneme_units = self.phoneme_df[self.phoneme_df["ref"] == utterance]
        timestamped_phonemes = [
            (
                xsampa2ipa(unit["ph"]),
                int(float(unit["start"]) * TARGET_SAMPLE_RATE),
                int(float(unit["end"]) * TARGET_SAMPLE_RATE),
            )
            for _, unit in phoneme_units.iterrows()
            if not is_special_symbol(unit["ph"])
        ]
        ipa = "".join(x[0] for x in timestamped_phonemes)

        word_ids = phoneme_units["wd_ID"].unique()
        timestamped_words = [
            (
                phoneme_units[phoneme_units["wd_ID"] == word_id].iloc[0]["wd"],
                int(
                    float(
                        phoneme_units[phoneme_units["wd_ID"] == word_id]["start"].min()
                    )
                    * TARGET_SAMPLE_RATE
                ),
                int(
                    float(phoneme_units[phoneme_units["wd_ID"] == word_id]["end"].max())
                    * TARGET_SAMPLE_RATE
                ),
            )
            for word_id in word_ids
            if not is_special_symbol(
                phoneme_units[phoneme_units["wd_ID"] == word_id].iloc[0]["wd"]
            )
        ]
        text = " ".join(x[0] for x in timestamped_words)

        # parse metadata
        name = phoneme_units["file"].iloc[0]
        metadata_dict = (
            self.metadata_df[self.metadata_df["name"] == name].iloc[0].to_dict()
        )
        is_core = metadata_dict["extended"] == "no"

        # parse audio
        start = timestamped_phonemes[0][1]
        end = timestamped_phonemes[-1][2] + int(TARGET_SAMPLE_RATE * 0.3)
        if is_core:
            filename = "doreco_sout3282_audiofiles_v2.0/" + name + ".wav"
            with self.audio_core_zip.open(filename) as f:
                audio = audio_bytes_to_array(
                    f.read(), SOURCE_SAMPLE_RATE, TARGET_SAMPLE_RATE
                )
        else:
            filename = name[len("doreco_sout3282_") :] + ".wav"
            with self.audio_zip.open(filename) as f:
                audio = audio_bytes_to_array(
                    f.read(), SOURCE_SAMPLE_RATE, TARGET_SAMPLE_RATE
                )
        audio = audio[start:end]

        result = [ipa, audio]
        if self.include_timestamps:
            result.append(timestamped_phonemes)
        if self.include_speaker_info:
            result.append(metadata_dict)
        if self.include_text:
            result.append(text)
        return tuple(result)


if __name__ == "__main__":
    dataset = DoReCoDataset(
        split="all",
        include_timestamps=True,
        include_speaker_info=True,
        include_text=True,
    )
    print(len(dataset))
    interactive_flag_samples(dataset)
